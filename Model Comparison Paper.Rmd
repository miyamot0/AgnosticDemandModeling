---
title             : "Hidden Equivalence in the Operant Demand Framework: A Review and Evaluation of Multiple Methods for Evaluating Non-Consumption"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn P. Gilroy"
    affiliation   : "1"
    corresponding : yes
    address       : "226 Audubon Hall Baton Rouge, Louisiana 70806"
    email         : "sgilroy1@lsu.edu"
    role:
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
 Shawn Patrick Gilroy is an Assistant Professor of School Psychology at Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu. The source code necessary to recreate this work is publicly hosted in a repository at: https://github.com/miyamot0/AgnosticDemandModeling 

abstract: |
 Operant translations of Behavioral Economic concepts and principles have enhanced the ability of researchers to characterize the effects of reinforcers under constraints and uncertainty. Operant Behavioral Economic models of choice, (i.e., Operant Demand) have been particularly useful in evaluating how the consumption of reinforcers is affected by various ecological factors (e.g., price, limited resources). Prevailing perspectives in the Operant Demand Framework are derived from the framework presented in Hursh & Silberburg (2008). Few dispute the utility of this framework and model, though debate continues regarding how to address limitations associated with the logarithmic scale in this framework. At present, there are competing views regarding the handling of non-consumption (i.e., 0 consumption values) and under which situations alternative restatements of this framework are recommended. The purpose of this report was to review the shared mathematical bases for the Hursh & Silberburg (2008) and Koffarnus et al. (2015) models and how each can accomodate non-consumption values. Simulations derived from those featured in Koffarnus et al. (2015) were used to conduct tests of equivalence between modeling strategies while controlling for interpretations of residual error and the absolute lower asymptotes. Simulations and proofs are provided to illustrate how neither the Hursh & Silberburg (2008) nor Koffarnus et al. (2015) can characterize 0 and how both ultimately arrive at the same upper and lower asymptotes. Additional discussions are provided which question whether debates and comparisons between between models in the same framework enhances the generality and utility of methods in the Operant Demand Framework.

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : yes

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
---

\raggedbottom

```{r include = FALSE}
library(beezdemand)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(parameters)
library(scales)
library(tidyverse)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Contemporary methods for evaluating the consumption of goods and services using the Operant Demand Framework are heavily influenced by the methodology proposed in @hursh2008economic. This framework and methodology has evolved through several forms [@hursh1987linear] and the latest iteration takes a non-linear (i.e., "S"-type) shape and is driven by an exponential decay process [@hursh2008economic]. This strategy for evaluating the effects of unit price on the consumption of reinforcers has achieved widespread adoption and has also inspired derivatives that model consumption on varying scales, e.g., linear [@koff2015expt], "log-like" [@gilroy2021zbe].

The original implementation of the @hursh2008economic framework was modeled from the notion that the prototypical shape of the demand curve was an "S"-type form bounded by upper and lower limits. The original intent of @hursh2008economic was to have an upper asymptote defined at a price of zero, i.e. $\lim_{P \to - \infty}$, and a lower asymptote reached as prices approached infinity, $\lim_{P \to \infty}$ [@gilroy2021zbe]. Models derived from this framework (e.g., @hursh2008economic, @koff2015expt) evaluate the demand for reinforcers with non-zero upper and lower asymptotes and these models are bounded at an upper limit (i.e., $Q_0$) and progress towards a non-zero lower asymptote in the "S"-type form. Non-zero asymptotes in models derived from the @hursh2008economic framework make good sense because the original values of interest were positive real values (i.e., not 0). Such quantities are expected because the logarithmic representation of consumption is undefined at 0.

In response to the statistical and philosophical issues related to the omission of 0 consumption values, @koff2015expt introduced a restatement of the @hursh2008economic framework designed to accommodate non-consumption (i.e., 0 consumption values) in non-linear regression. Specifically, this variant of the @hursh2008economic framework was adjusted to examine changes in observed consumption on the linear scale. An exponentiation of terms was performed such that the LHS (left-hand side) of the @hursh2008economic model was reflected in the linear scale to support the inclusion of non-consumption in the regression. In this way, the LHS of the model (i.e., observed consumption) need not be submitted to the log transformation that prevented the use of the @hursh2008economic model with non-consumption. This alternative approach drew considerable attention, as one of the largest issues associated with the log scale could be avoided. However, it warrants noting that most of the RHS (right-hand side) of the @koff2015expt model remained on the log scale. For instance, the span of the demand curve and the rate of exponential decay remained in the log scale [@gilroy2021zbe]. For this reason, the span of the demand curve in this restated model cannot reach a true 0 point, and thus, is also bound by non-zero asymptotes despite accommodating 0 consumption values in model regression. Furthermore, the regressive process for logarithmic and linear models differ with respect to how residual error is interpreted and this introduces behavior that differs between models [@gilroy2021zbe].

## Same Model But Different Error

The challenges associated with fitting models of operant demand (i.e., minimizing residual error) are increasingly discussed by researchers applying the Operant Demand Framework [@gilroy2021zbe]. @gilroy2021zbe noted, among other things, that residual error is reflected differently in log and linear scales and that such differences can affect model optimization and resulting parameters. Briefly, changes in log space reflect relative differences while changes in the linear space reflect absolute differences. In most economic applications, relative error is preferred because the quantities of interest and their associated projections (i.e., $\hat{y}$) typically span across multiple orders. In these situations, quantities observed at higher orders would be weighted more heavily than those at lower orders in the linear scale unless some form of correction was applied (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). Given this uneven weighting of information in the model, relative difference is typically the default because this approach treats error in a way balances how information is weighted in the model.

As an alternative to relative difference, absolute difference is straightforward. That is, absolute error is simply the difference from some observed quantity and $\hat{y}$ regardless of the order (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). Although simpler, the use of the linear scale in demand introduces some variability in how parameters are optimized in these models. For example, @gilroy2021zbe discussed how a departure from relative difference can affect model optimization when observed levels of consumption vary considerably across orders. This has led to occasional inconsistencies wherein estimates across derivatives of the @hursh2008economic framework lead to different conclusions (e.g., shared alpha across varying dose-response curves). As such, differences in residual error is one dimension along which derivatives of the @hursh2008economic framework differ.

## Different Error But Same Asymptotes

There has been renewed attention to the non-zero asymptotes in models derived from the @hursh2008economic framework [@gilroy2021zbe]. Specifically, inspection of the @hursh2008economic and the @koff2015expt models reveals that neither model can characterize demand at 0. This is the case even when non-consumption values can be included in the regression. The argument for a true 0 asymptote is not discussed here, though interested readers are encouraged to review @gilroy2021zbe for an exposition on why the inability to model demand at 0 limits the utility of the Operant Demand Framework.

Revisiting the topic of asymptotes, two novel terms are introduced in this work: $A_{Upper}$ and $A_{Lower}$. The term $A_{Upper}$ is used to refer to the absolute upper bound of the demand curve. In models derived from the @hursh2008economic framework, this is simply the fitted parameter $Q_{0}$. This is because parameter $Q_{0}$ is the absolute upper limit to the demand curve when prices equals 0, i.e. $\displaystyle{f(0)= A_{Upper}}$. In contrast, the term $A_{Lower}$ refers the absolute lower bound of the demand curve and this is not reflected by any *single* parameter. Mathematically, this absolute lower limit refers to the level of demand as price approaches $\infty$, i.e.$\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}$. These two upper and lower extremes are separated by the span constant $\textit{k}$, which specifies the distance in log units between these asymptotes [@gilroy2021zbe]. The notation of both $A_{Upper}$ and $A_{Lower}$ are noted below and are proofed in greater detail across models in the Appendix of this work.

```{=tex}
\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}
```
Further inspection of $A_{Lower}$ and its derivation evokes questions regarding how any model based on the @hursh2008economic framework could characterize non-consumption values. As noted in the bottom portion of *Equation 1*, both the @hursh2008economic and @koff2015expt models can never represent a value of 0 because such a value cannot exist between these upper and lower extremes. This introduces a complex situation wherein 0 consumption values could be included in non-linear regression, but levels of demand could never characterize this value. As such, similarities in non-zero asymptotes is one dimension along which derivatives of the @hursh2008economic framework are the same.

## Same Asymptotes and Same Spans

Understanding non-consumption in the @hursh2008economic framework requires a clear grasp of how the span parameter $\textit{k}$ influences the range of values that may be predicted (i.e., $\hat{y}$). In the original implementation of the @hursh2008economic framework, $\textit{k}$ represented the range of observed, non-zero consumption values. That is, $\textit{k}$ was derived in log units from the upper and lower extremes of all positive, real numbers. Since 0 consumption values were not included in the original implementation, parameter $\textit{k}$ was directly linked to the upper and lower limits of the observed data. As such, the specification of this constant was straightforward and parameter $\textit{k}$, $A_{Upper}$, and $A_{Lower}$ were all directly linked to positive real numbers. A visualization of parameter $\textit{k}$ is provided in \autoref{fig:fig1CurveSpanEmpirical} with respect to positive real consumption values.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes") +
  geom_point() +
  annotate("segment", 
           x      = 0.1, 
           xend   = 0.1, 
           y      = 100, 
           yend   = lowerAsymptote,
           colour = "grey40") +
  annotate("text", 
           x     = 0.175, 
           y     = 55, 
           angle = 90,
           adj   = 0.5,
           col   = "grey40",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = lowerAsymptote, 
           yend   = lowerAsymptote,
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "grey40",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = max(preFit$Q), 
           yend   = max(preFit$Q),
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "grey40",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = c(preFit$P, 10000),
                labels = c(preFit$P, 10000),
                limits = c(0.1, 10000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

```

Whereas the determination of parameter $\textit{k}$ in the @hursh2008economic implementation is linked to positive real numbers, the interpretation of parameter $\textit{k}$ became more complicated in the implementation introduced by @koff2015expt. This added complexity emerged because parameter $\textit{k}$ was still based on positive real numbers but had to be inflated to project $A_{Lower}$ *beyond* the range of positive real values to a quantity nearer to 0. This represented novel behavior for parameter $\textit{k}$ and various teams have constructed strategies to assist in driving $A_{Lower}$ beyond the range of non-zero consumption. For example, some have added a constant to parameter $\textit{k}$ (derived from positive real values) or allowed this parameter to vary as a fitted parameter [@kaplan2018understanding]. Regardless of the method, the rationale was to inflate the span of the demand curve to and drive $A_{Lower}$ to a lower point. A visualization of this span-inflating behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE, fig.height=3.5}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  scale_color_manual(values = c(
    "K (Empirical)" = "grey60",
    "K + 0.5"       = "grey50",
    "K + 1"         = "grey40",
    "K + 2"         = "grey20",
    "K + 3"         = "grey0"
  )) +
  annotate("text",
           x     = 100,
           y     = 90,
           adj   = 0,
           col   = "grey60",
           parse = TRUE,
           label = expression("K (Empirical): " ~ A[LOWER] == 5.442)) +
  annotate("text",
           x     = 100,
           y     = 75,
           adj   = 0,
           col   = "grey50",
           parse = TRUE,
           label = expression("K + 0.5: " ~ A[LOWER] == 1.7209)) +
  annotate("text",
           x     = 100,
           y     = 60,
           adj   = 0,
           col   = "grey40",
           parse = TRUE,
           label = expression("K + 1: " ~ A[LOWER] == 0.5442)) +
  annotate("text",
           x     = 100,
           y     = 45,
           adj   = 0,
           col   = "grey20",
           parse = TRUE,
           label = expression("K + 2: " ~ A[LOWER] == 0.0544)) +
  annotate("text",
           x     = 100,
           y     = 30,
           adj   = 0,
           col   = "grey0",
           parse = TRUE,
           label = expression("K + 3: " ~ A[LOWER] == 0.0054)) +
  scale_x_log10(breaks = c(preFit$P, 10000),
                labels = c(preFit$P, 10000),
                limits = c(0.1, 10000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none"
  )

```

\autoref{fig:fig2CurveSpanEmpiricalMod} illustrates how inflating the span affects $A_{Lower}$ and this has three appreciable effects on the demand curve. First, the rate of change in elasticity is jointly reflected by parameter $\alpha$ and the span of the demand curve [@gilroy2020interpretation]. Given that $\alpha$ is a unit-less quantity, it co-varies inversely with the size of the span constant. That is, relatively greater $\alpha$ values reflect rapid changes in $\hat{y}$ across prices and relatively lesser values reflecting gradual changes in $\hat{y}$ across prices. Second, $\textit{k}$ values (i.e., $k < \frac{e}{log(10)}$) influence both the span of the demand curve as well as the range of elasticity and inelasticity observed in models derived from the @hursh2008economic framework [@newman2020improved; @gilroy2019exactsolve]. That is, $\textit{k}$ values that do not permit a span of 1 log unit necessarily restrict the range of elasticity values (i.e., analytic solutions for $P_{MAX}$ are not possible). Third, and most relevant to the @koff2015expt model, inflated $\textit{k}$ values serve to lessen the absolute difference between $A_{Lower}$ and 0. That is, the distance between $A_{Lower}$ and 0 is is lessened, but no $\textit{k}$ value will drive the span to 0. \autoref{fig:fig2CurveSpanEmpiricalMod} provides an elegant display of how this gap decreases proportionally with each unit increase in the span parameter $\textit{k}$.

## Hidden Model Equivalence

The sections above outline the limited ways in which two of the most popular derivatives of the @hursh2008economic framework differ. These two modeling strategies differ in terms of optimization (i.e., minimization of residual error) but share the limitations related to asymptotes. Regarding the first point, residual error and optimization, the two models can provide equivalent results when the handling of residual error is made *comparable*. That is, re-weighting the errors (i.e., relative to $\hat{y}$) in the @koff2015expt model can yields fits and estimates approximate to those resulting from the @hursh2008economic model *in the absence of non-consumption.* Alternatively, the @hursh2008economic model can be adjusted to yield estimates comparable to those from the @koff2015expt model by adjusting residual error to be interpreted in terms of absolute difference, i.e. $E_i=10^{\hat{y}}-10^{y}$. A visualization of inter-related model fits are illustrated in \autoref{fig:fig3ComparisonNonzeroRelativeError}.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error", fig.height=2, warning=FALSE}

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           weights = 1/(predict(EXPT)),
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  rename(`Exponential`   = explPred,
         `Exponentiated (Absolute Error)` = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred) 

# kable(preFit.trim) %>%
#   kable_styling(full_width = TRUE)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P) 

ggplot(preFit.trim, aes(P, Q)) +
  #ggtitle("TODO: Add title") +
  geom_point(color = "black") +
  xlab("Unit Price") +
  ylab("Consumption") +
  geom_line(aes(P, Prediction), color = "grey40") +
  facet_wrap(~Model, ncol = 3) +
  # scale_y_log10(breaks = c(1, 10, 100),
  #               labels = c(1, 10, 100),
  #               limits = c(1, 100)) +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom", 
    strip.background.x = element_blank()
  )

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

preFit.point.1 <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0),
  Model = "Exponentiated"
)

preFit.point.2 = preFit %>%
  select(P, Q) %>%
  mutate(Model = "Exponential (Absolute)")

preFit.point.full = rbind(preFit.point.1,
                          preFit.point.2)

```

Regarding the second point, $A_{Lower}$ is seldom discussed in Operant Demand and this has considerable influence on models derived from the @hursh2008economic framework. This is an inherently complex topic, especially so in the @koff2015expt restatement, because consumption values observed at 0 are a quantity that cannot be predicted by models that reflect the range of consumption in log units. In attempting to accommodate non-consumption, modeling derived from the @hursh2008economic framework must minimize *two* sources of error instead of one. That is, the modeling must minimize residual error (as typical) as well as the distance between $A_{Lower}$ and 0. This is because the span must be sufficiently inflated in attempts to produce an $A_{Lower}$ that *approximates* 0. For instance, an application of the @koff2015expt model where $\textit{k}$ is included as a fitted parameter simultaneously optimizes demand intensity, rates of change in elasticity, and a span constant (i.e., $A_{Lower}$). As noted above, $A_{Lower}$ is driven lower by inflating the span constant towards some non-zero quantity that is *reasonably* close to 0. Pragmatically, proponents of the @koff2015expt approach would likely argue that such a small amount of error calls for little concern and that $A_{Lower}$ could be considered *close enough* of an approximation of 0 to enable analyses using the complete data set (non-consumption values included).

Revisiting the argument around a *close enough* approximation of non-consumption, let us consider the following hypothetical. Let us say that the interpretation of a fitted @koff2015expt model optimizes such that values at $A_{Lower}$ are a close enough approximation of 0 to proceed with demand curve analyses using a complete data set that includes non-consumption. Following this logic (i.e., $A_{Lower} \cong 0$), it stands to reason that treating sufficiently low $A_{Lower}$ values and 0 consumption values as the same *should* replicate the behavior of the @koff2015expt model in the @hursh2008economic model. Assuming an inflated $\textit{k}$ parameter, equivalent estimates should result because $\hat{y}$ can be predicted beyond the range of observed non-zero levels and the resulting $A_{Lower}$ should be *close enough* to 0 on the linear scale that differences between $A_{Lower}$ and 0 would be considered negligible. Controlling for differences in terms of error representation, it stands to reason that the @hursh2008economic model would provide equivalent estimates had non-consumption been replaced by respective $A_{Lower}$ values and error minimization been reflected in terms of absolute differences.

In a demonstration of this modified @hursh2008economic approach, the full data set from \autoref{fig:fig1CurveSpanEmpirical} was fitted with an inflated $\textit{k}$ parameter and non-consumption values replaced with respective $A_{Lower}$ values. Specifically, the most inflated span and corresponding $A_{Lower}$ from \autoref{fig:fig2CurveSpanEmpiricalMod} were used in this example demonstration, i.e. $A_{Lower} = 10^{log{10}Q_0 - (k + 3)}$. The results of this modified @hursh2008economic approach are illustrated along with the @koff2015expt approach are illustrated in \autoref{fig:fig4ComparisonZeroRelative}.

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes", fig.height=2}

ggplot(subset(preFit.g, Model != "Exponential (Relative)"), aes(P, Q)) +
  geom_point(data = preFit.point.full) +
  geom_line(aes(P, Prediction), color = "grey40") +
  scale_x_log10(breaks = c(0.1, 10, 100, 1000, 10000),
                labels = c(0.1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  facet_wrap(~Model, ncol = 3) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none", 
    strip.background.x = element_blank()
  )

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  select(-c(`Exponential.Relative`)) %>%
  relocate(Exponentiated, .after = Q) %>%
  mutate(`Q.Mod` = Q) %>%
  mutate(Q = preFit.point.1$Q) %>%
  relocate(`Exponential.Absolute`, .after = `Q.Mod`) %>%
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

Controlling for differences in error handling (absolute difference) and $A_{Lower}$ values (non-consumption replaced by lower asymptotes in the @hursh2008economic model), the model fits are functionally equivalent, see \autoref{tab:table1ComparisonZeroAbsolute}. This short example highlights several details that often go unnoticed when using the @koff2015expt model. First, this model does not characterize demand at 0. Rather, an inflated $\textit{k}$ parameter to drives $A_{Lower}$ to a quantity *close enough* to 0 that the absolute difference between 0 and $\hat{y}$ is negligible. This is the best that this approach can achieve because 0 does not fall within the interval between $A_{Upper}$ and $A_{Lower}$. Second, this approach is functionally equivalent to the @hursh2008economic model when non-consumption values are replaced with by the respective $A_{Lower}$ values and when residual errors are de-weighted (i.e., absolute). As such, both functional almost identically and differ in largely trivial aspects.

## Planned Comparisons

The purpose of this technical report was to clarify and evaluate the dimensions along which the @hursh2008economic and @koff2015expt models vary. Specifically, the shared mathematical bases between the two should allow for modifications wherein both provide statistically equivalent estimates---even when non-consumption values are present. The primary questions for the simulation study was to determine whether estimates resulting from the @hursh2008economic and @koff2015expt models would be statistically equivalent when controlling for differences in handling residual error (i.e., absolute, relative) and treating non-consumption values as respective $A_{Lower}$ values.

# Methods

```{r simulationParams}

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 1000

sim <- SimulateDemand(nruns = nParticipants, setparams = setparams, sdindex = sdindex, x = x)

dataSet = sim$simvalues

passingFrame = beezdemand::CheckUnsystematic(dataSet)

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 3

```

## Data Generating Process

A total of `r nParticipants` hypothetical data series were simulated using using the R Statistical Program [@R-base]. The specific syntax used to generate was featured in an R package that was submitted to peer-review [@kaplan2019r]. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to simulate hypothetical purchase task data that included a large composition of non-consumption values. The seed values and variance used to generate these data were identical to those that were used in @koff2015expt. This specific data generating process was used as the basis for comparisons with the @hursh2008economic model given that the authors of the @koff2015expt study modeled their approach around "messy" data frequently observed in "real-world" purchase tasks that are often conducted on "crowdsourced" platforms, e.g. Amazon's Mechanic Turk (mTurk).

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all generated demand data. Specifically, individual series were screened for *bounce*, *trend*, and *reversals from zero*. The first criterion, bounce, refers to local changes within an expected downward trend as a function of increasing price. That is, it is unexpected to see consumption increases immediately following a price increase. The second criterion, trend, refers to the molar change in consumption from the lowest to the highest price. That is, there is a certain amount of decrease in consumption expected across the full domain of price increases. Lastly, reversals from zero refer to the return of consumption at a higher price following the cessation of consumption at a lower price. Such trends are inconsistent with expected patterns of consumption. Simulated data were carried forward into the final analysis so long as each series met all indicators of systematic hypothetical purchase task data.

## Modeling Strategies

A total of 4 modeling approaches were evaluated (2 models, 2 error Interpretations). Each approach was referenced as a specific strategy for conducting demand curve analysis when non-consumption values were observed in the data. This facilitated two pairwise comparisons when both models shared a comparable approach for handling residual error. These comparisons were used to determine whether the various strategies provided statistically equivalent estimates when asymptotes and error differences were comparable. Consistent with efforts to maintain open and transparent science [@gilroy2019furthering], the source code necessary to reproduce these strategies and this report has been posted for public review in a GitHub repository managed by the corresponding author, see Author Note. Each of the strategies used in these comparisons are presented below in greater detail.

### Strategy 1: @koff2015expt Model (Absolute Error)

The @koff2015expt model (absolute error difference) was fitted to simulated consumption data at the individual-level. The model was fit using the *optim* package included in the R Statistical Program [@R-base] due to its considerable flexibility in performing ordinary least squares regression. Initial starts were derived based on the respective data for parameter $Q_0$ and both $Q_0$ and $\alpha$ were estimated on the log scale to 1) support more comparable step sizes in the optimization and 2) facilitate pairwise comparisons across strategies. The span constant $k$ was derived from the empirical range of the full data set with an added constant (3) to allow the span of the demand curve to extend below the lowest non-zero point of consumption, as is common practice [@kaplan2018understanding]. The same span constant was used across all models to enable consistent comparisons between $Q_0$ and $\alpha$. Non-consumption values remained at a value of 0 in this approach.

### Strategy 2: @koff2015expt Model (Percentage Error)

The @koff2015expt model (percentage error difference) was evaluated consistent with Strategy 1 with the exception of how differences in residual error were accommodated. In this approach, the absolute residuals simulated relative errors by referencing $\hat{y}$, i.e. $e_i = (\hat{y}- y) * \frac{1}{\hat{y}} = \frac{\hat{y}- y}{\hat{y}}$. It warrants noting that this manner of weighting error is not identical to log difference. That is, the weighting of the absolute error difference against $\hat{y}$ is equivalent to reflecting residual error as percentage difference and this corresponds with log difference only certain circumstances, i.e. $ln\frac{Y_1}{Y_2} \approx \frac{Y_2 - Y_1}{Y_1}$. Percentage-based difference is nearly identical to log difference with very small differences (e.g., 1% change) but the two diverge once the degree of difference between values grows larger (e.g., 50% change). As such, the varying approaches to reflecting relative differences are expected to vary to some degree in more extreme cases. This issue is presented more thoroughly in the Appendix of this work. Regardless, the two approaches are expected to behave comparably, but not identically, across many cases. All other parameters were estimated consistent with Strategy 1.

### Strategy 3: @hursh2008economic Model (Log Difference Error)

The @hursh2008economic model (log error difference) was fitted to simulated consumption data at the individual-level. During the fitting, non-consumption values were replaced by an $A_{Lower}$ value that was generated dynamically based on parameters $Q_0$ and $\textit{k}$ during parameter estimation. That is, a customized loss function was prepared for use with the *optim* method. As noted in Strategy 2, both Strategy 2 and Strategy 3 reflected relative differences in different ways. Specifically, the @hursh2008economic model reflected changes using log difference and may differ from percentage error. All other parameters were estimated consistent with the other strategies.

### Strategy 4: @hursh2008economic Model (Absolute Error)

The @hursh2008economic model (absolute error difference) was fitted to simulated consumption data at the individual-level as well. This strategy was identical to that of Strategy 3 with the exception of how residual error was interpreted during optimization. Consistent with Strategy 3, non-consumption values were replaced by an $A_{Lower}$ value that was generated dynamically based on parameters $Q_0$ and $\textit{k}$ during parameter estimation. A customized loss function was used to represent residual error in terms of absolute differences, i.e. $e_i = 10^{\hat{y}} - 10^{y}$. All other parameters were estimated consistent with that of the other strategies.

## Analytical Strategy

Pairwise comparisons were conducted for parameters $Q_0$ and $\alpha$ resulting from each of the four strategies while controlling for differences in how residual error was interpreted during optimization. Tests of equivalence were performed in a pairwise fashion (i.e., Strategy 1 vs. 3, Strategy 2 vs. 4) to evaluate estimates resulting from the @koff2015expt model and the @hursh2008economic model with and without modified error terms. The *tost* method in the *equivalence* R package [@robinson2016package] was used to perform two one-sided t-tests (TOSTs) with paired estimates resulting from each strategy. The focus here was not on determining *difference* between strategies but instead on determining *equivalence* between them. Across all tests, corrections were applied due to presence of repeated comparisons, i.e. $p=0.05/2=0.025$.

# Results

```{r coreAnalyses}

keepers = passingFrame %>%
  filter(TotalPass == 3 & NumPosValues > 0) %>%
  select(id) %>%
  pull()

nPassing = length(keepers)

dataFramePrep = data.frame(
  id       = 1:nPassing,
  q0.HS    = numeric(length = nPassing),
  q0.HSw   = numeric(length = nPassing),
  q0.HSw2  = numeric(length = nPassing),
  q0.Koff  = numeric(length = nPassing),
  q0.Koffw = numeric(length = nPassing),
  a.HS     = numeric(length = nPassing),
  a.HSw    = numeric(length = nPassing),
  a.HSw2   = numeric(length = nPassing),
  a.Koff   = numeric(length = nPassing),
  a.Koffw  = numeric(length = nPassing),
  K        = numeric(length = nPassing)
)

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(10^par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPT.aw <- function(data, par) {
  newData = data

  newData$ys   = getEXPT(10^par[1], kSet, 10^par[2], newData$x)
  newData$err  = (newData$y - newData$ys)/newData$ys
  newData$err  = newData$err^2

  sum(newData$err)
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(10^par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(10^par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(10^par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y    = log10(newData$y)
  newData$ys   = getEXPL(10^par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

min.RSS.EXPL.aw2 <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(10^par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(10^par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$y - 10^newData$ys)/10^newData$ys
  newData$err  = newData$err^2

  sum(newData$err)
}

#minQ0 = 0.01
#maxQ0 = 200

for (id in 1:nPassing) {

  currentData = dataSet[dataSet$id == keepers[id], ]

  dataFramePrep[id, "K"] = kSet

  # ==================================================
  # Un-Weighted sets
  # ==================================================
  
  fit.Koff <- optim(par  = c(log10(max(currentData$y)), -5),
                    fn   = min.RSS.EXPT,
                  method = "L-BFGS-B",
                  lower  = c(-6, -6),
                  upper  = c(log10(max(currentData$y) * 2), 0),
                    data = currentData)

  dataFramePrep[id, c("q0.Koff", "a.Koff")] = fit.Koff$par
  
  fit.HS   <- optim(par    = c(log10(max(currentData$y)), -5),
                    fn     = min.RSS.EXPL,
                    method = "L-BFGS-B",
                    lower  = c(-6, -6),
                    upper  = c(log10(max(currentData$y) * 3), 0),
                    data   = currentData)

  dataFramePrep[id, c("q0.HS", "a.HS")] = fit.HS$par

  # ==================================================
  # Weighted sets
  # ==================================================
  
  fit.Koffa <- optim(par    = c(log10(max(currentData$y)), -5),
                     fn     = min.RSS.EXPT.aw,
                    method = "L-BFGS-B",
                    lower  = c(-6, -6),
                    upper  = c(log10(max(currentData$y) * 2), 0),
                     data   = currentData)

  dataFramePrep[id, c("q0.Koffw", "a.Koffw")] = fit.Koffa$par

  fit.HWa <- optim(par    = c(log10(max(currentData$y)), -5),
                   fn     = min.RSS.EXPL.aw,
                  method = "L-BFGS-B",
                  lower  = c(-6, -6),
                  upper  = c(log10(max(currentData$y) * 2), 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw", "a.HSw")] = fit.HWa$par
  
  fit.HWb <- optim(par    = c(log10(max(currentData$y)), -5),
                   fn     = min.RSS.EXPL.aw2,
                  method = "L-BFGS-B",
                  lower  = c(-6, -6),
                  upper  = c(log10(max(currentData$y) * 2), 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw2", "a.HSw2")] = fit.HWb$par
}

frameToAnalyze = dataFramePrep

```

The data generating process yielded a total of `r nParticipants` distinct data sets that simulated hypothetical purchase task data. Within this number, a total of `r nrow(frameToAnalyze)` series met all 3 indicators of systematic purchase task data and contained at least 1 positive real consumption value (from *N*=`r nParticipants`; `r round((nrow(frameToAnalyze)/nParticipants) * 100, 2)`%). The results of specific pairwise comparisons are presented below.

## Strategy 1 vs. Strategy 4

```{r s1vs4fig, fig.cap="Comparisons of Strategy 1 and 4 (Absolute Error)", fig.height=4}

library(equivalence)

tost.1v4.q0 = tost(frameToAnalyze$q0.Koff, 
                   frameToAnalyze$q0.HSw, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.1v4.q0 = cor.test(frameToAnalyze$q0.Koff, 
                      frameToAnalyze$q0.HSw)

tost.1v4.a = tost(frameToAnalyze$a.Koff, 
                  frameToAnalyze$a.HSw, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.1v4.a = cor.test(frameToAnalyze$a.Koff, 
                     frameToAnalyze$a.HSw)

par(mfrow = c(1, 2),
    oma   = c(0, 0, 0, 0))

comps.base <- lm(q0.Koff ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main = expression(Q[0] ~ "Estimates"),
     col  = alpha("black", 0.1),
     ylab = "Strategy 1",
     ylim = c(0,3),
     xlab = "Strategy 4",
     xlim = c(0,3),
     frame=FALSE)
abline(comps.base)

box(bty="l")

comps.base <- lm(a.Koff ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main = expression(alpha ~ "Estimates"),
     col  = alpha("black", 0.1),
     ylab = "Strategy 1",
     ylim = c(-5,0),
     xlab = "Strategy 4",
     xlim = c(-5,0),
     frame=FALSE)
abline(comps.base)

box(bty="l")

```

Comparisons between Strategy 1 and Strategy 4 evaluated the correspondence between the @hursh2008economic and @koff2015expt models when error differences were interpreted in terms of absolute difference and when non-consumption was treated as $A_{Lower}$ for the @hursh2008economic model. An evaluation of the relationship between Strategy 1 and 4 revealed perfect correlations for both $Q_0$ (r=`r cor.1v4.q0$estimate`, t=`r cor.1v4.q0$statistic`, df=`r cor.1v4.q0$parameter`, `r scales::pvalue(cor.1v4.q0$p.value, accuracy = .025, add_p = TRUE)`) and for $\alpha$ (r=`r cor.1v4.a$estimate`, t=`r cor.1v4.a$statistic`, df=`r cor.1v4.a$parameter`, `r scales::pvalue(cor.1v4.a$p.value, accuracy = .025, add_p = TRUE)`). That is, a perfect rank ordering was observed across strategies and across parameters.

Two one-sided T-tests (TOSTs) were calculated to determine whether the results of each strategy were statistically equivalent. The results of TOSTs were significant for $Q_0$ (`r scales::pvalue(tost.1v4.q0$tost.p.value, accuracy = .025, add_p = TRUE)`) and for $\alpha$ (`r scales::pvalue(tost.1v4.a$tost.p.value, accuracy = .025, add_p = TRUE)`). Results of equivalence testing rejected the null hypothesis of statistical difference for both parameters and this indicated that estimates resulting from each strategy were statistically equivalent. A visualization of these corresponding estimates are illustrated in \autoref{fig:s1vs4fig}.

## Strategy 2 vs. Strategy 3

```{r s2vs3fig, fig.cap="Comparisons of Strategy 2 and 3 (Relative Error)", fig.height=4}

tost.2v3.q0 = tost(frameToAnalyze$q0.Koffw, 
                   frameToAnalyze$q0.HS, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.2v3.q0 = cor.test(frameToAnalyze$q0.Koffw, 
                      frameToAnalyze$q0.HS)

tost.2v3.a = tost(frameToAnalyze$a.Koffw, 
                  frameToAnalyze$a.HS, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.2v3.a = cor.test(frameToAnalyze$q0.Koffw, 
                     frameToAnalyze$q0.HS)

par(mfrow = c(1, 2),
    oma   = c(0, 0, 0, 0))

#comps.base <- lm(q0.Koffw ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koffw ~ frameToAnalyze$q0.HS,
     main = expression(Q[0] ~ "Estimates"),
     col  = alpha("black", 0.1),
     ylab = "Strategy 2",
     ylim = c(0,3),
     xlab = "Strategy 3",
     xlim = c(0,3),
     frame=FALSE)
#abline(comps.base)

box(bty="l")

#comps.base <- lm(a.Koffw ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koffw ~ frameToAnalyze$a.HS,
     main = expression(alpha ~ "Estimates"),
     col  = alpha("black", 0.1),
     ylab = "Strategy 2",
     ylim = c(-5,0),
     xlab = "Strategy 3",
     xlim = c(-5,0),
     frame=FALSE)
#abline(comps.base)

box(bty="l")


```

Comparisons between Strategy 2 and Strategy 3 evaluated the correspondence between the @hursh2008economic and @koff2015expt models when error differences were interpreted in terms of relative difference and when non-consumption was treated as $A_{Lower}$ for the @hursh2008economic model. Specifically, the @hursh2008economic model evaluated error using log difference and the @koff2015expt model evaluated error using percentage difference. An evaluation of the relationship between Strategy 2 and Strategy 3 revealed strong, but not perfect correlations for $Q_0$ (r=`r cor.2v3.q0$estimate`, t=`r cor.2v3.q0$statistic`, df=`r cor.2v3.q0$parameter`, `r scales::pvalue(cor.2v3.q0$p.value, accuracy = .025, add_p = TRUE)`) and for $\alpha$ (r=`r cor.2v3.a$estimate`, t=`r cor.2v3.a$statistic`, df=`r cor.2v3.a$parameter`, `r scales::pvalue(cor.2v3.a$p.value, accuracy = .025, add_p = TRUE)`). That is, parameter values were highly correlated but the varying approaches to determining relative difference introduced an unmeasured degree of noise into the optimization of individual parameters.

Results of TOSTs were significant for $Q_0$ (`r scales::pvalue(tost.2v3.q0$tost.p.value, accuracy = .025, add_p = TRUE)`) as well as for $\alpha$ (`r scales::pvalue(tost.2v3.a$tost.p.value, accuracy = .025, add_p = TRUE)`). These results indicated that both strategies yielded statistically equivalent model parameters for both $Q_0$ and $\alpha$. A visualization of these relationships are illustrated in \autoref{fig:s2vs3fig}.

# Discussion

The Operant Demand Framework has achieved high regard as a robust approach for evaluating choices and behavior of societal significance [@hursh2013behavioral; @reed2013behavioral]. Various labs and teams have been working towards expanding the scale and scope of this approach, moving from questions specific to individuals and groups to society at-large [@hursh2013behavioral; @roma2017progress]. This approach and its methods are increasingly represented in a range of scientific tools and packages as well [@kaplan2019r; @gilroy2018demand]. Despite increasing popularity and accessibility, there are few guidelines with which to assist analysts in navigating between options for demand curve analyses. The purpose of this technical report was to review the mathematical underpinnings of the two prevailing models derived from the framework of @hursh2008economic and present an argument as to why distinctions between these statistically equivalent approaches create more confusion than consensus.

This report provided an in-depth discussion and review of how non-consumption values (i.e., 0) have, thus far, been incorporated into models derived from the @hursh2008economic framework. As noted throughout this report, both the @hursh2008economic and the @koff2015expt approach are unable to model demand at 0 and both are restricted to a non-zero lower asymptote, $A_{Lower}$. This is the case regardless of whether non-consumption values are included in the regression. As such, the approach put forward in @koff2015expt is not a complete solution for non-consumption values because the same limitations of the original approach remain in this regard. This is because the span of the demand curve in the @hursh2008economic framework remains in the log scale, despite LHS exponentiation, and the span in log scale cannot support 0. As an alternative to this issue with span, others have argued that a true solution to this issue would require deviating from the log scale altogether [@gilroy2021zbe].

The proofs and simulations featured in this study facilitated comparisons between the @hursh2008economic and @koff2015expt models when controlling for the common $A_{Lower}$ and differences in how residual errors are interpreted during optimization. The goals of these comparisons were to advance the argument that the Exponential [@hursh2008economic] and Exponentiated [@koff2015expt] models should not be so strongly distinguished. Indeed, it is quite trivial to arrive at statistically equivalent estimates in both approaches when the role of the span constant and $A_{Lower}$ are understood as they relate to non-consumption values. The results of planned comparisons confirmed that the two approaches provide statistically equivalent estimates when controlling for such differences---even when non-consumption values are included. This is because the behavior of $A_{Lower}$ is identical regardless of which approach is used. However, it warrants noting that the two models are perfectly correlated with absolute error but just strongly correlated for relative error. This is because percentage and log differences are virtually identical at small differences (e.g., 1%) but diverge as differences grow larger. As such, differences here are unrelated to the models and how relative is interpreted in each approach.

Given that neither approach can characterize demand at 0, $A_{Lower}$ is the *best* approximation of 0 possible in the @hursh2008economic framework. Following this logic, replacing non-consumption values with respective $A_{Lower}$ values results in estimates that are statistically equivalent across the Exponential and Exponentiated models. In advancing this argument, it necessary to state clearly that this claim is not presented with the intent of favoring any specific approach as a de facto standard or a recommended default when applying methods from the Operant Demand Framework. Rather, the intent of this work was to reveal how these supposedly opposing strategies are functionally equivocal. Indeed, they are so similar that distinguishing the two only serves to obscure the shared mathematical bases for each. That said, each approach has common utility and future efforts should be directed towards improving the understanding the properties of the @hursh2008economic framework overall rather than reinforcing any stance, position, or bias towards a specific implementation.

The final aim of this work was to reiterate the ways in which the proponents of each approach have extended the Operant Demand Framework. That is, the proponents of each approach were successful in advancing both the utility and scope of the Operant Demand Framework. For example, the finding that the @hursh2008economic model can replicate the behavior of the @koff2015expt without exponentiation terms in no way detracts from the contributions of the @koff2015expt implementation of the framework. Indeed, the initiative of the @koff2015expt team warrants considerable praise even today because this led the charge towards addressing the problematic issue of removing otherwise valid research data. For decades, substantial portions of otherwise consumption data were never carried forward into analyses and it is unclear how these prior analyses would compare had these data been included. Regardless of whether analysts have a preference for one approach or the other, it is clear that the methods featured in Operant Demand Framework are better equipped now that non-consumption values can now be considered in the analysis.

## Future Directions in Operant Demand

The future is bright for the Operant Demand Framework. This perspective and this framework currently reflects a range of consumption (and non-consumption) and efforts are underway to leverage multilevel modeling as a future methodological extension [@kaplanMlm]. Indeed, various labs are working toward increasing the applicability and generality of this approach. Towards this end, the intent and mission of the original @koff2015expt study regarding non-consumption values is as valid and valuable today as it was when this work was first published. However, debates regarding model superiority (or inferiority) in the absence of formal tests and mathematical proofing does not enhance the Operant Demand Framework in any appreciable manner. That said, the two approaches are functionally interchangeable (even in the presence of non-consumption) the reader is cautioned against thinking that any single model is inherently "true", "better", or otherwise superior in the absence of careful and individualized statistical evaluation. That said, it is unclear whether the prevailing approach in the Operant Demand Framework will remain based on the framework presented in @hursh2008economic well into the future. Indeed, it is possible that future research could explore to deviations from the log scale [@gilroy2021zbe] or adopt a different framework altogether [@newman2020improved]. Regardless of the where the future takes the Operant Demand Framework, future approaches and advances should be met with cautious optimism and consideration rather than outright disregard in favor what has come before.

\newpage

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

\endgroup

\newpage

# Appendix

Several proofs are provided here to illustrate how the upper and lower asymptotes are determined. Despite the shared mathematical basis, derivations of each are provided below.

## Modified Hursh & Silberburg (2008) Optimization (Relative Error)

$$
e_i = \begin{cases} 
\hat{y}_i - log_{10}y_i       &    \mbox{if } y_i \not= 0 \\
\hat{y}_i - log_{10}A_{Lower} &    \mbox{if } y_i = 0 \\
\end{cases}
$$

## Modified Hursh & Silberburg (2008) Optimization (Absolute Error)

$$
e_i = \begin{cases} 
10^{\hat{y}_i} - 10^{log_{10}y_i}       &    \mbox{if } y_i \not= 0 \\
10^{\hat{y}_i} - 10^{log_{10}A_{Lower}} &    \mbox{if } y_i = 0 \\
\end{cases}
$$

## Hursh & Silberburg (2008) Proofs

### $A_{Upper}$ at $P = 0$

$$
\begin{aligned}
log_{10}A_{Upper} &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * 0} - 1) \\
 &= log_{10}Q_0 + k(e^{0} - 1) \\
 &= log_{10}Q_0 + k(1 - 1) \\
 &= log_{10}Q_0 + k(0) \\
 &= log_{10}Q_0\\
 A_{Upper} &= Q_0
\end{aligned}
$$

Note: Euler's constant raised to the power of 0 is equal to a value of 1. This essentially zeroes out the $\textit{k}$ constant, leaving just the $Q_0$ parameter at 0 $P$.

### $A_{Lower}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
log_{10}A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * \infty} - 1) \\
 &= log_{10}Q_0 + k(e^{-\infty} - 1) \\
 &= log_{10}Q_0 + k(0 - 1) \\
 &= log_{10}Q_0 + k(-1) \\
 &= log_{10}Q_0 - k \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$

Note: Euler's constant raised to the power of $-\infty$ equates to a value of 0. That is, $e^{-\infty}=\frac{1}{e^\infty}=\frac{1}{\infty} \approx 0$. This is has effect of making the value in parentheses equal to $-1$, which in turn results in the full subtraction of quantity $k$ from $log_{10}Q_0$.

## Koffarnus et al. (2015) Proofs

### $A_{Upper}$ at $P = 0$

$$
\begin{aligned}
A_{Upper} &= Q_0 * 10^{k(e^{-\alpha * Q_0 * 0} - 1)}  \\
 &= Q_0 * 10^{k(e^{0} - 1)}  \\
 &= Q_0 * 10^{k(1 - 1)}  \\
 &= Q_0 * 10^{k(0)}  \\
 &= Q_0 * 10^{0}  \\
 &= Q_0 * 1  \\
 &= Q_0  \\
log_{10}A_{Upper} &= log_{10}Q_0  \\
\end{aligned}
$$

### $A_{Lower}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= Q_0 * 10^{k(e^{-\alpha * Q_0 * \infty} - 1)}  \\
 &= Q_0 * 10^{k(e^{-\infty} - 1)}  \\
 &= Q_0 * 10^{k(0 - 1)}  \\
 &= Q_0 * 10^{k(-1)}  \\
 &= Q_0 * 10^{-k}  \\
log_{10}A_{Lower} &= log_{10}Q_0 + (-k)  \\
 &= log_{10}Q_0 - k  \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$ \newpage

## Differences between Log and Percentage Difference

### Logarithmic Difference

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 90
\end{aligned}
$$

$$
\begin{aligned}
ln(\frac{V_2}{V_1})      & = -1 * ln(\frac{V_1}{V_2}) \\
ln(\frac{90}{100})       & = -1 * ln(\frac{100}{90}) \\
ln(0.9)                  & = -1 * ln(1.11) \\
-0.1053                  & = -1 * 0.1053 \\
-0.1053                  & = -0.1053
\end{aligned}
$$

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 50
\end{aligned}
$$

$$
\begin{aligned}
ln(\frac{V_2}{V_1})      & = -1 * ln(\frac{V_1}{V_2}) \\
ln(\frac{50}{100})       & = -1 * ln(\frac{100}{50}) \\
ln(0.5)                  & = -1 * ln(2) \\
-0.6931                  & = -1 * 0.6931 \\
-0.6931                  & = -0.6931
\end{aligned}
$$

\newpage

### Percentage Difference

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 90
\end{aligned}
$$

$$
\begin{aligned}
\frac{ V_2 - V_1}{V_1} & \approx -1 *\frac{ V_1 - V_2}{V_2} \\
\frac{ 90 - 100}{100}  & \approx -1 *\frac{ 100 - 90}{90} \\
\frac{ -10}{100}        & \approx -1 *\frac{ 10}{90} \\
-0.1                    & \approx -1 * 0.11 \\
-0.1                    & \approx -0.11
\end{aligned}
$$

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 50
\end{aligned}
$$

$$
\begin{aligned}
\frac{ V_2 - V_1}{V_1} & \approx -1 *\frac{ V_1 - V_2}{V_2} \\
\frac{ 50 - 100}{100}  & \approx -1 *\frac{ 100 - 50}{50} \\
\frac{ -50}{100}       & \approx -1 *\frac{ 50}{50} \\
-0.5                   & \approx -1 * 1 \\
-0.5                   & \approx -1
\end{aligned}
$$ Note: The examples provided above illustrate how log difference $(-0.1053)$ and percentage difference $(-0.1)$ are quite close for small differences. However, the difference between log difference $(-0.6931)$ and percentage difference $(-0.5)$ begins to differ considerably with larger changes. As such, the two approaches to reflecting relative differences are unlikely to be perfectly related outside of optimal conditions.
