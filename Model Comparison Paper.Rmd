---
title             : "Split Hairs in the Operant Demand Framework: Clarifying Methods of Accomodating Zero in Demand Curve Analysis"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn P. Gilroy"
    affiliation   : "1"
    corresponding : yes
    address       : "226 Audubon Hall Baton Rouge, Louisiana 70806"
    email         : "sgilroy1@lsu.edu"
    role:
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
  Shawn Patrick Gilroy is an Assistant Professor of School Psychology at Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu. The source code necessary to recreate this work is publicly hosted in a repository at: https://github.com/miyamot0/AgnosticDemandModeling 

abstract: |
  Operant translations of Behavioral Economic concepts and principles have enhanced the ability of researchers to characterize individual choice under constraints and uncertainty. Operant Behavioral Economic models of concurrent choices (i.e., demand) have been particularly useful in evaluating how individual choices are affected by various ecological factors (e.g., price, limited resources). Prevailing perspectives in Operant Demand are derived from the framework of Hursh & Silberburg (2008). Few dispute the utility of this framework, though considerable debate continues regarding how best to address limitations associated with the logarithmic scale. At present, there are opposing views regarding the handling of zero values and under which situations alternative restatements of this framework are warranted, c.f. Koffarnus et al. (2015). The purpose of this report is to review the importance of asymptotes in the Operant Demand Framework and the relatively minor ways in which the Hursh & Silberburg (2008) and Koffarnus et al. (2015) differ. A simulation study is provided as a demonstration of how results from the two models can be largely interchangeable when addressing asymptote and error differences. Additional discussion is provided on whether having multiple models extends the Operant Demand Framework.

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
---

\raggedbottom

```{r include = FALSE}

library(beezdemand)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(parameters)
library(tidyverse)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Contemporary methods for evaluating the consumption of goods and services using the Operant Demand Framework draw heavily from the methodology proposed in @hursh2008economic. This framework and this methodology have evolved through several iterations [@hursh1987linear], with the most recent form taking a non-linear shape and using an exponential decay process. Beyond the @hursh2008economic specifically, the more recent extensions of the @hursh2008economic framework have also explored variants that evaluate consumption on the linear [@koff2015expt] or other log-like scales [@gilroy2021zbe].

The original implementation of the @hursh2008economic framework was constructed to predict an overall "s-type" form as the prototypical shape of the demand curve. Indeed, the original intent of @hursh2008economic was to have an upper asymptote reached at a low (or free) price and a lower asymptote reached as prices approached infinity [@gilroy2021zbe]. Until most recently, all models derived this framework [@hursh2008economic, @koff2015expt] evaluated the demand for reinforcers with these non-zero upper and lower asymptotes. That is, these models were bounded at an upper limit (i.e., $Q_0$) and progressed towards a lower asymptote in the roughly S-type form. The non-zero asymptotes in the @hursh2008economic framework make good sense because the traditional values of interest were positive real values (i.e., not 0). Such quantities were necessary because the logarithmic representation of consumption is naturally undefined.

In response to statistical and philosophical issues related to the omission of 0 consumption values, @koff2015expt introduced a restatement of the @hursh2008economic framework. This variant of the @hursh2008economic model was adjusted to examine changes in observed consumption on the linear scale. Specifically, an exponentiation of terms was performed such that the LHS (left-hand side) of the @hursh2008economic model were reflected in the linear scale. In this way, the LHS of the model (i.e., consumption) need not be submitted to the log transformation that previous prevented the use of the @hursh2008economic model. This modification drew considerable attention, as one of the largest issues associated with the log scale can be avoided. However, it warrants noting that portions of the RHS (right-hand side) of the @koff2015expt model remain on the log scale. Of particular importance, the span of the demand curve and the rate of exponential decay remain reflected in the log scale. It is for this reason that the span of the demand curve in this restated model cannot reach a a true 0 point, and thus, is also bound to non-zero upper and lower asymptotes. Furthermore, the regressive process for logarithmic and linear models differs with respect to how error is weighted and this also introduces behavior that differs from that of the original @hursh2008economic framework.

## Same Model, Different Error

The various challenges associated with fitting models of operant demand (i.e., minimizing residual error) are increasingly discussed in the Operant Demand Framework [@gilroy2021zbe]. @gilroy2021zbe, among various topics, noted how residual error is reflected differently in log and linear scales and how such differences affect model optimization. Briefly, changes in log space reflect relative differences while changes in the linear space reflect absolute differences. In most economic applications, relative error is typically preferred because quantities of interest and their projections (i.e., $\hat{y}$) often span multiple orders and quantities observed at higher orders would naturally be weighted more heavily than those at lower orders in the linear scale (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). For this reason, relative difference is often the default because and this approach balances how each piece of information is weighted in the model. 

As an alternative to relative differences, which are weighted against observed values, absolute differences are more straightforward. That is, absolute error is simply the difference from some observed quantity and $\hat{y}$ regardless of the order (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). @gilroy2021zbe discussed how the departure from relative error can affect model optimization when observed levels of consumption vary considerably across orders. This has led to occasional inconsistencies wherein model fits across the log and linear variants of @hursh2008economic framework lead to different conclusions (e.g., shared alpha across varying dose-response curves).

## Different Error, Same Asymptotes

Apart from differences in how error is minimized across models in the Operant Demand Framework, there has been renewed attention regarding the upper and lower asymptotes of these models [@gilroy2021zbe]. Specifically, @gilroy2021zbe highlighted how neither the @hursh2008economic nor @koff2015expt models could characterize demand at 0. The topic of the zero-asymptote is not discussed at length here and readers are encouraged to review this work for an exposition on why the inability to model demand at zero limits the utility of the Operant Demand Framework.

Revisiting the topic of asymptotes, two novel terms are noted in this work: $A_{Upper}$ and $A_{Lower}$. The term $A_{Upper}$ refers to the upper, typically non-zero bound of the demand curve. In models derived from the @hursh2008economic framework, this is simply the fitted parameter $Q_{0}$. That is, this is the absolute upper limit to the demand curve as prices approach 0, $\displaystyle{\lim_{P \to 0} f(x)= A_{Upper}}$. In contrast, the term $A_{Lower}$ refers the absolute lower bound of the demand curve. That is, this is the absolute lower limit to the demand curve as prices approach $\infty$, $\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}$. These two upper and lower extremes are separates by the span constant $\textit{k}$, which specify the distance in log units between these asymptotes. The derivation of both $A_{Upper}$ and $A_{Lower}$ are noted below.

\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}

Further inspection of the $A_{Lower}$ and its derivation naturally evoke questions regarding how 0 consumption values could be evaluated in the @hursh2008economic framework. As noted in the bottom portion of *Equation 1*, both the @hursh2008economic and @koff2015expt models can *never* take a value of 0 because both asymptotes exist in a space where such a value cannot exist. As such, all predictions from these model must take values between these limits, $\hat{y} \in [ A_{Lower}, A_{Upper} ]$.

## Same Asymptotes, Same Spans

Revisiting the *Equation 1*, this notation highlights the heavy influence of the span parameter $\textit{k}$ and how it affects the values that can be predicted ($\hat{y}$). In the original implementation of the @hursh2008economic framework, parameter $\textit{k}$ was specified based on the range of *observed consumption*. Since 0 consumption values were omitted from this implementation, parameter $\textit{k}$ was directly linked to the observed data. Indeed, dropping the 0 values made the specification of the span straightforward. That is, both parameter $\textit{k}$ and $A_{Lower}$ could be directly matched to the lowest observed non-zero levels of consumption. A visualization of the specification of parameter $\textit{k}$ is provided in \autoref{fig:fig1CurveSpanEmpirical}.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes") +
  geom_point() +
  annotate("segment", 
           x      = 0.1, 
           xend   = 0.1, 
           y      = 100, 
           yend   = lowerAsymptote,
           colour = "red") +
  annotate("text", 
           x     = 0.175, 
           y     = 55, 
           angle = 90,
           adj   = 0.5,
           col   = "red",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = lowerAsymptote, 
           yend   = lowerAsymptote,
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "red",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = max(preFit$Q), 
           yend   = max(preFit$Q),
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "red",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw()

```

Whereas the determination of parameter $\textit{k}$ in the @hursh2008economic implementation is more directly linked to observed (non-zero) consumption data, the determination of parameter $\textit{k}$ became more abstract in the implementation introduced by @koff2015expt. That is, parameter $\textit{k}$ could not longer be linked to observed (non-zero) consumption data because $A_{Lower}$ would not extend beyond the range of non-zero consumption. Anecdotally, various labs and teams using this model have either opted to either add a small constant to parameter $\textit{k}$ or fit this as a free parameter--both with the intent of *increasing* the span, and thus, driving $A_{Lower}$ to a point closer to 0 on the linear scale. A visualization of this behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  annotate("text",
           x     = 5000,
           y     = 90,
           adj   = 1,
           col   = "red",
           parse = TRUE,
           label = expression("K (Empirical): " ~ A[LOWER] == 5.442)) +
  annotate("text",
           x     = 5000,
           y     = 80,
           adj   = 1,
           col   = "yellow",
           parse = TRUE,
           label = expression("K + 0.5: " ~ A[LOWER] == 1.7209)) +
  annotate("text",
           x     = 5000,
           y     = 70,
           adj   = 1,
           col   = "green",
           parse = TRUE,
           label = expression("K + 1: " ~ A[LOWER] == 0.5442)) +
  annotate("text",
           x     = 5000,
           y     = 60,
           adj   = 1,
           col   = "blue",
           parse = TRUE,
           label = expression("K + 2: " ~ A[LOWER] == 0.0544)) +
  annotate("text",
           x     = 5000,
           y     = 50,
           adj   = 1,
           col   = "purple",
           parse = TRUE,
           label = expression("K + 3: " ~ A[LOWER] == 0.0054)) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw() +
  theme(legend.position = "bottom")

```

\autoref{fig:fig2CurveSpanEmpiricalMod} illustrates how inflating the span constant affects $A_{Lower}$. This has two significant effects on the demand curve. First, the rate of change in elasticity is jointly reflected by parameter $\alpha$ and the span of the demand curve. Given that $\alpha$ is unitless, it covaries inversely with the size of the span constant--with relatively greater values reflecting rapid changes in $\hat{y}$ across prices and relatively lesser values reflecting gradual changes in $\hat{y}$ across prices. Second, and specific to the @koff2015expt model, larger $\textit{k}$ values serve to lessen the absolute difference between $A_{Lower}$ and true 0--a point that cannot be characterized in the model. \autoref{fig:fig2CurveSpanEmpiricalMod} elegantly displays how this gap decreases proportionally with each unit increase in the span parameter $\textit{k}$.

## Unnecessary Distinctions

The areas above serve to highlight the minimal ways in which the @hursh2008economic and @koff2015expt models differ. Admittedly, these approaches differ in terms of optimization but ultimately share the same limitations with respect to a non-zero lower asymptote. Regarding the first point, error representation and optimization, the two models can provide functionally identical results when error handling is made comparable. That is, re-weighting the errors (i.e., relative to $\hat{y}$) in the @koff2015expt model yields fits and estimates that are nearly identical to that of the @hursh2008economic model in the absence of 0 values in consumption, \autoref{fig:fig3ComparisonNonzeroRelativeError}. That is, in the presence of non-zero data, there is little need to extend beyond the original implementation of the @hursh2008economic framework.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error", fig.height=3, warning=FALSE}

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           weights = 1/(predict(EXPT)^2),
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  rename(`Exponential`   = explPred,
         `Exponentiated (Absolute Error)` = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred) 

# kable(preFit.trim) %>%
#   kable_styling(full_width = TRUE)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P) 

ggplot(preFit.trim, aes(P, Q, color = Model)) +
  #ggtitle("TODO: Add title") +
  geom_point(color = "black") +
  geom_line(aes(P, Prediction)) +
  facet_wrap(~Model, ncol = 3) +
  scale_y_log10(breaks = c(1, 10, 100),
                labels = c(1, 10, 100),
                limits = c(1, 100)) +
  theme_bw() +
  theme(legend.position = "bottom")

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

```

Regarding the second point, few researchers have explored $A_{Lower}$ and how this feature of the demand curve differs across series with and without 0 consumption values. This is a complex topic, especially in the linear restatement 0 is an observed quantity than cannot be characterized by either model. Evaluations of this model and its the behavior have revealed that optimization of the @koff2015expt model minimizes differences between 0 and $\hat{y}$ by using $\textit{k}$ that extend beyond the non-zero range in an effort to minimize the impassible absolute distance between $A_{Lower}$ and 0. Pragmatically, one would argue that such a small amount of error calls for little concern and that values at $A_{Lower}$ are considered *close enough* to approximate 0 consumption in demand curve analysis.

Revisiting the second point, consider the following logic. For the sake of argument, let us say that the @koff2015expt model considers values at $A_{Lower}$ to be *close enough* to 0 to reasonably approximate consumption at such levels. That is, $A_{Lower}$ is considered sufficiently approximate to 0. Following this logic, it stands to reason that treating 0 consumption values as $A_{Lower}$ is equally appropriate because 1) this allows demand to be projected beyond observed non-zero levels (i.e., accomodates 0 values) and 2) it is close enough to 0 that the absolute differences are considered negligible. In this arrangement, it stands to reason that the @koff2015expt model should perform in the same ways as the @hursh2008economic model, were errors also treated in terms of absolute difference. Using the full data displayed in \autoref{fig:fig1CurveSpanEmpirical}, let us fit the @koff2015expt model using the full dataset and the @hursh2008economic model with the zeroes replaced by the lowest $A_{Lower}$ illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}, i.e. $10^{log{10}Q_0 - (k + 3)}$. The results of these hypothetical analyses are illustrated in \autoref{fig:fig4ComparisonZeroRelative}. Across these series, the @hursh2008economic model (with and without relative error weighting) provide nearly identical fits to the @koff2015expt model. Additionally, de-weighting the @hursh2008economic to minimize absolute differences produces an even closer approximation between the two, see \autoref{tab:table1ComparisonZeroAbsolute}.

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes", fig.height=3.5}

ggplot(preFit.g, aes(P, Q, color = Model)) +
  geom_point() +
  geom_line(aes(P, Prediction)) +
  scale_x_log10(breaks = c(0.1, 10, 100, 1000, 10000),
                labels = c(0.1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  scale_y_log10(breaks = c(0.001, 0.01, 0.1, 1, 10, 100),
                labels = c(0.001, 0.01, 0.1, 1, 10, 100),
                limits = c(0.001, 100)) +
  facet_wrap(~Model, ncol = 3) +
  theme_bw() +
  theme(legend.position = "bottom")

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

This brief example serves to highlight a core limitation of the @koff2015expt model--it cannot characterize demand at zero and is functionally identical to the @hursh2008economic model with 0 values replaced with $A_{Lower}$ using de-weighted residuals. Said another way, the approach put forward in @koff2015expt implicitly uses an inflated $\textit{k}$ parameter to drive $A_{Lower}$ close enough to 0 that most users would not be aware of the non-zero lower asymptote. 

## Planned Comparisons

The purpose of this technical report was to better clarify how prevailing models in the @hursh2008economic framework essentially function in the same way, but with different methods of addressing error. The question of this study was whether data sets containing 0 consumption values could be fitted and interpreted equivalently across models when replacing 0 consumption with the same $A_{Lower}$.

# Methods

```{r }

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 100

sim <- SimulateDemand(nruns = nParticipants, setparams = setparams, sdindex = sdindex, x = x)

dataSet = sim$simvalues

passingFrame = beezdemand::CheckUnsystematic(dataSet)

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 0.5

```

## Data Generating Process

A total of `r nParticipants` hypothetical data series were simulated using software submitted to peer-review. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to generate demand series data. The seed values and variance used to generate hypothetical purchase task data were identical to those that were used in the earlier @koff2015expt study. This specific data generating process was used as the basis for comparisons with the @hursh2008economic model to further evaluate the shared basis for the two models.

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all hypothetical series. Specifically, individual series were screened for bounce, trend, and reversals from zero. The first criterion, bounce, refers to local changes within the *expected* downward trend across increasing prices. That is, it would be unexpected to see consumption increases immediately following a price increase. The second criterion, trend, refers to the molar change in consumption from the lowest to the highest price. That is, there is a certain amount of decrease expected across the full run of consumption across prices. Lastly, reversals from zero refer to the return of consumption at a higher price following the cessation of consumptoin at a lower price. Simulated data were carried forward into the final analysis so long as each series met all indicators of systematic hypothetical purchase task data.

## Modeling Strategies

A total of three different demand curve modeling strategies were evaluated. Each of the three approaches referred to a different strategy for conducting demand curve analysis when 0 values were observed in the data. The core aim was to evaluate estimated parameters across each strategy to determine whether the varying strategies provide equivalent results. Each of the modeling strategies is listed below in greater detail.

### Strategy 1: @koff2015expt

The @koff2015expt model (absolute error) was applied to simulated consumption data at the individual-level. The model was fit using the *optim* package in the R Statistical Program [@R-base] due to its considerable flexibility in performing ordinary least squares regression. Initial starts were derived based on the respective data for parameter $Q_0$ and $\alpha$ was estimated on the log scale to support more equal step sizes in the optimization steps. The span constant $k$ was derived from the empirical range of the data with an added constant (0.5) to extend slightly below the lowest non-zero point of consumption, as is common practice [@kaplan2018understanding]. The same span constant was used across all models to ensure comparable fits.

### Strategy 2: @hursh2008economic Model (Relative Error)

The @hursh2008economic model (relative error) was applied to simulated consumption data at the individual-level. During the fitting, 0 consumption values were replaced by a $A_{Lower}$ value generated dynamically based on parameter $Q_0$ and $\textit{k}$. That is, a customized loss function was prepared for use with the *optim* method. In efforts to maintain transparency, the source code necessary to reproduce this approach and these calculations have been posted  for public review by the author at [https://github.com/miyamot0/AgnosticDemandModeling](https://github.com/miyamot0/AgnosticDemandModeling). All other parameters were derived consistent with that of Strategy 1.

### Strategy 3 : @hursh2008economic Model (Absolute Error)

The @hursh2008economic (absolute error) was applied to simulated consumption data at the individual-level. This strategy was identical to Strategy 2 with the exception of how loss was interpreted during optimization. That is, a customized loss function was prepared and residual error was represented in terms of absolute differences, i.e. $(10^{\hat{y}} - 10^{y})^2$. All other parameters were derived consistent with that of Strategies 1 and 2.

## Analytical Strategy

Individual comparisons were prepared for both parameters $Q_0$ and $\alpha$ resulting from each of the models. Given that both the @hursh2008economic and @koff2015expt models emerge from the same framework, tests of equivalence were performed in a pairwise fashion between Strategies 1 and 2 and 1 and 3. The *tost* method in the *equivalence* R package [@robinson2016package] was used to perform two one-sided t-tests (TOST) with paired estimates resulting from each strategy. Corrections were applied in each TOST due to repeated comparisons, $p = 0.05/4=0.0125$.

# Results

```{r fig4, fig.cap="Comparisons of Modeling Strategies", fig.height=9}

dataFramePrep = data.frame(
  id      = 1:nParticipants,
  q0.HS   = numeric(length = nParticipants),
  q0.HSw  = numeric(length = nParticipants),
  q0.Koff = numeric(length = nParticipants),
  a.HS    = numeric(length = nParticipants),
  a.HSw   = numeric(length = nParticipants),
  a.Koff  = numeric(length = nParticipants),
  K       = numeric(length = nParticipants)
)

###

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

###

minQ0 = 0.01
maxQ0 = 200

for (id in 1:nParticipants) {

  currentData = dataSet[dataSet$id == id, ]

  dataFramePrep[id, "K"] = kSet

  fit.Koff <- optim(par  = c(max(currentData$y), -3),
                    fn   = min.RSS.EXPT,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data = currentData)

  dataFramePrep[id, c("q0.Koff", "a.Koff")] = fit.Koff$par

  fit.HS   <- optim(par    = c(fit.Koff$par[1], fit.Koff$par[2]),
                    fn     = min.RSS.EXPL,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data   = currentData)

  dataFramePrep[id, c("q0.HS", "a.HS")] = fit.HS$par

  fit.HWa <- optim(par    = c(fit.HS$par[1], fit.HS$par[2]),
                   fn     = min.RSS.EXPL.aw,
                   method = "L-BFGS-B",
                   lower  = c(0.01, -5),
                   upper  = c(max(currentData$y) * 1.25, 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw", "a.HSw")] = fit.HWa$par
}

keepers = passingFrame %>%
  filter(TotalPass == 3) %>%
  select(id) %>%
  pull()

frameToAnalyze = dataFramePrep %>%
  filter(id %in% keepers)

par(mfrow = c(2, 2),
    oma = c(0, 0, 0, 0))

comps.base <- lm(q0.Koff ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HS,
     main = expression(A[LOWER]~ "with Relative Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.base)

comps.weight <- lm(q0.Koff ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main = expression(A[LOWER]~"with Absolute Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.weight)

comps.base <- lm(a.Koff ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HS,
     main = expression(A[LOWER]~"with Relative Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.base)

comps.weight <- lm(a.Koff ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main = expression(A[LOWER]~ "with Absolute Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.weight)


```

The simulated data revealed a total of `r nrow(frameToAnalyze)` that met all criteria for systematic hypothetical purchase task data (from *N*=100). Scatterplot comparisons across each of the three modeling strategies are illustrated in \autoref{fig:fig4}. As illustrated in this figure, each of the three strategies varied with respect to correspondence between strategies. 

## Strategy 1 vs. Strategy 2

```{r strat1vsstrat2, include=FALSE}

### TODO: Addin in

library(equivalence)

print("Q0s")
tost(frameToAnalyze$q0.Koff, 
     frameToAnalyze$q0.HS, 
     epsilon = 1, 
     conf.level = 0.9875,
     paired = TRUE)

cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HS)

print("As")
tost(frameToAnalyze$a.Koff, 
     frameToAnalyze$a.HS, 
     epsilon = 1, 
     conf.level = 0.9875,
     paired = TRUE)

cor.test(frameToAnalyze$a.Koff, frameToAnalyze$a.HS)
```

An evaluation of the relationship between Strategy 1 and 2 revealed strong correlations for $Q_0$ (r = 0.8955, t = 17.313, df = 74, p-value < 2.2e-16) as well as for $\alpha$ (r = 0.9344, t = 22.515, df = 74, p-value < 2.2e-16). The results of TOSTs for $Q_0$ and $\alpha$ were non-significant (*p* = 0.244) and significant (*p* < 2.2e-16), respectively. That is, the results of the two strategies appeared to provide equivalent estimates for $\alpha$ but not for $Q_0$.

## Strategy 1 vs. Strategy 3

```{r strat1vsstrat3}
### TODO: Addin in

print("Q0s (w)")
tost(frameToAnalyze$q0.Koff, 
     frameToAnalyze$q0.HSw, 
     epsilon = 1, 
     conf.level = 0.9875,
     paired = TRUE)

cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

print("As (w)")
tost(frameToAnalyze$a.Koff, 
     frameToAnalyze$a.HSw, 
     epsilon = 1, 
     conf.level = 0.9875,
     paired = TRUE)

cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

```

```{r }
### TODO: Addin in
```

Comparisons between Strategy 2 and 3 revealed a perfect correlation for $Q_0$ (r = 1, t = 82678, df = 74, p-value < 2.2e-16). Similarly, there was also a near perfect correlation for $\alpha$ (r = 0.9999, t = 22.515, df = 74, p-value < 2.2e-16). Scatterplot visualization of these two strategie indicated that, overall, the two approaches corresponded near perfectly.

# Discussion

The purpose of this technical report was to further explore the role of zero and non-zero asymptotes in two popular models of operant demand. Indeed, debate as to whether 0 consumption values contribute to the characterization of demand continue to date. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
