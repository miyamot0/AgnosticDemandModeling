---
title             : "Hidden Equivalence in the Operant Demand Framework: A Review and Evaluation of Multiple Methods for Evaluating Non-Consumption"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn P. Gilroy"
    affiliation   : "1"
    corresponding : yes
    address       : "226 Audubon Hall Baton Rouge, Louisiana 70806"
    email         : "sgilroy1@lsu.edu"
    role:
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
 Shawn Patrick Gilroy is an Assistant Professor of School Psychology at Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu. The source code necessary to recreate this work is publicly hosted in a repository at: https://github.com/miyamot0/AgnosticDemandModeling 

abstract: |
 Operant translations of Behavioral Economic concepts and principles have enhanced the ability of researchers to characterize the effects of reinforcers under constraints and uncertainty. Operant Behavioral Economic models of choice, (i.e., Operant Demand) have been particularly useful in evaluating how the consumption of reinforcers is affected by various ecological factors (e.g., price, limited resources). Prevailing perspectives in the Operant Demand Framework are derived from the framework presented in Hursh & Silberburg (2008). Few dispute the utility of this framework and model, though debate continues regarding how to address the limitations associated with the logarithmic scale in this framework. At present, there are competing views regarding the handling of non-consumption (i.e., 0 consumption values) and under which situations alternative restatements of this framework are recommended, cf. Koffarnus et al. (2015). The purpose of this report was to review the shared mathematical bases for the Hursh & Silberburg (2008) and Koffarnus et al. (2015) models and how each can accomodate 0 consumption values. Simulations derived from those featured in Koffarnus et al. (2015) were used to conduct tests of equivalence between modeling strategies while controlling for interpretations of residual error and the absolute lower asymptotes. Simulations and proofs are provided to illustrate how neither the Hursh & Silberburg (2008) nor Koffarnus et al. (2015) can characterize 0 and how both ultimately arrive at the same upper and lower asymptotes. Additional discussions are provided regarding whether debates and comparisons between between models in the same framework enhances the generality and utility of methods in the Operant Demand Framework.

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : yes

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
---

\raggedbottom

```{r include = FALSE}
library(beezdemand)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(parameters)
library(scales)
library(tidyverse)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Contemporary methods for evaluating the consumption of goods and services using the Operant Demand Framework are heavily influenced by the methodology proposed in @hursh2008economic. This framework and methodology has evolved through several forms [@hursh1987linear] and the latest iteration takes a non-linear (i.e., "S"-type) shape driven by an exponential decay process [@hursh2008economic]. This strategy for evaluating the effects of unit price on the consumption of reinforcers has achieved widespread adoption and has also inspired derivatives that model consumption on varying scales, e.g., linear [@koff2015expt], "log-like" [@gilroy2021zbe].

The original implementation of the @hursh2008economic framework was modeled from the notion that the prototypical shape of the demand curve was an "S"-type form bounded by upper and lower limits. The original intent of @hursh2008economic was to have an upper asymptote defined at a price of zero, i.e. $\lim_{P \to - \infty}$, and a lower asymptote reached as prices approached infinity, $\lim_{P \to \infty}$ [@gilroy2021zbe]. Models derived from this framework (e.g., @hursh2008economic, @koff2015expt) evaluate the demand for reinforcers with non-zero upper and lower asymptotes and these models are bounded at an upper limit (i.e., $Q_0$) and progress towards a non-zero lower asymptote in the "S"-type form. Non-zero asymptotes in models derived from the @hursh2008economic framework make good sense because the original values of interest were positive real values (i.e., not 0). Such quantities are expected because the logarithmic representation of consumption is undefined at 0.

In response to the statistical and philosophical issues related to the omission of 0 consumption values, @koff2015expt introduced a restatement of the @hursh2008economic framework designed to accommodate non-consumption (i.e., 0 consumption values) in non-linear regression. Specifically, this variant of the @hursh2008economic framework was adjusted to examine changes in observed consumption on the linear scale. An exponentiation of terms was performed such that the LHS (left-hand side) of the @hursh2008economic model was reflected in the linear scale to support the inclusion of non-consumption in the regression. In this way, the LHS of the model (i.e., observed consumption) need not be submitted to the log transformation that prevented the use of the @hursh2008economic model with non-consumption. This alternative approach drew considerable attention, as one of the largest issues associated with the log scale could be avoided. However, it warrants noting that most of the RHS (right-hand side) of the @koff2015expt model remained on the log scale. For instance, the span of the demand curve and the rate of exponential decay remain in the log scale [@gilroy2021zbe]. For this reason, the span of the demand curve in this restated model cannot reach a true 0 point, and thus, is also bound by non-zero asymptotes despite accommodating 0 consumption values in model regression. Furthermore, the regressive process for logarithmic and linear models differ with respect to how residual error is interpreted and this introduces behavior that differs between models [@gilroy2021zbe].

## Same Model But Different Error

The challenges associated with fitting models of operant demand (i.e., minimizing residual error) are increasingly discussed by researchers applying the Operant Demand Framework [@gilroy2021zbe]. @gilroy2021zbe noted, among other things, that residual error is reflected differently in log and linear scales and that such differences can affect model optimization and resulting parameters. Briefly, changes in log space reflect relative differences while changes in the linear space reflect absolute differences. In most economic applications, relative error is preferred because the quantities of interest and their associated projections (i.e., $\hat{y}$) typically span across multiple orders. In these situations, quantities observed at higher orders would naturally be weighted more heavily than those at lower orders in the linear scale (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). Given this uneven weighting of information in the model, relative difference is typically the default because this treats error in a way balances how information is weighted in the model.

As an alternative to relative difference, more accurately referred to as log difference in this case, absolute difference is straightforward. That is, absolute error is simply the difference from some observed quantity and $\hat{y}$ regardless of the order (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). Although simpler, the use of the linear scale in demand introduces some variability in how parameters are optimized in these models. For example, @gilroy2021zbe discussed how a departure from relative difference can affect model optimization when observed levels of consumption vary considerably across orders. This has led to occasional inconsistencies wherein estimates across derivatives of the @hursh2008economic framework lead to different conclusions (e.g., shared alpha across varying dose-response curves). As such, differences in residual error is one dimension along which derivatives of the @hursh2008economic framework differ.

## Different Error But Same Asymptotes

There has been renewed attention to the non-zero asymptotes in models derived from the @hursh2008economic framework [@gilroy2021zbe]. Specifically, inspection of the @hursh2008economic and the @koff2015expt models reveals that neither model can characterize demand at 0. This is the case even when non-consumption values can be included in the regression. The argument for a true 0 asymptote is not discussed here, though interested readers are encouraged to review @gilroy2021zbe for an exposition on why the inability to model demand at 0 limits the utility of the Operant Demand Framework.

Revisiting the topic of asymptotes, two novel terms are introduced in this work: $A_{Upper}$ and $A_{Lower}$. The term $A_{Upper}$ is used to refer to the absolute upper bound of the demand curve. In models derived from the @hursh2008economic framework, this is simply the fitted parameter $Q_{0}$. This is because parameter $Q_{0}$ is the absolute upper limit to the demand curve when prices equals 0, i.e. $\displaystyle{f(0)= A_{Upper}}$. In contrast, the term $A_{Lower}$ refers the absolute lower bound of the demand curve and this is not reflected by any *single* paramter. Mathematically, this absolute lower limit refers to the level of demand as price approaches $\infty$, i.e.$\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}$. These two upper and lower extremes are separated by the span constant $\textit{k}$, which specifies the distance in log units between these asymptotes [@gilroy2021zbe]. The notation of both $A_{Upper}$ and $A_{Lower}$ are noted below and are proofed in greater detail across models in the Appendix of this work.

```{=tex}
\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}
```
Further inspection of $A_{Lower}$ and its derivation evokes questions regarding how any model based on the @hursh2008economic framework could characterize non-consumption values. As noted in the bottom portion of *Equation 1*, both the @hursh2008economic and @koff2015expt models can never represent a value of 0 because such a value cannot exist between these upper and lower extremes. This introduces a complex situation wherein 0 consumption values could be included in non-linear regression, but levels of demand could never characterize this value. As such, similarities in non-zero asymptotes is one dimension along which derivatives of the @hursh2008economic framework are the same.

## Same Asymptotes and Same Spans

Understanding non-consumption in the @hursh2008economic framework requires a clear grasp of how the span parameter $\textit{k}$ influences the range of values that may be predicted (i.e., $\hat{y}$). In the original implementation of the @hursh2008economic framework, $\textit{k}$ represented the range of observed, non-zero consumption values. That is, $\textit{k}$ was derived in log units from the upper and lower extremes of all positive, real numbers. Since 0 consumption values were not included in the original implementation, parameter $\textit{k}$ was directly linked to the upper and lower limits of the observed data. As such, the specification of this constant was straightforward and parameter $\textit{k}$, $A_{Upper}$, and $A_{Lower}$ were directly linked to positive real numbers. A visualization of parameter $\textit{k}$ is provided in \autoref{fig:fig1CurveSpanEmpirical} with respect to positive real consumption values.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes") +
  geom_point() +
  annotate("segment", 
           x      = 0.1, 
           xend   = 0.1, 
           y      = 100, 
           yend   = lowerAsymptote,
           colour = "grey40") +
  annotate("text", 
           x     = 0.175, 
           y     = 55, 
           angle = 90,
           adj   = 0.5,
           col   = "grey40",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = lowerAsymptote, 
           yend   = lowerAsymptote,
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "grey40",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = max(preFit$Q), 
           yend   = max(preFit$Q),
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "grey40",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = c(preFit$P, 10000),
                labels = c(preFit$P, 10000),
                limits = c(0.1, 10000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

```

Whereas the determination of parameter $\textit{k}$ in the @hursh2008economic implementation is linked to positive real numbers, the interpretation of parameter $\textit{k}$ became more complicated in the implementation introduced by @koff2015expt. This added complexity emerged because parameter $\textit{k}$ was still based on positive real numbers but had to be inflated to project $A_{Lower}$ *beyond* the range of positive real values to a quantity nearer to 0. This represented novel behavior for parameter $\textit{k}$ and various teams have constructed strategies to assist in driving $A_{Lower}$ beyond the range of non-zero consumption. For example, some have added a constant to parameter $\textit{k}$ (derived from positive real values) or allowed this parameter to vary as a fitted parameter [@kaplan2018understanding]. Regardless of the method, the rationale was to inflate the span of the demand curve to and drive $A_{Lower}$ to a lower point. A visualization of this span-inflating behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE, fig.height=3.5}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  scale_color_manual(values = c(
    "K (Empirical)" = "grey80",
    "K + 0.5"       = "grey60",
    "K + 1"         = "grey40",
    "K + 2"         = "grey20",
    "K + 3"         = "grey0"
  )) +
  annotate("text",
           x     = 100,
           y     = 90,
           adj   = 0,
           col   = "grey80",
           parse = TRUE,
           label = expression("K (Empirical): " ~ A[LOWER] == 5.442)) +
  annotate("text",
           x     = 100,
           y     = 75,
           adj   = 0,
           col   = "grey60",
           parse = TRUE,
           label = expression("K + 0.5: " ~ A[LOWER] == 1.7209)) +
  annotate("text",
           x     = 100,
           y     = 60,
           adj   = 0,
           col   = "grey40",
           parse = TRUE,
           label = expression("K + 1: " ~ A[LOWER] == 0.5442)) +
  annotate("text",
           x     = 100,
           y     = 45,
           adj   = 0,
           col   = "grey20",
           parse = TRUE,
           label = expression("K + 2: " ~ A[LOWER] == 0.0544)) +
  annotate("text",
           x     = 100,
           y     = 30,
           adj   = 0,
           col   = "grey0",
           parse = TRUE,
           label = expression("K + 3: " ~ A[LOWER] == 0.0054)) +
  scale_x_log10(breaks = c(preFit$P, 10000),
                labels = c(preFit$P, 10000),
                limits = c(0.1, 10000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none"
  )

```

\autoref{fig:fig2CurveSpanEmpiricalMod} illustrates how inflating the span affects $A_{Lower}$ and this has three appreciable effects on the demand curve. First, the rate of change in elasticity is jointly reflected by parameter $\alpha$ and the span of the demand curve [@gilroy2020interpretation]. Given that $\alpha$ is a unit-less quantity, it co-varies inversely with the size of the span constant. That is, relatively greater $\alpha$ values reflect rapid changes in $\hat{y}$ across prices and relatively lesser values reflecting gradual changes in $\hat{y}$ across prices. Second, $\textit{k}$ values (i.e., $k < \frac{e}{log(10)}$) influence both the span of the demand curve as well as the range of elasticity and inelasticity observed in models derived from the @hursh2008economic framework [@newman2020improved; @gilroy2019exactsolve]. That is, $\textit{k}$ values that do not permit a span of 1 log unit necessarily restrict the range of elasticity values (i.e., analytic solutions for $P_{MAX}$ are not possible). Third, and most relevant to the @koff2015expt model, inflated $\textit{k}$ values serve to lessen the absolute difference between $A_{Lower}$ and 0. That is, the distance between $A_{Lower}$ and 0 is is lessened, but no $\textit{k}$ value could drive the span to 0. \autoref{fig:fig2CurveSpanEmpiricalMod} provides an elegant display of how this gap decreases proportionally with each unit increase in the span parameter $\textit{k}$.

## Hidden Model Equivalence

The sections above outline the limited ways in which two of the most popular derivatives of the @hursh2008economic framework differ. These two modeling strategies differ in terms of optimization (i.e., minimization of residual error) but share the limitations related to asymptotes. Regarding the first point, residual error and optimization, the two models can provide statistically equivalent results when the handling of residual error is made *comparable*. That is, re-weighting the errors (i.e., relative to $\hat{y}$) in the @koff2015expt model can yields fits and estimates approximate to those resulting from the @hursh2008economic model *in the absence of non-consumption.* Simultaneously, the @hursh2008economic model can be adjusted to yield estimates identical to those from the @koff2015expt model by adjusting residual error to be interpreted in terms of absolute difference, i.e. $E_i=10^{\hat{y}}-10^{y}$. A visualization of inter-related model fits are illustrated in \autoref{fig:fig3ComparisonNonzeroRelativeError}.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error", fig.height=2, warning=FALSE}

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           weights = 1/(predict(EXPT)),
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  rename(`Exponential`   = explPred,
         `Exponentiated (Absolute Error)` = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred) 

# kable(preFit.trim) %>%
#   kable_styling(full_width = TRUE)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P) 

ggplot(preFit.trim, aes(P, Q)) +
  #ggtitle("TODO: Add title") +
  geom_point(color = "black") +
  xlab("Unit Price") +
  ylab("Consumption") +
  geom_line(aes(P, Prediction), color = "grey40") +
  facet_wrap(~Model, ncol = 3) +
  # scale_y_log10(breaks = c(1, 10, 100),
  #               labels = c(1, 10, 100),
  #               limits = c(1, 100)) +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom", 
    strip.background.x = element_blank()
  )

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

preFit.point.1 <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0),
  Model = "Exponentiated"
)

preFit.point.2 = preFit %>%
  select(P, Q) %>%
  mutate(Model = "Exponential (Absolute)")

preFit.point.full = rbind(preFit.point.1,
                          preFit.point.2)

```

Regarding the second point, $A_{Lower}$ is seldom discussed in Operant Demand and this has considerable influence on models derived from the @hursh2008economic framework. This is an inherently complex topic, especially so in the @koff2015expt restatement, because consumption values observed at 0 are a quantity that cannot be predicted by models that reflect the range of consumption in log units. In attempting to accommodate observed 0 values, modeling based on the @hursh2008economic framework must minimize *two* sources of error instead of one. That is, the modeling must minimize residual error (as typical) as well as the distance between $A_{LOWER}$ and 0. This is because the span must be sufficnetly inflated in attempts to produce an $A_{LOWER}$ that *approximates* 0. For instance, an application of the @koff2015expt model where $\textit{k}$ is included as a fitted parameter simultaneously optimizes demand intensity, rates of change in elasticity, and a span constant (i.e., $A_{LOWER}$). As noted above, $A_{LOWER}$ is driven lower by inflating the span constant towards some non-zero number that is *reasonably* close to 0. Pragmatically, proponents of the @koff2015expt approach would likely argue that such a small amount of error calls for little concern and that $A_{Lower}$ could be considered *close enough* of an approximation of 0 consumption values to enable analyses of the complete data set.

Revisiting the argument around a *close enough* approximation of 0 consumption values, let us consider the following hypothetical. Let us say that the interpretation of a fitted @koff2015expt model optimizes such that values at $A_{Lower}$ are a close enough approximation of 0 to proceed with demand curve analyses using a complete data set that includes non-consumption. Following this logic (i.e., $A_{LOWER} \cong 0$), it stands to reason that treating sufficiently low $A_{LOWER}$ values and 0 consumption values as the same *should* replicate the behavior of the @koff2015expt model in the @hursh2008economic model. Assuming an inflated $\textit{k}$ parameter, equivalent estimates should result because $\hat{y}$ can be predicted beyond the range of observed non-zero levels and the resulting $A_{LOWER}$ should be *close enough* to 0 on the linear scale that differences between $A_{LOWER}$ and 0 would be considered negligible. Controlling for differences in terms of error representation, it stands to reason that the @hursh2008economic model would provide equivalent estimates had non-consumption been replaced by respective $A_{LOWER}$ values and error minimization been reflected in terms of absolute differences.

In a demonstration of this modified @hursh2008economic approach, the full data set from \autoref{fig:fig1CurveSpanEmpirical} was fitted with an inflated $\textit{k}$ parameter and non-consumption values replaced with respective $A_{Lower}$ values. Specifically, the most inflated span and corresponding $A_{Lower}$ from \autoref{fig:fig2CurveSpanEmpiricalMod} were used in this example demonstration, i.e. $A_{LOWER} = 10^{log{10}Q_0 - (k + 3)}$. The results of this modified @hursh2008economic approach are illustrated along with the @koff2015expt approach are illustrated in \autoref{fig:fig4ComparisonZeroRelative}.

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes", fig.height=2}

ggplot(subset(preFit.g, Model != "Exponential (Relative)"), aes(P, Q)) +
  geom_point(data = preFit.point.full) +
  geom_line(aes(P, Prediction), color = "grey40") +
  scale_x_log10(breaks = c(0.1, 10, 100, 1000, 10000),
                labels = c(0.1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  facet_wrap(~Model, ncol = 3) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none", 
    strip.background.x = element_blank()
  )

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  select(-c(`Exponential.Relative`)) %>%
  relocate(Exponentiated, .after = Q) %>%
  mutate(`Q.Mod` = Q) %>%
  mutate(Q = preFit.point.1$Q) %>%
  relocate(`Exponential.Absolute`, .after = `Q.Mod`) %>%
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

Controlling for differences in error handling (absolute difference) and $A_{Lower}$ values (non-consumption replaced by lower asymptotes in the @hursh2008economic model), the model fits are remarkably similar, see \autoref{tab:table1ComparisonZeroAbsolute}. This short example highlights several details that often go unnoticed when using the @koff2015expt model. First, this model does not characterize demand at 0. Rather, an inflated $\textit{k}$ parameter to drives $A_{Lower}$ to a quantity *close enough* to 0 that the absolute difference between 0 and $\hat{y}$ is negligible. This is the best that this approach can achieve because 0 does not fall within the interval between $A_{Upper}$ and $A_{Lower}$. Second, this approach is functionally equivalent to the @hursh2008economic model when non-consumption values are replaced with by the respective $A_{Lower}$ valuesand when residual errors are de-weighted (i.e., absolute). As such, both functional almost identically and differ in largely trivial aspects.

## Planned Comparisons

The purpose of this technical report was to clarify and evaluate the dimensions along which the @hursh2008economic and @koff2015expt models vary. Specifically, the shared mathematical bases between the two should allow for modifications wherein both provide statistically equivalent estimates---even when non-consumption values are present. The primary questions for the simulation study was to determine whether estimates resulting from the @hursh2008economic and @koff2015expt models would be statistically equivalent when controlling for differences in handling residual error (i.e., absolute, relative) and treating non-consumption values as respective $A_{Lower}$ values.

# Methods

```{r simulationParams}

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 1000

sim <- SimulateDemand(nruns = nParticipants, setparams = setparams, sdindex = sdindex, x = x)

dataSet = sim$simvalues

passingFrame = beezdemand::CheckUnsystematic(dataSet)

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 3

```

## Data Generating Process

A total of `r nParticipants` hypothetical data series were simulated using using the R Statistical Program [@R-base]. The specific syntax used to generate was included in an R package that was submitted to peer-review [@kaplan2019r]. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to simulate hypothetical purchase task data that included a large composition of 0 consumption values. The seed values and variance used to generate these data were identical to those that were used in @koff2015expt. This specific data generating process was used as the basis for comparisons with the @hursh2008economic model given that the authors of the @koff2015expt study modeled their approach around the "messy" data frequently observed in "real-world" purchase tasks that are often conducted on crowdsourced platforms.

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all hypothetical series. Specifically, individual series were screened for *bounce*, *trend*, and *reversals from zero*. The first criterion, bounce, refers to local changes within an expected downward trend as a function of increasing price. That is, it would be unexpected to see consumption increases immediately following a price increase. The second criterion, trend, refers to the molar change in consumption from the lowest to the highest price. That is, there is a certain amount of decrease in consumption expected across the full domain of prices. Lastly, reversals from zero refer to the return of consumption at a higher price following the cessation of consumption at a lower price. Simulated data were carried forward into the final analysis so long as each series met all indicators of systematic hypothetical purchase task data.

## Modeling Strategies

A total of 4 modeling approaches were evaluated (2 x Model, 2 x Error Handling). Each approach was referenced as a specific strategy for conducting demand curve analysis when 0 consumption values were observed in the data. This facilitated two pairwise comparisons when both models shared a comparable approach for handling residual error. These comparisons were used to determine whether the various strategies provided statistically equivalent estimates when asymptotes and error management were held constant. Consistent with efforts to maintain open and transparent science [@gilroy2019furthering], the source code necessary to reproduce this approach and this report has been posted for public review in a GitHub repository managed by the corresponding author, see Author Note. Each of the strategies used in these comparisons are presented below in greater detail.

### Strategy 1: @koff2015expt Model (Absolute)

The @koff2015expt model (absolute error difference) was fitted to simulated consumption data at the individual-level. The model was fit using the *optim* package included in the R Statistical Program [@R-base] due to its considerable flexibility in performing ordinary least squares regression. Initial starts were derived based on the respective data for parameter $Q_0$ and $\alpha$ was estimated on the log scale to support more comparable step sizes in the optimization. The span constant $k$ was derived from the empirical range of the full data set with an added constant (3) to allow the span of the demand curve to extend below the lowest non-zero point of consumption, as is common practice [@kaplan2018understanding]. The same span constant was used across all models to enable consistent comparisons between $Q_0$ and $\alpha$.

### Strategy 2: @koff2015expt Model (Relative Error)

The @koff2015expt model (relative error difference) was evaluated consistent with Strategy 1 with the exception of how differences in residual error were accommodated. In this approach, the absolute residual *approximated* relative error by referencing $\hat{y}$, i.e. $e_i = (\hat{y}- y) * \frac{1}{\hat{y}} = \frac{\hat{y}- y}{\hat{y}}$. It warrants noting that this manner of weighting error approximates log difference but the two are expected to differ. Specifically, the weighting of the error residual against $\hat{y}$ is equivalent to reflecting residual error as percentage or fractional change and this is closely approximates log difference under a specific set of circumstances, i.e. $ln\frac{Y_1}{Y_2} \approx \frac{Y_2 - Y_1}{Y_1}$. For instance, fractional approaches are identical to log difference exact with very small differences (e.g., 1%) but the approximation diverges from log difference once the degree of difference between values grows larger (e.g., 50%). This issue is presented more thoroughly in the Appendix of this work. Regardless, the two approaches are expected to behave comparably, but not identically, across many cases. All other parameters were estimated consistent with Strategy 1.

### Strategy 3: @hursh2008economic Model (Relative Error)

The @hursh2008economic model (relative error difference) was fitted to simulated consumption data at the individual-level. During the fitting, 0 consumption values were replaced by an $A_{Lower}$ value that was generated dynamically based on parameters $Q_0$ and $\textit{k}$. That is, a customized loss function was prepared for use with the *optim* method. As noted in Strategy 2, both Strategy 2 and Strategy 3 reflected relative but in different ways. That is, the @hursh2008economic model reflects changes in log difference by default and does not use approximation. All other parameters were estimated consistent with the other strategies.

### Strategy 4: @hursh2008economic Model (Absolute Error)

The @hursh2008economic model (absolute error difference) was fitted to simulated consumption data at the individual-level as well. This strategy was identical to that of Strategy 3 with the exception of how residual error was interpreted during optimization. That is, a customized loss function was used to represent residual error in terms of absolute differences, i.e. $e_i = 10^{\hat{y}} - 10^{y}$. All other parameters were estimated consistent with that of the other strategies.

## Analytical Strategy

Pairwise comparisons were conducted for parameters $Q_0$ and $\alpha$ resulting from each of the four strategies when controlling for differences in how residual error was interpreted during optimization. Tests of equivalence were performed in a pairwise fashion (i.e., Strategy 1 vs. 3, Strategy 2 vs. 4) to evaluate estimates resulting from the @koff2015expt model and the @hursh2008economic model with and without modified error terms. The *tost* method in the *equivalence* R package [@robinson2016package] was used to perform two one-sided t-tests (TOSTs) with paired estimates resulting from each strategy. That is, the focus was not on determining *difference* between strategies but instead on determining *equivalence* between them. Across all tests, corrections were applied due to presence of repeated comparisons, i.e. $p=0.05/2=0.025$.

# Results

```{r coreAnalyses}

dataFramePrep = data.frame(
  id       = 1:nParticipants,
  q0.HS    = numeric(length = nParticipants),
  q0.HSw   = numeric(length = nParticipants),
  q0.Koff  = numeric(length = nParticipants),
  q0.Koffw = numeric(length = nParticipants),
  a.HS     = numeric(length = nParticipants),
  a.HSw    = numeric(length = nParticipants),
  a.Koff   = numeric(length = nParticipants),
  a.Koffw  = numeric(length = nParticipants),
  K        = numeric(length = nParticipants)
)

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPT.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  
  subs = newData$y
  subs = ifelse(subs == 0, newAsymptote, subs)
  
  newData$ys   = getEXPT(par[1], kSet, 10^par[2], newData$x)
  newData$err  = ((newData$ys - subs)/newData$ys)^2

  sum(newData$err)
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

minQ0 = 0.01
maxQ0 = 200

for (id in 1:nParticipants) {

  currentData = dataSet[dataSet$id == id, ]

  dataFramePrep[id, "K"] = kSet

  fit.Koff <- optim(par  = c(max(currentData$y), -3),
                    fn   = min.RSS.EXPT,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data = currentData)

  dataFramePrep[id, c("q0.Koff", "a.Koff")] = fit.Koff$par
  
  fit.HS   <- optim(par    = c(fit.Koff$par[1], fit.Koff$par[2]),
                    fn     = min.RSS.EXPL,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data   = currentData)

  dataFramePrep[id, c("q0.HS", "a.HS")] = fit.HS$par

  # ==================================================
  # Weighted sets
  # ==================================================
  
  fit.Koffa <- optim(par   = c(max(currentData$y), -3),
                     fn     = min.RSS.EXPT.aw,
                     method = "L-BFGS-B",
                     lower  = c(0.01, -6),
                     upper  = c(max(currentData$y) * 1.5, 0),
                     data   = currentData)

  dataFramePrep[id, c("q0.Koffw", "a.Koffw")] = fit.Koffa$par

  fit.HWa <- optim(par    = c(fit.HS$par[1], fit.HS$par[2]),
                   fn     = min.RSS.EXPL.aw,
                   method = "L-BFGS-B",
                   lower  = c(0.01, -5),
                   upper  = c(max(currentData$y) * 1.25, 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw", "a.HSw")] = fit.HWa$par
  
  
  
  

  # fit.Koffa = nls(y ~ Q0 * 10^(kSet*(exp(-a * Q0 * x)-1)),
  #          data = currentData,
  #          start = c(a  = 0.0001,
  #                    Q0 = 100),
  #          lower = c(a = -Inf,
  #                    Q0 = 1),
  #          upper = c(a = Inf,
  #                    Q0 = 125),
  #          algorithm = "port",
  #          weights = predict(fit.Koff),
  #          control = list(
  #            maxiter = 1000,
  #            warnOnly = TRUE
  #          ))
  # 
  # dataFramePrep[id, c("a.Koffw", "q0.Koffw")] = c(coef(fit.Koffa)["a"],
  #                                                 coef(fit.Koffa)["Q0"])
  


}

keepers = passingFrame %>%
  filter(TotalPass == 3) %>%
  select(id) %>%
  pull()

frameToAnalyze = dataFramePrep %>%
  filter(id %in% keepers)

```

The data generating process yielded a total of `r nrow(frameToAnalyze)` series that met all 3 indicators of systematic purchase task data (from *N*=`r nParticipants`; `r round((nrow(frameToAnalyze)/nParticipants) * 100, 2)`%). Overall, the correspondence between each of the strategies was remarkable and the results of specific pairwise comparisons are presented below.

## Strategy 1 vs. Strategy 4

```{r s1vs4fig, fig.cap="Comparisons of Strategy 1 and 4 (Absolute Error)", fig.height=4}

library(equivalence)

tost.1v4.q0 = tost(frameToAnalyze$q0.Koff, 
                   frameToAnalyze$q0.HSw, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.1v4.q0 = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

tost.1v4.a = tost(frameToAnalyze$a.Koff, 
                  frameToAnalyze$a.HSw, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.1v4.a = cor.test(frameToAnalyze$a.Koff, frameToAnalyze$a.HSw)

par(mfrow = c(1, 2),
    oma   = c(0, 0, 0, 0))

#comps.base <- lm(q0.Koff ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main = expression(Q[0] ~ "Estimates"),
     col  = alpha("black", 0.1),
     ylab = "Strategy 1",
     ylim = c(0,100),
     xlab = "Strategy 4",
     xlim = c(0,100),
     frame=FALSE)
#abline(comps.base)

box(bty="l")

#comps.base <- lm(a.Koff ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main = expression(alpha ~ "Estimates"),
     col  = alpha("black", 0.1),
     ylab = "Strategy 1",
     ylim = c(-5,0),
     xlab = "Strategy 4",
     xlim = c(-5,0),
     frame=FALSE)
#abline(comps.base)

box(bty="l")

```

The comparison between Strategy 1 and Strategy 4 was designed to evaluate difference between the two approaches error was interpreted in terms of absolute difference. An evaluation of the relationship between Strategy 1 and 4 revealed perfect correlations for both $Q_0$ (r=`r cor.1v4.q0$estimate`, t=`r cor.1v4.q0$statistic`, df=`r cor.1v4.q0$parameter`, `r scales::pvalue(cor.1v4.q0$p.value, accuracy = .025, add_p = TRUE)`) and for $\alpha$ (r=`r cor.1v4.a$estimate`, t=`r cor.1v4.a$statistic`, df=`r cor.1v4.a$parameter`, `r scales::pvalue(cor.1v4.a$p.value, accuracy = .025, add_p = TRUE)`). Two one-sided T-tests (TOSTs) were significant for both $Q_0$ (`r scales::pvalue(tost.1v4.q0$tost.p.value, accuracy = .025, add_p = TRUE)`) and $\alpha$ (`r scales::pvalue(tost.1v4.a$tost.p.value, accuracy = .025, add_p = TRUE)`). Results of equivalence testing rejected the null hypothesis of statistical difference and indicated that the estimates resulting from each strategy were statistically equivalent for both parameters. A visualization of these corresponding estimates are illustrated in \autoref{fig:s1vs4fig}.

## Strategy 2 vs. Strategy 3

```{r s2vs3fig, fig.cap="Comparisons of Strategy 2 and 3 (Relative Error)", fig.height=4}

tost.2v4.q0 = tost(frameToAnalyze$q0.Koffw, 
                   frameToAnalyze$q0.HS, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.2v4.q0 = cor.test(frameToAnalyze$q0.Koffw, frameToAnalyze$q0.HS)

tost.2v4.a = tost(frameToAnalyze$a.Koffw, 
                  frameToAnalyze$a.HS, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.2v4.a = cor.test(frameToAnalyze$q0.Koffw, frameToAnalyze$q0.HS)

par(mfrow = c(1, 2),
    oma   = c(0, 0, 0, 0))

#comps.base <- lm(q0.Koffw ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koffw ~ frameToAnalyze$q0.HS,
     main = expression(Q[0] ~ "Estimates"),
     col  = alpha("black", 0.1),
     ylab = "Strategy 2",
     ylim = c(0,100),
     xlab = "Strategy 3",
     xlim = c(0,100),
     frame=FALSE)
#abline(comps.base)

box(bty="l")

#comps.base <- lm(a.Koffw ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koffw ~ frameToAnalyze$a.HS,
     main = expression(alpha ~ "Estimates"),
     col  = alpha("black", 0.1),
     ylab = "Strategy 2",
     ylim = c(-5,0),
     xlab = "Strategy 3",
     xlim = c(-5,0),
     frame=FALSE)
#abline(comps.base)

box(bty="l")

```

TODO: update from here

Evaluations of the relationship between Strategy 2 and 4 revealed strong, but not perfect correlations for $Q_0$ (r=`r cor.2v4.q0$estimate`, t=`r cor.2v4.q0$statistic`, df=`r cor.2v4.q0$parameter`, `r scales::pvalue(cor.2v4.q0$p.value, accuracy = .025, add_p = TRUE)`) and for $\alpha$ (r=`r cor.2v4.a$estimate`, t=`r cor.2v4.a$statistic`, df=`r cor.2v4.a$parameter`, `r scales::pvalue(cor.2v4.a$p.value, accuracy = .025, add_p = TRUE)`). Despite imperfect correlations, the results of TOSTs for $Q_0$ (`r scales::pvalue(tost.2v4.q0$tost.p.value, accuracy = .025, add_p = TRUE)`) and $\alpha$ (`r scales::pvalue(tost.2v4.a$tost.p.value, accuracy = .025, add_p = TRUE)`) were significant overall. That is, the results of equivalence testing indicated that both strategies yielded statistically equivalent model parameters for both $Q_0$ and $\alpha$. That is, results of equivalence testing rejected the null hypothesis of statistical difference and indicated that the estimates yielded from each strategy were statistically equivalent for $\alpha$ but and for $Q_0$. A visualization of these relationships are illustrated in \autoref{fig:s2vs3fig}.

# Discussion

The Operant Demand Framework has grown into a popular and well-regarded approach for evaluating choices and behavior of societal significance [@hursh2013behavioral; @reed2013behavioral]. Indeed, various labs and teams have shifted their focus from specific clinical questions towards issues of public policy @hursh2013behavioral; @roma2017progress]. Furthermore, specific modeling strategies in the Operant Demand Framework are increasingly represented in a range of scientific tools and packages [@kaplan2019r; @gilroy2018demand]. Despite an increasing range of scientific tools and resources, few firm guidelines exist with which to assist analysts in navigating between options for demand curve analyses. The purpose of this technical report was to review mathematical underpinnings of the two prevailing models derived from the framework of @hursh2008economic and present an argument why distinctions between these interchangeable approaches offers little to the advancement of the Operant Demand Framework.

This report provides an in-depth discussion and review of how 0 consumption values have, thus far, been included in models derive from the @hursh2008economic framework. As noted throughout this report, both the @hursh2008economic and the @koff2015expt approach are unable to model demand at 0 and both are restricted to the non-zero lower asymptote, $A_{Lower}$, in the same manner. This is the case regardless of whether 0 consumption values can be included in the regression. As such, the approach put forward in @koff2015expt is not a complete solution for 0 consumption values because it retains the same limitations of the original approach in this regard. This is because the span of the demand curve in the @hursh2008economic framework remains in the log scale and the log scale does not support 0 values. As an alternative, others have argued that a true solution to this issue would require deviating from the traditional log scale @gilroy2021zbe].

The simulations performed in this study were designed to facilitate comparisons between the @hursh2008economic and @koff2015expt models when controlling for the common $A_{LOWER}$ and differences in how residual errors were weighted. The goal of these comparisons were to advance the argument that the Exponential [@hursh2008economic] and Exponentiated [@koff2015expt] models should not be so strongly distinguished. Indeed, it is quite trivial to arrive at statistically equivalent estimates in both approaches when the role of the span constant and $A_{LOWER}$ is understood as they relate to 0 consumption values. The results of computer simulations confirmed that the two approach provide statistically equivalent estimates when controlling for such differences. This is because, mathematically, the behavior of $A_{LOWER}$ is identical regardless of which approach is used. Given that neither approach can characterize demand at zero, $A_{LOWER}$ is the *best* approximation of 0 in these circumstances. As such, replacing 0 consumption values with respective $A_{LOWER}$ values results in estimates that are statistically equivalent across the Exponential and Exponentiated models. This argument is not presented with the intent of favoring any specific approach as the de facto standard or recommended default when applying methods from the Operant Demand Framework. Rather, the intent of this work is in revealing how these (often) opposing strategies are much more similar than they are different. Indeed, they are so similar that distinguishing the two only serves to observe the shared mathematical bases for each. That said, each approach has utility and future efforts should be directed towards improving the understanding the properties of the @hursh2008economic framework rather than reinforcing any stance, position, or bias towards a specific implementation.

The final contribution of this work was in clarifying the common mathematical bases for the Exponential and Exponentiated models and highlight the ways in which the proponents of each approach have extended the Operant Demand Framework. That is, the proponents of each approach were successful in advancing both the utility and scope of the Operant Demand Framework. That is, the finding that the @hursh2008economic model can replicate the behavior of the @koff2015expt without exponentiation terms does not negate the contributions of the @koff2015expt approach. Indeed, the initiative of the @koff2015expt team warrants considerable praise even today because this led an initiative towards addressing the problematic issue of removing otherwise valid data. For decades, substantial portions of otherwise consumption data were never carried forward into analyses and it is unclear how these prior analyses would compare had these data been included. Regardless of whenever analysts have a preference for one approach or the other, it is clear that the methods featured in Operant Demand Framework are better equipped now that 0 consumption values are considered in the analysis.

## Future Directions in Operant Demand

The future is promising for the Operant Demand Framework. This perspective and this framework currently reflects a range of consumption (and non-consumption) and efforts are underway to leverage multilevel modeling as a future extension [@kaplanMlm]. Indeed, various labs are working toward increasing the applicability and generality of this approach. Towards this end, the intent and mission of the original @koff2015expt study regarding 0 consumption values is as valid and valuable today as it was when this work was first published. However, debates regarding model superiority (or inferiority) in the absence of formal tests does not enhance the Operant Demand Framework and its methods in any appreciable manner. That said, the two approaches are functionally interchangeable the reader is cautioned against thinking that any single model is inherently "true", "better", or otherwise superior in the absence of careful statistical evaluation. That said, it is unclear whether the prevailing approach in the Operant Demand Framework will remain based on the framework presented in @hursh2008economic. Indeed, it is possible that future research could lead to deviating from the use of log scale [@gilroy2021zbe] or towards a different framework altogether [@newman2020improved]. Regardless of the where the future takes the Operant Demand Framework, future approaches and advances should be met with cautious optimism and consideration rather than outright disregard in favor what has come before.

\newpage

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

\endgroup

\newpage

# Appendix

Several proofs are provided here to illustrate how the upper and lower asymptotes are determined. Despite the shared mathematical basis, derivations of each are provided below.

## Modified Hursh & Silberburg (2008) Optimization (Relative Error)

$$
e_i = \begin{cases} 
\hat{y}_i - log_{10}y_i       &    \mbox{if } y_i \not= 0 \\
\hat{y}_i - log_{10}A_{LOWER} &    \mbox{if } y_i = 0 \\
\end{cases}
$$

## Modified Hursh & Silberburg (2008) Optimization (Absolute Error)

$$
e_i = \begin{cases} 
10^{\hat{y}_i} - 10^{log_{10}y_i}       &    \mbox{if } y_i \not= 0 \\
10^{\hat{y}_i} - 10^{log_{10}A_{LOWER}} &    \mbox{if } y_i = 0 \\
\end{cases}
$$

## Hursh & Silberburg (2008) Proofs

### $A_{UPPER}$ at $P = 0$

$$
\begin{aligned}
log_{10}A_{Upper} &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * 0} - 1) \\
 &= log_{10}Q_0 + k(e^{0} - 1) \\
 &= log_{10}Q_0 + k(1 - 1) \\
 &= log_{10}Q_0 + k(0) \\
 &= log_{10}Q_0\\
 A_{Upper} &= Q_0
\end{aligned}
$$

Note: Euler's constant raised to the power of 0 is equal to a value of 1. This essentially zeroes out the $\textit{k}$ constant, leaving just the $Q_0$ parameter at 0 $P$.

### $A_{LOWER}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
log_{10}A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * \infty} - 1) \\
 &= log_{10}Q_0 + k(e^{-\infty} - 1) \\
 &= log_{10}Q_0 + k(0 - 1) \\
 &= log_{10}Q_0 + k(-1) \\
 &= log_{10}Q_0 - k \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$

Note: Euler's constant raised to the power of $-\infty$ equates to a value of 0. That is, $e^{-\infty}=\frac{1}{e^\infty}=\frac{1}{\infty} \approx 0$. This is has effect of making the value in parentheses equal to $-1$, which in turn results in the full subtraction of quantity $k$ from $log_{10}Q_0$.

## Koffarnus et al. (2015) Proofs

### $A_{UPPER}$ at $P = 0$

$$
\begin{aligned}
A_{Upper} &= Q_0 * 10^{k(e^{-\alpha * Q_0 * 0} - 1)}  \\
 &= Q_0 * 10^{k(e^{0} - 1)}  \\
 &= Q_0 * 10^{k(1 - 1)}  \\
 &= Q_0 * 10^{k(0)}  \\
 &= Q_0 * 10^{0}  \\
 &= Q_0 * 1  \\
 &= Q_0  \\
log_{10}A_{Upper} &= log_{10}Q_0  \\
\end{aligned}
$$

### $A_{LOWER}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= Q_0 * 10^{k(e^{-\alpha * Q_0 * \infty} - 1)}  \\
 &= Q_0 * 10^{k(e^{-\infty} - 1)}  \\
 &= Q_0 * 10^{k(0 - 1)}  \\
 &= Q_0 * 10^{k(-1)}  \\
 &= Q_0 * 10^{-k}  \\
log_{10}A_{Lower} &= log_{10}Q_0 + (-k)  \\
 &= log_{10}Q_0 - k  \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$ \newpage

## Differences between Log and Fractional Difference

### Logarithmic Difference

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 90
\end{aligned}
$$

$$
\begin{aligned}
ln(\frac{V_2}{V_1})      & = -1 * ln(\frac{V_1}{V_2}) \\
ln(\frac{90}{100})       & = -1 * ln(\frac{100}{90}) \\
ln(0.9)                  & = -1 * ln(1.11) \\
-0.1053                  & = -1 * 0.1053 \\
-0.1053                  & = -0.1053
\end{aligned}
$$

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 50
\end{aligned}
$$

$$
\begin{aligned}
ln(\frac{V_2}{V_1})      & = -1 * ln(\frac{V_1}{V_2}) \\
ln(\frac{50}{100})       & = -1 * ln(\frac{100}{50}) \\
ln(0.5)                  & = -1 * ln(2) \\
-0.6931                  & = -1 * 0.6931 \\
-0.6931                  & = -0.6931
\end{aligned}
$$

\newpage

### Fractional Difference

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 90
\end{aligned}
$$

$$
\begin{aligned}
\frac{ V_2 - V_1}{V_1} & \approx -1 *\frac{ V_1 - V_2}{V_2} \\
\frac{ 90 - 100}{100}  & \approx -1 *\frac{ 100 - 90}{90} \\
\frac{ -10}{100}        & \approx -1 *\frac{ 10}{90} \\
-0.1                    & \approx -1 * 0.11 \\
-0.1                    & \approx -0.11
\end{aligned}
$$

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 50
\end{aligned}
$$

$$
\begin{aligned}
\frac{ V_2 - V_1}{V_1} & \approx -1 *\frac{ V_1 - V_2}{V_2} \\
\frac{ 50 - 100}{100}  & \approx -1 *\frac{ 100 - 50}{50} \\
\frac{ -50}{100}       & \approx -1 *\frac{ 50}{50} \\
-0.5                   & \approx -1 * 1 \\
-0.5                   & \approx -1
\end{aligned}
$$ Note: The examples provided above illustrate how log difference $(-0.1053)$ and fractional difference $(-0.1)$ are quite close for small differences. However, the difference between log difference $(-0.6931)$ and fractional difference $(-0.5)$ begins to differ considerably with larger changes. As such, the two approaches to reflecting relative differences are unlikely to be perfectly related outside of optimal conditions.
