---
title             : "Split Hairs in the Operant Demand Framework: Clarifying Methods of Accomodating Zero in Demand Curve Analysis"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn P. Gilroy"
    affiliation   : "1"
    corresponding : yes
    address       : "226 Audubon Hall Baton Rouge, Louisiana 70806"
    email         : "sgilroy1@lsu.edu"
    role:
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
  Shawn Patrick Gilroy is an Assistant Professor of School Psychology at Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu. The source code necessary to recreate this work is publicly hosted in a repository at: https://github.com/miyamot0/AgnosticDemandModeling 

abstract: |
  Operant translations of Behavioral Economic concepts and principles have enhanced the ability of researchers to characterize individual choice under constraints and uncertainty. Operant Behavioral Economic models of concurrent choices (i.e., demand) have been particularly useful in evaluating how individual choices are affected by various ecological factors (e.g., price, limited resources). Prevailing perspectives in Operant Demand are derived from the framework of Hursh & Silberburg (2008). Few dispute the utility of this framework, though considerable debate continues regarding how best to address limitations associated with the logarithmic scale. At present, there are opposing views regarding the handling of zero values and under which situations alternative restatements of this framework are warranted, c.f. Koffarnus et al. (2015). The purpose of this report is to review the importance of asymptotes in the Operant Demand Framework and the relatively minor ways in which the Hursh & Silberburg (2008) and Koffarnus et al. (2015) differ. A simulation study is provided as a demonstration of how results from the two models can be largely interchangeable when addressing asymptote and error differences. Additional discussion is provided on whether having multiple models extends the Operant Demand Framework.

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
---

\raggedbottom

```{r include = FALSE}

library(beezdemand)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(tidyverse)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Contemporary methods for evaluating the consumption of goods and services using the Operant Demand Framework draw heavily from the methodology proposed in @hursh2008economic. This framework and this methodology have evolved through several iterations [@hursh1987linear], with the most recent form taking a non-linear shape and using an exponential decay process. Beyond the @hursh2008economic specifically, the more recent extensions of the @hursh2008economic framework have also explored variants that evaluate consumption on the linear [@koff2015expt] or other log-like scales [@gilroy2021zbe].

The original implementation of the @hursh2008economic framework was constructed to predict an overall "s-type" form as the prototypical shape of the demand curve. Indeed, the original intent of @hursh2008economic was to have an upper asymptote reached at a low (or free) price and a lower asymptote reached as prices approached infinity [@gilroy2021zbe]. Until most recently, all models derived this framework [@hursh2008economic, @koff2015expt] evaluated the demand for reinforcers with these non-zero upper and lower asymptotes. That is, these models were bounded at an upper limit (i.e., $Q_0$) and progressed towards a lower asymptote in the roughly S-type form. The non-zero asymptotes in the @hursh2008economic framework make good sense because the traditional values of interest were positive real values (i.e., not 0). Such quantities were necessary because the logarithmic representation of consumption is naturally undefined.

In response to statistical and philosophical issues related to the omission of 0 consumption values, @koff2015expt introduced a restatement of the @hursh2008economic framework. This variant of the @hursh2008economic model was adjusted to examine changes in observed consumption on the linear scale. Specifically, an exponentiation of terms was performed such that the LHS (left-hand side) of the @hursh2008economic model were reflected in the linear scale. In this way, the LHS of the model (i.e., consumption) need not be submitted to the log transformation that previous prevented the use of the @hursh2008economic model. This modification drew considerable attention, as one of the largest issues associated with the log scale can be avoided. However, it warrants noting that portions of the RHS (right-hand side) of the @koff2015expt model remain on the log scale. Of particular importance, the span of the demand curve and the rate of exponential decay remain reflected in the log scale. It is for this reason that the span of the demand curve in this restated model cannot reach a a true 0 point, and thus, is also bound to non-zero upper and lower asymptotes. Furthermore, the regressive process for logarithmic and linear models differs with respect to how error is weighted and this also introduces behavior that differs from that of the original @hursh2008economic framework.

## Same Model, Different Error

The various challenges associated with fitting models of operant demand (i.e., minimizing residual error) are increasingly discussed in the Operant Demand Framework [@gilroy2021zbe]. @gilroy2021zbe, among various topics, noted how residual error is reflected differently in log and linear scales and how such differences affect model optimization. Briefly, changes in log space reflect relative differences while changes in the linear space reflect absolute differences. In most economic applications, relative error is typically preferred because quantities of interest and their projections (i.e., $\hat{y}$) often span multiple orders and quantities observed at higher orders would naturally be weighted more heavily than those at lower orders in the linear scale (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). For this reason, relative difference is often the default because and this approach balances how each piece of information is weighted in the model. 

As an alternative to relative differences, which are weighted against observed values, absolute differences are more straightforward. That is, absolute error is simply the difference from some observed quantity and $\hat{y}$ regardless of the order (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). @gilroy2021zbe discussed how the departure from relative error can affect model optimization when observed levels of consumption vary considerably across orders. This has led to occasional inconsistencies wherein model fits across the log and linear variants of @hursh2008economic framework lead to different conclusions (e.g., shared alpha across varying dose-response curves).

## Different Error, Same Asymptotes

Apart from differences in how error is minimized across models in the Operant Demand Framework, there has been renewed attention regarding the upper and lower asymptotes of these models [@gilroy2021zbe]. Specifically, @gilroy2021zbe highlighted how neither the @hursh2008economic nor @koff2015expt models could characterize demand at 0. The topic of the zero-asymptote is not discussed at length here and readers are encouraged to review this work for an exposition on why the inability to model demand at zero limits the utility of the Operant Demand Framework.

Revisiting the topic of asymptotes, two novel terms are noted in this work: $A_{Upper}$ and $A_{Lower}$. The term $A_{Upper}$ refers to the upper, typically non-zero bound of the demand curve. In models derived from the @hursh2008economic framework, this is simply the fitted parameter $Q_{0}$. That is, this is the absolute upper limit to the demand curve as prices approach 0, $\displaystyle{\lim_{P \to 0} f(x)= A_{Upper}}$. In contrast, the term $A_{Lower}$ refers the absolute lower bound of the demand curve. That is, this is the absolute lower limit to the demand curve as prices approach $\infty$, $\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}$. These two upper and lower extremes are separates by the span constant $\textit{k}$, which specify the distance in log units between these asymptotes. The derivation of both $A_{Upper}$ and $A_{Lower}$ are noted below.

\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}

Further inspection of the $A_{Lower}$ and its derivation naturally evoke questions regarding how 0 consumption values could be evaluated in the @hursh2008economic framework. As noted in the bottom portion of *Equation 1*, both the @hursh2008economic and @koff2015expt models can *never* take a value of 0 because both asymptotes exist in a space where such a value cannot exist. As such, all predictions from these model must take values between these limits, $\hat{y} \in [ A_{Lower}, A_{Upper} ]$.

## Same Asymptotes, Same Spans

Revisiting the *Equation 1*, this notation highlights the heavy influence of the span parameter $\textit{k}$ and how it affects the values that can be predicted ($\hat{y}$). In the original implementation of the @hursh2008economic framework, parameter $\textit{k}$ was specified based on the range of *observed consumption*. Since 0 consumption values were omitted from this implementation, parameter $\textit{k}$ was directly linked to the observed data. Indeed, dropping the 0 values made the specification of the span straightforward. That is, both parameter $\textit{k}$ and $A_{Lower}$ could be directly matched to the lowest observed non-zero levels of consumption. A visualization of the specification of parameter $\textit{k}$ is provided in \autoref{fig:fig1CurveSpanEmpirical}.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes") +
  geom_point() +
  annotate("segment", 
           x      = 0.1, 
           xend   = 0.1, 
           y      = 100, 
           yend   = lowerAsymptote,
           colour = "red") +
  annotate("text", 
           x     = 0.175, 
           y     = 55, 
           angle = 90,
           adj   = 0.5,
           col   = "red",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = lowerAsymptote, 
           yend   = lowerAsymptote,
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "red",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = max(preFit$Q), 
           yend   = max(preFit$Q),
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "red",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw()

```

Whereas the determination of parameter $\textit{k}$ in the @hursh2008economic implementation is more directly linked to observed (non-zero) consumption data, the determination of parameter $\textit{k}$ became more abstract in the implementation introduced by @koff2015expt. That is, parameter $\textit{k}$ could not longer be linked to observed (non-zero) consumption data because $A_{Lower}$ would not extend beyond the range of non-zero consumption. Anecdotally, various labs and teams using this model have either opted to either add a small constant to parameter $\textit{k}$ or fit this as a free parameter--both with the intent of *increasing* the span, and thus, driving $A_{Lower}$ to a point closer to 0 on the linear scale. A visualization of this behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  annotate("text",
           x     = 5000,
           y     = 90,
           adj   = 1,
           col   = "red",
           parse = TRUE,
           label = expression("K (Empirical): " ~ A[LOWER] == 5.442)) +
  annotate("text",
           x     = 5000,
           y     = 80,
           adj   = 1,
           col   = "yellow",
           parse = TRUE,
           label = expression("K + 0.5: " ~ A[LOWER] == 1.7209)) +
  annotate("text",
           x     = 5000,
           y     = 70,
           adj   = 1,
           col   = "green",
           parse = TRUE,
           label = expression("K + 1: " ~ A[LOWER] == 0.5442)) +
  annotate("text",
           x     = 5000,
           y     = 60,
           adj   = 1,
           col   = "blue",
           parse = TRUE,
           label = expression("K + 2: " ~ A[LOWER] == 0.0544)) +
  annotate("text",
           x     = 5000,
           y     = 50,
           adj   = 1,
           col   = "purple",
           parse = TRUE,
           label = expression("K + 3: " ~ A[LOWER] == 0.0054)) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw() +
  theme(legend.position = "bottom")

```

\autoref{fig:fig2CurveSpanEmpiricalMod} illustrates how inflating the span constant affects $A_{Lower}$. This has two significant effects on the demand curve. First, the rate of change in elasticity is jointly reflected by parameter $\alpha$ and the span of the demand curve. Given that $\alpha$ is unitless, it covaries inversely with the size of the span constant--with relatively greater values reflecting rapid changes in $\hat{y}$ across prices and relatively lesser values reflecting gradual changes in $\hat{y}$ across prices. Second, and specific to the @koff2015expt model, larger $\textit{k}$ values serve to lessen the absolute difference between $A_{Lower}$ and true 0--a point that cannot be characterized in the model. \autoref{fig:fig2CurveSpanEmpiricalMod} elegantly displays how this gap decreases proportionally with each unit increase in the span parameter $\textit{k}$.

## Unnecessary Distinctions

The areas above serve to highlight the minimal ways in which the @hursh2008economic and @koff2015expt models differ. Admittedly, these approaches differ in terms of optimization but ultimately share the same limitations with respect to a non-zero lower asymptote. Regarding the first point, error representation and optimization, the two models can provide functionally identical results when error handling is made comparable. That is, re-weighting the errors (i.e., relative to $\hat{y}$) in the @koff2015expt model yields fits and estimates that are nearly identical to that of the @hursh2008economic model in the absence of 0 values in consumption, \autoref{fig:fig3ComparisonNonzeroRelativeError}.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error", fig.height=3}

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * 100 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * 100 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * 100 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           weights = 1/(predict(EXPT)^2),
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  rename(`Exponential`   = explPred,
         `Exponentiated (Absolute Error)` = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred) 

# kable(preFit.trim) %>%
#   kable_styling(full_width = TRUE)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P) 

ggplot(preFit.trim, aes(P, Q, color = Model)) +
  #ggtitle("TODO: Add title") +
  geom_point(color = "black") +
  geom_line(aes(P, Prediction)) +
  facet_wrap(~Model, ncol = 3) +
  scale_y_log10(breaks = c(1, 10, 100),
                labels = c(1, 10, 100),
                limits = c(1, 100)) +
  theme_bw() +
  theme(legend.position = "bottom")

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * 100 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * 100 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * 100 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

```

Although the literature better characterizes the impact of relative and absolute error, few researchers have focused on $A_{Lower}$ and its role in models derived from the @hursh2008economic framework. This is a particularly complex topic in the application of the linear restatement because 0 consumption values are typically included in the regression despite an inability to characterize demand at 0 levels. The implementation put forward in @koff2015expt minimizes the impact of zeroes on the log scale by capitalizing on the relatively small difference between $A_{Lower}$ and 0 on the linear scale when $\textit{k}$ values exceed that of the observed range. Pragmatically, most care little for such a small amount of error, this raises an interesting argument regarding the management of 0 values. That is, what is being considered *close enough* when accomodating 0 consumption values in demand curve analyses.

For the sake of argument, let us say that the model introduced in @koff2015expt considers values at $A_{Lower}$ to be *close enough* to 0 to reasonably approximate consumption at such levels. That is, $A_{Lower}$ is considered sufficiently approximate to 0. Following this logic (i.e., this degree of difference is negligible), it stands to reason that treating 0 values as $A_{Lower}$ is appropriate because 1) it allows demand to be projected beyond observed non-zero levels and 2) it is close enough to 0 that absolute differences are minimal. As such, it stands to reason that one could simply replace the 0 values with $A_{Lower}$, a quantity easily derived from $Q_0$ and $\textit{k}$. 

Reconsidering the full data displayed \autoref{fig:fig2CurveSpanEmpiricalMod}, only the @koff2015expt model would proceed under these conditions because of how 0 values interact with log transformation. However, as noted above, 0 values can be replaced by an individually-set $A_{Lower}$ value and both the @koff2015expt and @hursh2008economic models converge quite closely in this case, see \autoref{fig:fig4ComparisonZeroRelative}. Furthermore, re-weighting the @hursh2008economic to absolute difference error produces an even closer approximation between the two that renders almost identical, see \autoref{fig:fig4ComparisonZeroRelative} and see \autoref{tab:table1ComparisonZeroAbsolute}.

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes"}

ggplot(preFit.g, aes(P, Q, color = Model)) +
  geom_point() +
  geom_line(aes(P, Prediction)) +
  scale_x_log10(breaks = c(0.1, 10, 100, 1000, 10000),
                labels = c(0.1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  scale_y_log10(breaks = c(0.001, 0.01, 0.1, 1, 10, 100),
                labels = c(0.001, 0.01, 0.1, 1, 10, 100),
                limits = c(0.001, 100)) +
  facet_wrap(~Model, ncol = 3) +
  theme_bw() +
  theme(legend.position = "bottom")

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

The hypothetical data series modeled in \autoref{fig:fig4ComparisonZeroRelative} demonstrate highly similar fits when controlling for $A_{Lower}$ (i.e., a common lower bound). That is, the approach put forward in @koff2015expt uses $A_{Lower}$ as a *close enough* proxy to zero and functionally treats 0 values as $A_{Lower}$. When adapting this same logic to the @hursh2008economic implementation, this results in nearly the same fit. As indicated in \autoref{tab:table1ComparisonZeroAbsolute}, the projections between each of these implementations becomes even closer when the @hursh2008economic implementation is amended to treat differences in error as absolute.

## Different Model, Same Limitations

 



The considerable influence of near-zero values and relative error are described well in @koff2015expt. As noted in that work, the replacement of zero values with *arbitrary* constants led to wildly varying effects on fits using the @hursh2008economic framework. Indeed, most agree that constructing a common $A_{Lower}$ (e.g., for all demand series) with a small constant (e.g., 0.1, 0.01) is unfavorable. The authors highlight issues with applying an *arbitrary* constant to zero-valued consumption and used this logic to make a case for a restatement of the @hursh2008economic model in linear terms. The desire to include all data, zero-valued or not, is admirable and statistically desirable but two issues warrant noting in this approach. 

First, the evaluation of *arbitrary* constants is fundamentally at odds with how individual demand curves and span constants interact to determine the demand curve, see \autoref{fig:fig1CurveSpanEmpirical}. That is, $Q_{0}$ forms the basis for $A_{Upper}$ and the arbitrary constant (in log scale) differ from $A_{Lower}$ by several magnitudes. This is because $A_{Upper}$ and $A_{Lower}$ are jointly determined by individual-level parameters and the global span constant $k$. As such, replacing *all* zero values with a common constant simply does not provide a reasonable approximation of $A_{Upper}$.

Second, neither the @hursh2008economic nor the @koff2015expt models characterize demand at zero. Although the @hursh2008economic fails for obvious reasons, the limitations of the @koff2015expt approach are less self-evident. Specifically, regression in the @koff2015expt approach succeeds in the case of 0 value consumption because 0 values *are treated as* $A_{Lower}$, which is a bound determined by individual-level $Q_{0}$ values and the span parameter $k$. As such, this strategy succeeds because 0 values are approximated by individual-level $A_{Upper}$ bounds and because the absolute difference $A_{Upper}$ decreases by an order of magnitude for each unit increase in parameter $k$. As such, the @koff2015expt approach is essentially equivalent to using the @hursh2008economic with individually-derived constants and normally distributed error in the linear scale. However, this claim is not trivial and simulation and analysis is warranted to whether the two models differ in an appreciable manner.

# Methods

## Data Generating Process

A total of 500 hypothetical data series were simulated using software submitted to peer-review. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to generate demand series data. The seed values and variance were consistent with those used in the @koff2015expt publication as the basis for this approach over the earlier @hursh2008economic model.

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all hypothetical series. Specifically, individual series were screened for 1) bounce (i.e., local changes within an expected downward trend), 2) trend (i.e., molar changes within an expected downward trend), and reversals from zero (i.e., return of consumption at a higher price from 0 consumption at a lower price). Data were included in this methodological comparison so long as they met all three indicators of systematic demand data.

## Modeling Strategies

Three modeling strategies were evaluated in this simulation study. The core aim of these comparisons was to evaluate how both replacing 0 values with an individually-generated $A_{Lower}$ constant and altering the weighting of error influenced the correspondence between the @hursh2008economic and @koff2015expt models. The individual modeling strategies are listed below:

### Strategy 1 : @koff2015expt

The @koff2015expt model was applied to eligible data series at the individual level. This model was fit using the *optim* package in the R Statistical Program [@R-base]. Initial starts were derived based on the respective data and parameters $Q_0$ and $\alpha$ were carried forward into the final comparisons. The span constant $k$ was estimated based on the empirical range of the data with an added constant (0.5) to extend slightly below the lowest non-zero point of consumption. The same span constant was used across all models.

### Strategy 2: @hursh2008economic Model (Relative Error)

The @hursh2008economic was adjusted to dynamically determine the $A_{Lower}$ bound during the optimization procedure. Specifically, a customized loss function was prepared for use with the *optim* method whereby all values of 0 were replaced by $A_{Lower}$. The source code necessary to reproduce this approach and calculation has been provided for public review by the author at [https://github.com/miyamot0/AgnosticDemandModeling](https://github.com/miyamot0/AgnosticDemandModeling). Respective parameters were derived consistent with that of the @koff2015expt model.

### Strategy 3 : @hursh2008economic Model (Absolute Error)

The @hursh2008economic was adjusted further to dynamically determine the $A_{Lower}$ as a replacement for zero and also represented error (i.e., loss) in the linear scale. That is, residual error was reflected by ($10^{\hat{y}} - 10^{y}$). This model and resulting parameters were handled in the same manner as the others.

## Analytical Strategy

Given that both the @hursh2008economic and @koff2015expt models emerge from the same framework...

TODO: Equivalence test

# Results

```{r fig4, fig.cap="Comparisons of Modeling Strategies", fig.height=9}

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 100

sim <- SimulateDemand(nruns = nParticipants, setparams = setparams, sdindex = sdindex, x = x)

dataSet = sim$simvalues

passingFrame = beezdemand::CheckUnsystematic(dataSet)

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 0.5

dataFramePrep = data.frame(
  id      = 1:nParticipants,
  q0.HS   = numeric(length = nParticipants),
  q0.HSw  = numeric(length = nParticipants),
  q0.Koff = numeric(length = nParticipants),
  a.HS    = numeric(length = nParticipants),
  a.HSw   = numeric(length = nParticipants),
  a.Koff  = numeric(length = nParticipants),
  K       = numeric(length = nParticipants)
)

###

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

###

minQ0 = 0.01
maxQ0 = 200

for (id in 1:nParticipants) {

  currentData = dataSet[dataSet$id == id, ]

  dataFramePrep[id, "K"] = kSet

  fit.Koff <- optim(par  = c(max(currentData$y), -3),
                    fn   = min.RSS.EXPT,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data = currentData)

  dataFramePrep[id, c("q0.Koff", "a.Koff")] = fit.Koff$par

  fit.HS   <- optim(par    = c(fit.Koff$par[1], fit.Koff$par[2]),
                    fn     = min.RSS.EXPL,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data   = currentData)

  dataFramePrep[id, c("q0.HS", "a.HS")] = fit.HS$par

  fit.HWa <- optim(par    = c(fit.HS$par[1], fit.HS$par[2]),
                   fn     = min.RSS.EXPL.aw,
                   method = "L-BFGS-B",
                   lower  = c(0.01, -5),
                   upper  = c(max(currentData$y) * 1.25, 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw", "a.HSw")] = fit.HWa$par
}

keepers = passingFrame %>%
  filter(TotalPass == 3) %>%
  select(id) %>%
  pull()

frameToAnalyze = dataFramePrep %>%
  filter(id %in% keepers)

par(mfrow = c(2, 2),
    oma = c(0, 0, 0, 0))

comps.base <- lm(q0.Koff ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HS,
     main = expression(A[LOWER]~ "with Relative Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.base)

comps.weight <- lm(q0.Koff ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main = expression(A[LOWER]~"with Absolute Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.weight)

comps.base <- lm(a.Koff ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HS,
     main = expression(A[LOWER]~"with Relative Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.base)

comps.weight <- lm(a.Koff ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main = expression(A[LOWER]~ "with Absolute Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.weight)


```

The simulated data revealed a total of `r nrow(frameToAnalyze)` that met all criteria for systematic hypothetical purchase task data (from *N*=100). Scatterplot comparisons across each of the three modeling strategies are illustrated in \autoref{fig:fig4}. As illustrated in this figure, each of the three strategies differed with respect to their correspondence. 

## Strategy 1 vs. Strategy 2

Comparisons between Strategy 1 and 2 revealed a significant correlation for $Q_0$ (r = 0.8955, t = 17.313, df = 74, p-value < 2.2e-16). Similarly, there was a significant correlation for $\alpha$ (r = 0.9344, t = 22.515, df = 74, p-value < 2.2e-16). Scatterplot visualization of these two strategie indicated that, overall, the two approaches generally corresponded to a non-trivial degree.

## Strategy 1 vs. Strategy 3

Comparisons between Strategy 2 and 3 revealed a perfect correlation for $Q_0$ (r = 1, t = 82678, df = 74, p-value < 2.2e-16). Similarly, there was also a near perfect correlation for $\alpha$ (r = 0.9999, t = 22.515, df = 74, p-value < 2.2e-16). Scatterplot visualization of these two strategie indicated that, overall, the two approaches corresponded near perfectly.

# Discussion

The purpose of this technical report was to further explore the role of zero and non-zero asymptotes in two popular models of operant demand. Indeed, debate as to whether 0 consumption values contribute to the characterization of demand continue to date. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
