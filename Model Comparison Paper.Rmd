---
title             : "False Dilemmas in the Operant Demand Framework: Zero Values and Demand Curve Analysis"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn P. Gilroy"
    affiliation   : "1"
    corresponding : yes
    address       : "226 Audubon Hall Baton Rouge, Louisiana 70806"
    email         : "sgilroy1@lsu.edu"
    role:
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
 Shawn Patrick Gilroy is an Assistant Professor of School Psychology at Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu. The source code necessary to recreate this work is publicly hosted in a repository at: https://github.com/miyamot0/AgnosticDemandModeling 

abstract: |
 Operant translations of Behavioral Economic concepts and principles have enhanced the ability of researchers to characterize the effects of reinforcers under constraint and uncertainty. Operant Behavioral Economic models of choice (i.e., demand) have been particularly useful in evaluating how the consumption of reinforcers is affected by various ecological factors (e.g., price, limited resources). Prevailing perspectives in the Operant Demand Framework are derived from the framework presented in Hursh & Silberburg (2008). Few dispute the utility of this framework, though debate continues regarding how best to address the limitations associated with the logarithmic scale in this framework. At present, there are opposing views regarding the handling of zero values and under which situations alternative restatements of this framework are recommended, cf. Koffarnus et al. (2015). The purpose of this report was to review the relevance of asymptotes in the models derived from Hursh & Silberburg (2008) and the ways in which both the Hursh & Silberburg (2008) and Koffarnus et al. (2015) models may accomodate 0 consumption values. Simulations derived from those featured in Koffarnus et al. (2015) were used to determine whether the modifications to the Hursh & Silberburg (2008) would provide statistically equivalent estimates when controlling for the lower asymptotes and differences in error weighting. Simulations and proofs are provided to illustrate how neither the Hursh & Silberburg (2008) nor Koffarnus et al. (2015) cannot characterize zero and how both ultimately arrive at the same upper and lower asymptotes. Additional discussions are provided regarding whether competition and debate between models significantly enhances the generality and utility of methods in the Operant Demand Framework.

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : yes

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
---

\raggedbottom

```{r include = FALSE}
library(beezdemand)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(parameters)
library(tidyverse)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Contemporary methods for evaluating the consumption of goods and services using the Operant Demand Framework draw heavily from the methodology proposed in @hursh2008economic. This framework and this methodology have passed through several iterations [@hursh1987linear], with the latest form taking a non-linear shape that is modeled from an exponential decay process [@hursh2008economic]. Beyond the @hursh2008economic model, specifically, more recent extensions of the this framework have explored variants that evaluate consumption on the linear [@koff2015expt] or other log-like scales [@gilroy2021zbe].

The original implementation of the @hursh2008economic framework was modeled around an "s-type" form as the prototypical shape of the demand curve. Indeed, the original intent of @hursh2008economic was to have an upper asymptote reached at a low (or free) price and a lower asymptote reached as prices approached infinity, $\lim_{P \to \infty}$ [@gilroy2021zbe]. At present, all models derived this framework (e.g., @hursh2008economic, @koff2015expt) evaluate the demand for reinforcers with non-zero upper and lower asymptotes. That is, both models are bounded at an upper limit (i.e., $Q_0$) and progressed towards a lower asymptote in the "s-type" form. The non-zero asymptotes in the @hursh2008economic framework make good sense because the traditional values of interest were positive real values (i.e., not 0). Such quantities were necessary because the logarithmic representation of consumption is naturally undefined at 0.

In response to the statistical and philosophical issues related to the omission of 0 consumption values, @koff2015expt introduced a restatement of the @hursh2008economic framework. This variant of the @hursh2008economic framework was adjusted to examine changes in observed consumption on the linear scale. Specifically, an exponentiation of terms was performed such that the LHS (left-hand side) of the @hursh2008economic model were reflected in the linear scale. In this way, the LHS of the model (i.e., observed consumption) need not be submitted to the log transformation that previous prevented the use of the @hursh2008economic model. This modification drew considerable attention, as one of the largest issues associated with the log scale can be avoided. However, it warrants noting that portions of the RHS (right-hand side) of the @koff2015expt model remain on the log scale. Of particular importance, the span of the demand curve and the rate of exponential decay remain reflected in the log scale [@gilroy2021zbe]. It is for this reason that the span of the demand curve in this restated model cannot reach a true 0 point, and thus, is also bound by non-zero upper and lower asymptotes. Furthermore, the regressive process for logarithmic and linear models differs with respect to how error is weighted and this also introduces behavior that differs from that of the original @hursh2008economic framework [@gilroy2021zbe].

## Same Model, Different Error

The challenges associated with fitting models of operant demand (i.e., minimizing residual error) are increasingly discussed in research employing the the Operant Demand Framework [@gilroy2021zbe]. @gilroy2021zbe noted, among other things, that residual error is reflected differently in log and linear scales and that such differences affect model optimization. Briefly, changes in log space reflect relative differences while changes in the linear space reflect absolute differences. In most economic applications, relative error is typically preferred because the quantities of interest and their associated projections (i.e., $\hat{y}$) often span across multiple orders and quantities observed at higher orders would naturally be weighted more heavily than those at lower orders in the linear scale (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). For this reason, relative difference is often the default because this approach balances how each piece of information is weighted in the regression.

As an alternative to relative differences, which are weighted against observed values, absolute differences are more straightforward. That is, absolute error is simply the difference from some observed quantity and $\hat{y}$ regardless of the order (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). @gilroy2021zbe discussed how the departure from relative error can affect model optimization when observed levels of consumption vary considerably across orders. This has led to occasional inconsistencies wherein model fits across the log and linear variants of @hursh2008economic framework lead to different conclusions (e.g., shared alpha across varying dose-response curves).

## Different Error, Same Asymptotes

Apart from differences in how error is minimized across models in the Operant Demand Framework, there has been renewed attention regarding the upper and lower asymptotes of these models [@gilroy2021zbe]. Specifically, @gilroy2021zbe highlighted how neither the @hursh2008economic nor @koff2015expt models can characterize demand at 0. The topic of the zero-asymptote is not discussed at length here, but interested readers are encouraged to review @gilroy2021zbe for an exposition on why the inability to model demand at zero limits the utility of the Operant Demand Framework.

Revisiting the topic of asymptotes, two novel terms are introduced in this work: $A_{Upper}$ and $A_{Lower}$. The term $A_{Upper}$ refers to the upper non-zero bound of the demand curve. In models derived from the @hursh2008economic framework, this is simply the fitted parameter $Q_{0}$. That is, this is the absolute upper limit to the demand curve as prices approach 0, $\displaystyle{\lim_{P \to 0} f(x)= A_{Upper}}$. In contrast, the term $A_{Lower}$ refers the absolute lower bound of the demand curve. That is, this is the absolute lower limit to the demand curve as prices approach $\infty$, $\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}$. These two upper and lower extremes are separated by the span constant $\textit{k}$, which specify the distance in log units between these asymptotes. The notation of both $A_{Upper}$ and $A_{Lower}$ are noted below and these are proofed in greater detail in the Appendix of this work.

\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}

Further inspection of the $A_{Lower}$ and its derivation naturally evoke questions regarding how 0 consumption values could ever be included in a model derived from the @hursh2008economic framework. As noted in the bottom portion of *Equation 1*, both the @hursh2008economic and @koff2015expt models can never take a value of 0 because both asymptotes exist in a space where such a value cannot exist. As such, all predictions from these model must take values that emerge from within these bounds, $\hat{y} \in [ A_{Lower}, A_{Upper} ]$.

## Same Asymptotes, Same Spans

Revisiting *Equation 1*, these limits highlight the importance of the span parameter $\textit{k}$ and how it affects the values that can be projected (i.e., $\hat{y}$). In the original implementation of the @hursh2008economic framework, parameter $\textit{k}$ was specified based on the range of *observed consumption*. Since 0 consumption values were omitted from this implementation altogether, parameter $\textit{k}$ was directly linked to the observed data. Indeed, dropping the 0 values made the specification of this constant straightforward and largely unremarkable. That is, both parameter $\textit{k}$, $A_{Upper}$, and $A_{Lower}$ could be directly linked to the highest and lowest observed non-zero levels of consumption, respectively. A visualization of parameter $\textit{k}$ is provided in \autoref{fig:fig1CurveSpanEmpirical} with respect to non-zero consumption values.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes") +
  geom_point() +
  annotate("segment", 
           x      = 0.1, 
           xend   = 0.1, 
           y      = 100, 
           yend   = lowerAsymptote,
           colour = "grey40") +
  annotate("text", 
           x     = 0.175, 
           y     = 55, 
           angle = 90,
           adj   = 0.5,
           col   = "grey40",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = lowerAsymptote, 
           yend   = lowerAsymptote,
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "grey40",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = max(preFit$Q), 
           yend   = max(preFit$Q),
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "grey40",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = c(preFit$P, 10000),
                labels = c(preFit$P, 10000),
                limits = c(0.1, 10000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

```

Whereas the determination of parameter $\textit{k}$ in the @hursh2008economic implementation is essentially linked to observed (non-zero) consumption data, the determination of parameter $\textit{k}$ became more complicated in the implementation introduced by @koff2015expt. That is, parameter $\textit{k}$ could no longer be linked to observed (non-zero) consumption data because $A_{Lower}$ would have to extend *beyond* the range of non-zero consumption. Anecdotally, various labs and teams using this model have opted to either add a small constant to parameter $\textit{k}$ or fit this as a free parameter [@kaplan2018understanding]--both with the intent of *increasing* the span beyond the observed (non-zero) data, and thus, driving $A_{Lower}$ to a point closer to 0 on the linear scale. A visualization of this span-extending behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE, fig.height=3.5}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  scale_color_manual(values = c(
    "K (Empirical)" = "grey80",
    "K + 0.5"       = "grey60",
    "K + 1"         = "grey40",
    "K + 2"         = "grey20",
    "K + 3"         = "grey0"
  )) +
  annotate("text",
           x     = 100,
           y     = 90,
           adj   = 0,
           col   = "grey80",
           parse = TRUE,
           label = expression("K (Empirical): " ~ A[LOWER] == 5.442)) +
  annotate("text",
           x     = 100,
           y     = 75,
           adj   = 0,
           col   = "grey60",
           parse = TRUE,
           label = expression("K + 0.5: " ~ A[LOWER] == 1.7209)) +
  annotate("text",
           x     = 100,
           y     = 60,
           adj   = 0,
           col   = "grey40",
           parse = TRUE,
           label = expression("K + 1: " ~ A[LOWER] == 0.5442)) +
  annotate("text",
           x     = 100,
           y     = 45,
           adj   = 0,
           col   = "grey20",
           parse = TRUE,
           label = expression("K + 2: " ~ A[LOWER] == 0.0544)) +
  annotate("text",
           x     = 100,
           y     = 30,
           adj   = 0,
           col   = "grey0",
           parse = TRUE,
           label = expression("K + 3: " ~ A[LOWER] == 0.0054)) +
  scale_x_log10(breaks = c(preFit$P, 10000),
                labels = c(preFit$P, 10000),
                limits = c(0.1, 10000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none"
  )

```

\autoref{fig:fig2CurveSpanEmpiricalMod} illustrates how inflating the span constant affects $A_{Lower}$ and this has three effects on the demand curve. First, the rate of change in elasticity is jointly reflected by parameter $\alpha$ and the span of the demand curve [@gilroy2020interpretation]. Given that $\alpha$ is unitless, it covaries inversely with the size of the span constant--with relatively greater values reflecting rapid changes in $\hat{y}$ across prices and relatively lesser values reflecting gradual changes in $\hat{y}$ across prices [@gilroy2019exactsolve]. Second, $\textit{k}$ values (i.e., $k < \frac{e}{log(10)}$) influence both the span of the demand curve as well as the range of elasticity and inelasticity possible [@newman2020improved]. Third, and specific to the @koff2015expt model, larger $\textit{k}$ values serve to lessen the absolute difference between $A_{Lower}$ and true 0 on the linear scale--a point that cannot be characterized in the model. \autoref{fig:fig2CurveSpanEmpiricalMod} elegantly displays how this gap decreases proportionally with each unit increase in the span parameter $\textit{k}$.

## Unnecessary Distinctions

The sections above highlight the limited ways in which the @hursh2008economic and @koff2015expt models differ. Indeed, these approaches differ in terms of optimization (i.e., minimization of residual error) but share the same limitations with respect to non-zero asymptotes. Regarding the first point, residual error and optimization, the two models can provide functionally identical results when error representation is made comparable. That is, re-weighting the errors (i.e., relative to $\hat{y}$) in the @koff2015expt model yields fits and estimates that are nearly identical to that of the @hursh2008economic model *in the absence of 0 consumption value*, see \autoref{fig:fig3ComparisonNonzeroRelativeError}. However, such use cases are rare as there is little need to extend beyond the original implementation of the @hursh2008economic framework in the absence of 0 consumption data.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error", fig.height=3, warning=FALSE}

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           weights = 1/(predict(EXPT)^2),
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  rename(`Exponential`   = explPred,
         `Exponentiated (Absolute Error)` = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred) 

# kable(preFit.trim) %>%
#   kable_styling(full_width = TRUE)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P) 

ggplot(preFit.trim, aes(P, Q)) +
  #ggtitle("TODO: Add title") +
  geom_point(color = "black") +
  xlab("Unit Price") +
  ylab("Consumption") +
  geom_line(aes(P, Prediction), color = "grey40") +
  facet_wrap(~Model, ncol = 3) +
  scale_y_log10(breaks = c(1, 10, 100),
                labels = c(1, 10, 100),
                limits = c(1, 100)) +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom", 
    strip.background.x = element_blank()
  )

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

```

Regarding the second point, few researchers have explored $A_{Lower}$ and how this feature of the demand curve differs across series with and without 0 consumption values. This is a complex topic, especially in the linear restatement, because 0 is an observed quantity that cannot be characterized by either model. Evaluations of the @koff2015expt model and its behavior have revealed that the optimization of this model functions by both minimizing residual error as well as the difference between $A_{LOWER}$ and true 0. This is typically done by manually inflating the span (i.e., adding an additional constant) or fitting $\textit{k}$ parameter that extends beyond the non-zero range of consumption values. Regardless of the strategy, this is done to minimize the absolute distance between $A_{Lower}$ and 0 in the linear scale. Pragmatically, one could and would likely argue that such a small amount of error calls for little concern. That is, at $A_{Lower}$ would likely be considered *close enough* of an approximation of 0 consumption for most analysts.

Revisiting the point made above, consider the following scenario. Let us say that the interpretation of a fitted @koff2015expt model operates such that values at $A_{Lower}$ are a *close enough* approximation of 0 to proceed with demand curve analyses using a complete data set (i.e., including 0 consumption values). As such, considering the two approximate to one another, it stands to reason that inflating the span constant (beyond the range of non-zero consumption) for fitted demand curves and replacing 0 values with respective $A_{LOWER}$ limits would be equivalent. Assuming a sufficiently inflated $\textit{k}$ parameter, this approach should be equivalent because 1) $\hat{y}$ can be predicted well beyond observed non-zero levels and 2) and significantly inflated $\textit{k}$ parameters drive $A_{LOWER}$ close enough to true 0 on the linear scale that absolute differences are considered negligible. In such an arrangement, it stands to reason that the @koff2015expt model should perform in the same ways as the @hursh2008economic model, had errors been treated in terms of absolute difference and 0 consumption values treated as if they were $A_{Lower}$. Using this strategy, the full data set from \autoref{fig:fig1CurveSpanEmpirical} is fitted using the @koff2015expt model with the original data and the @hursh2008economic model with 0 consumption values replaced $A_{Lower}$. Specifically, the most inflated span and corresponding $A_{Lower}$ from \autoref{fig:fig2CurveSpanEmpiricalMod} were used in the optimization of each model, i.e. $A_{LOWER} = 10^{log{10}Q_0 - (k + 3)}$. The projections emerging from these fits are illustrated in \autoref{fig:fig4ComparisonZeroRelative}. 

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes", fig.height=3}

ggplot(preFit.g, aes(P, Q)) +
  geom_point() +
  geom_line(aes(P, Prediction), color = "grey40") +
  scale_x_log10(breaks = c(0.1, 10, 100, 1000, 10000),
                labels = c(0.1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  scale_y_log10(breaks = c(0.001, 0.01, 0.1, 1, 10, 100),
                labels = c(0.001, 0.01, 0.1, 1, 10, 100),
                limits = c(0.001, 100)) +
  facet_wrap(~Model, ncol = 3) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none", 
    strip.background.x = element_blank()
  )

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

Across the three fitted models, the @hursh2008economic model adjusted to accommodate 0 consumption values (with and without relative error weighting) provided nearly identical fits to the @koff2015expt model. Furthermore, de-weighting the @hursh2008economic to minimize the absolute difference between residuals produced the closest approximation between the two, see \autoref{tab:table1ComparisonZeroAbsolute}. This short example serves to highlight several details that often go unnoticed when using the the @koff2015expt model. First, this model does not characterize demand at zero. Rather, an inflated $\textit{k}$ parameter to drive $A_{Lower}$ down to a region *close enough* to 0 that the absolute difference between true 0 and $\hat{y}$ is negligible. However, 0 values can never emerge from this model, i.e. $0 \not\in [ A_{Lower}, A_{Upper} ]$. Second, this approach is functionally identical to the @hursh2008economic model with 0 values replaced with a common $A_{Lower}$ value when residual errors are de-weighted (i.e., absolute). That is, controlling for the same span, both the @hursh2008economic and @koff2015expt will hit the same $A_{Lower}$ that cannot be zero. 

## Planned Comparisons

The purpose of this technical report was to clarify and to evaluate the dimensions along which both the @hursh2008economic and @koff2015expt models differ. Specifically, the shared mathematical basis between the two should theoretically allow for modifications wherein both provide equivalent estimates--even in the presence of 0 consumption values. The primary research question was whether parameter estimates derived from the @hursh2008economic and @koff2015expt models would be made statistically equivalent by 1) accommodating absolute error and 2) replacing 0 consumption values with $A_{Lower}$ when applying the @hursh2008economic model.

# Methods

```{r }

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 500

sim <- SimulateDemand(nruns = nParticipants, setparams = setparams, sdindex = sdindex, x = x)

dataSet = sim$simvalues

passingFrame = beezdemand::CheckUnsystematic(dataSet)

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 0.5

```

## Data Generating Process

A total of `r nParticipants` hypothetical data series were simulated using peer-reviewed software previously submitted to peer-review [@kaplan2019r]. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to simulate hypothetical hypothetical purchase task data. The seed values and variance used to generate these hypothetical purchase task data were identical to those that were used in original @koff2015expt study. This specific data generating process was used as the basis for comparisons with the @hursh2008economic model, given that the @koff2015expt simulation was designed to be simulate to the "messy" data frequently observed in "real-world" purchase tasks.

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all hypothetical series. Specifically, individual series were screened for bounce, trend, and reversals from zero. The first criterion, bounce, refers to local changes within the *expected* downward trend across increasing prices. That is, it would be unexpected to see consumption increases immediately following a price increase. The second criterion, trend, refers to the molar change in consumption from the lowest to the highest price. That is, there is a certain amount of decrease expected across the full run of consumption across prices. Lastly, reversals from zero refer to the return of consumption at a higher price following the cessation of consumption at a lower price. Simulated data were carried forward into the final analysis so long as each series met all indicators of systematic hypothetical purchase task data.

## Modeling Strategies

A total of 3 modeling strategies were evaluated. Each of the 3 approaches was referenced as a different strategy for conducting demand curve analysis when 0 consumption values were observed in the data. The core aim of each was to compare estimated parameters across each strategy to determine whether the various strategies provided statistically equivalent results. Each of the modeling strategies is listed below in greater detail.

### Strategy 1: @koff2015expt Model

The @koff2015expt model (absolute error) was fitted to simulated consumption data at the individual-level. The model was fit using the *optim* package in the R Statistical Program [@R-base] due to its considerable flexibility in performing ordinary least squares regression. Initial starts were derived based on the respective data for parameter $Q_0$ and $\alpha$ was estimated on the log scale to support more equal step sizes in the optimization steps. The span constant $k$ was derived from the empirical range of the full data set with an added constant (0.5) to extend below the lowest non-zero point of consumption, as is common practice [@kaplan2018understanding]. The same span constant was used across all models to ensure comparable estimates for $Q_0$ and $\alpha$.

### Strategy 2: @hursh2008economic Model (Relative Error)

The @hursh2008economic model (relative error) was also fitted to simulated consumption data at the individual-level. During the fitting, 0 consumption values were replaced by an $A_{Lower}$ value that was generated dynamically based on parameters $Q_0$ and $\textit{k}$. That is, a customized loss function was prepared for use with the *optim* method. Consistent with efforts to main open and transparent science [@gilroy2019furthering], the source code necessary to reproduce this approach and these calculations has been posted for public review in a GitHub repository managed by the corresponding author, see Author Note. All other parameters were derived consistent with that of Strategy 1.

### Strategy 3: @hursh2008economic Model (Absolute Error)

The @hursh2008economic model (absolute error) was fitted to simulated consumption data at the individual-level as well. This strategy was identical to Strategy 2 with the exception of how loss was interpreted during optimization. That is, a customized loss function was prepared and residual error was represented in terms of absolute differences, i.e. $(10^{\hat{y}} - 10^{y})^2$. All other parameters were derived consistent with that of Strategies 1 and 2.

## Analytical Strategy

Individual comparisons were prepared for both parameters $Q_0$ and $\alpha$ resulting from each of the 3 strategies. Given that both the @hursh2008economic and @koff2015expt models emerge from the same framework, tests of equivalence were performed in a pairwise fashion (i.e., Strategy 1 vs. 2, Strategy 1 vs. 3). The *tost* method in the *equivalence* R package [@robinson2016package] was used to perform two one-sided t-tests (TOSTs) with paired estimates resulting from each strategy. That is, the focus was not on determining difference between strategies but instead on determining *equivalence* between strategies and fitted estimates. Across all tests, corrections were applied due to presence of repeated comparisons, i.e. $p=0.05/4=0.0125$.

# Results

```{r fig5scatterplots, fig.cap="Scatterplot Comparisons Resulting from Modeling Strategies", fig.height=7}

dataFramePrep = data.frame(
  id      = 1:nParticipants,
  q0.HS   = numeric(length = nParticipants),
  q0.HSw  = numeric(length = nParticipants),
  q0.Koff = numeric(length = nParticipants),
  a.HS    = numeric(length = nParticipants),
  a.HSw   = numeric(length = nParticipants),
  a.Koff  = numeric(length = nParticipants),
  K       = numeric(length = nParticipants)
)

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

minQ0 = 0.01
maxQ0 = 200

for (id in 1:nParticipants) {

  currentData = dataSet[dataSet$id == id, ]

  dataFramePrep[id, "K"] = kSet

  fit.Koff <- optim(par  = c(max(currentData$y), -3),
                    fn   = min.RSS.EXPT,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data = currentData)

  dataFramePrep[id, c("q0.Koff", "a.Koff")] = fit.Koff$par

  fit.HS   <- optim(par    = c(fit.Koff$par[1], fit.Koff$par[2]),
                    fn     = min.RSS.EXPL,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data   = currentData)

  dataFramePrep[id, c("q0.HS", "a.HS")] = fit.HS$par

  fit.HWa <- optim(par    = c(fit.HS$par[1], fit.HS$par[2]),
                   fn     = min.RSS.EXPL.aw,
                   method = "L-BFGS-B",
                   lower  = c(0.01, -5),
                   upper  = c(max(currentData$y) * 1.25, 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw", "a.HSw")] = fit.HWa$par
}

keepers = passingFrame %>%
  filter(TotalPass == 3) %>%
  select(id) %>%
  pull()

frameToAnalyze = dataFramePrep %>%
  filter(id %in% keepers)

par(mfrow = c(2, 2),
    oma = c(0, 0, 0, 0))

comps.base <- lm(q0.Koff ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HS,
     main = expression(A[LOWER]~ "with Relative Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.base)

comps.weight <- lm(q0.Koff ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main = expression(A[LOWER]~"with Absolute Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.weight)

comps.base <- lm(a.Koff ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HS,
     main = expression(A[LOWER]~"with Relative Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.base)

comps.weight <- lm(a.Koff ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main = expression(A[LOWER]~ "with Absolute Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.weight)

```

The data generating process yielded a total of `r nrow(frameToAnalyze)` series that met all 3 indicators of systematic purchase task data (from *N*=`r nParticipants`; `r round((nrow(frameToAnalyze)/nParticipants) * 100, 2)`%). The correspondence between parameter estimates was visualized in scatter plots and these are displayed in \autoref{fig:fig5scatterplots}. Overall, the observed relationships indicated varying degrees of correspondence between each of the different strategies. These results are presented in greater detail below.

## Strategy 1 vs. Strategy 2

```{r strat1vsstrat2, include=TRUE}

library(equivalence)

tost.1v2.q0 = tost(frameToAnalyze$q0.Koff, 
                   frameToAnalyze$q0.HS, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.1v2.q0 = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HS)

tost.1v2.a = tost(frameToAnalyze$a.Koff, 
                  frameToAnalyze$a.HS, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.1v2.a = cor.test(frameToAnalyze$a.Koff, frameToAnalyze$a.HS)
```

An evaluation of the relationship between Strategy 1 and 2 revealed strong correlations for $Q_0$ (r=`r cor.1v2.q0$estimate`, t=`r cor.1v2.q0$statistic`, df=`r cor.1v2.q0$parameter`, `r scales::pvalue(cor.1v2.q0$p.value, accuracy = .0125, add_p = TRUE)`) as well as for $\alpha$ (r=`r cor.1v2.a$estimate`, t=`r cor.1v2.a$statistic`, df=`r cor.1v2.a$parameter`, `r scales::pvalue(cor.1v2.a$p.value, accuracy = .0125, add_p = TRUE)`). The results of TOSTs for $Q_0$ and $\alpha$ were non-significant (`r scales::pvalue(tost.1v2.q0$tost.p.value, accuracy = .0125, add_p = TRUE)`) and significant (`r scales::pvalue(tost.1v2.a$tost.p.value, accuracy = .0125, add_p = TRUE)`), respectively. That is, the results of equivalence testing indicated that both two strategies provided statistically equivalent estimates for $\alpha$ but not for $Q_0$.

## Strategy 1 vs. Strategy 3

```{r strat1vsstrat3, include=TRUE}

tost.1v3.q0 = tost(frameToAnalyze$q0.Koff, 
                   frameToAnalyze$q0.HSw, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.1v3.q0 = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

tost.1v3.a = tost(frameToAnalyze$a.Koff, 
                  frameToAnalyze$a.HSw, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.1v3.a = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

```

Evaluations of the relationship between Strategy 2 and 3 revealed perfect correlations for $Q_0$ (r=`r cor.1v3.q0$estimate`, t=`r cor.1v3.q0$statistic`, df=`r cor.1v3.q0$parameter`, `r scales::pvalue(cor.1v3.q0$p.value, accuracy = .0125, add_p = TRUE)`) as well as for $\alpha$ (r=`r cor.1v3.a$estimate`, t=`r cor.1v3.a$statistic`, df=`r cor.1v3.a$parameter`, `r scales::pvalue(cor.1v3.a$p.value, accuracy = .0125, add_p = TRUE)`). Similarly, the results of TOSTs for $Q_0$ (`r scales::pvalue(tost.1v3.q0$tost.p.value, accuracy = .0125, add_p = TRUE)`) and $\alpha$ (`r scales::pvalue(tost.1v3.a$tost.p.value, accuracy = .0125, add_p = TRUE)`) were significant overall. That is, the results of equivalence testing indicated that both strategies yielded statistically equivalent model parameters for both $Q_0$ and $\alpha$.

# Discussion

The Operant Demand Framework is increasingly applied to various issues of clinical and societal significance [@hursh2013behavioral; @reed2013behavioral]. Indeed, such methods and analyses have merit in informing various areas of public policy [@hursh2013behavioral; @roma2017progress]. Furthermore, formal models of operant demand are increasingly supported across a range of mature statistical packages and tools [@kaplan2019r; @gilroy2018demand]. Within these applications and tools, various modeling strategies are available to researchers with few firm guidelines to assist analysts in navigating among these options. The purpose of this technical report was to review the pragmatic and mathematical underpinnings of the two prevailing models derived from the framework of @hursh2008economic--the @hursh2008economic (Exponential) and the @koff2015expt (Exponentiated) models. The goals of this report were three-fold: First, one of the goals of this report was to review and characterize the behavior of the lower limit ($A_{LOWER}$) for both the Exponential and Exponentiated models as this relates to both the presence and absence of 0 consumption values. Second, and building from the first, another goal was to evaluate whether the @hursh2008economic model could be adjusted to provide equivalent estimates to the @koff2015expt model when addressing $A_{LOWER}$ and controlling for differences in error weighting. The third and final goal was to clarify the shared mathematical basis for each model and affirm the ways in which each of the models reviewed here have extended the Operant Demand Framework.

Regarding the first point, this report provides an in-depth discussion and review of how 0 consumption values have, thus far, been included in models derived from the @hursh2008economic framework. In sum, neither the Exponential nor the Exponentiated models characterize demand at 0. Both are equally restricted to the same non-zero lower asymptote, $A_{Lower}$, which can never be 0. Regardless of whether 0 consumption values are included in the regression, this mathematically cannot succeed as currently specified in the @hursh2008economic framework. As such, it warrants re-stating that the approach put forward in @koff2015expt is not a complete solution for including *and characterizing* 0 consumption values with the @hursh2008economic framework. As an alternative, others have presented alternative means of achieving a zero asymptote if such behavior is necessary to answer specific research questions [@gilroy2021zbe].

The second aim of this study was to outline the shared mathematical basis of the Exponential and Exponentiated models and lay a case for why the models should not be so strongly distinguished. That is, controlling for differences in scale and $A_{LOWER}$, it is trivial to arrive at equivalent estimates for both models regardless of whether 0 consumption values are included in the data set. The results of computer simulation confirmed that the two models provide statistically equivalent estimates when controlling for such differences. This is because $A_{LOWER}$ is as low as $\hat{y}$ can go and replacing 0 values with this hard lower limit ultimately achieves the same goal. However, this fact is not stated with the intent of favoring any specific model as the de facto standard for use in the Operant Demand Framework. Rather, the true value here is in finding that these historically opposing strategies are more similar than they are different. As such, each approach has value and future efforts are better directed towards a greater understanding the properties of the  @hursh2008economic framework rather than towards reinforcing any prior biases towards or against any particular implementation.

Lastly, the final goal of this report was to affirm the contributions that emerged following the introduction of each variant of the @hursh2008economic framework. Stated plainly, the proponents of each modeling strategy were successful in advancing both the utility and scope of the Operant Demand Framework. Although the @hursh2008economic model can replicate the behavior of the @koff2015expt with trivial adjustments, the initiative of the @koff2015expt study warrants considerable praise even today. That is, @koff2015expt led the charge towards addressing the historically problematic issue of removing data not occurring at random. For decades, substantial portions of otherwise valid consumption data were never carried forward into formal analyses for study. Regardless of which implementation is considered preferred in a given case, the Operant Demand Framework is better now that 0 consumption values are now available for formal analysis.

## Future Directions in Operant Demand

In closing, future efforts in the Operant Demand Framework should be directed towards advancing the applicability and the generality of an Operant Behavioral Economic perspective and the Operant Demand Framework. Towards this end, the intent and mission of the original @koff2015expt study regarding 0 consumption values is as valid and true today as it was when first published. However, debate regarding model superiority (or inferiority) in the absence of formal tests does not enhance the Operant Demand Framework perspective in any appreciable manner. As such, the reader should be cautioned against thinking that any single model is inherently "true", "better", or otherwise superior in the absence of careful statistical evaluation. Indeed, future contributions may drop the reliance on the log scale [@gilroy2021zbe] or choose not to extend the framework altogether [@newman2020improved]. Regardless, such approaches and advances should be met with scientific scrutiny and cautious optimism rather than outright disregard.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage

# Appendix

Several proofs are provided here to illustrate how the upper and lower asymptotes are determined. Despite the shared mathematical basis, derivations of each are provided below.

## Modified Hursh & Silberburg (2008) Optimization (Relative Error)

$$
e(n) = \begin{cases} 
\hat{y} - log_{10}y         &    \mbox{if } y \not= 0 \\
\hat{y} - log_{10}A_{LOWER} &    \mbox{if } y = 0 \\
\end{cases}
$$

## Modified Hursh & Silberburg (2008) Optimization (Absolute Error)

$$
e(n) = \begin{cases} 
10^{\hat{y}} - 10^{log_{10}y}         &    \mbox{if } y \not= 0 \\
10^{\hat{y}} - 10^{log_{10}A_{LOWER}} &    \mbox{if } y = 0 \\
\end{cases}
$$

## Hursh & Silberburg (2008) Proofs

### $A_{UPPER}$ at $P = 0$

$$
\begin{aligned}
log_{10}A_{Upper} &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * 0} - 1) \\
 &= log_{10}Q_0 + k(e^{0} - 1) \\
 &= log_{10}Q_0 + k(1 - 1) \\
 &= log_{10}Q_0 + k(0) \\
 &= log_{10}Q_0\\
 A_{Upper} &= Q_0
\end{aligned}
$$

Note: Euler's constant raised to the power of 0 is equal to a value of 1. This essentially zeroes out the $\textit{k}$ constant, leaving just the $Q_0$ parameter at 0 $P$.

### $A_{LOWER}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
log_{10}A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * \infty} - 1) \\
 &= log_{10}Q_0 + k(e^{-\infty} - 1) \\
 &= log_{10}Q_0 + k(0 - 1) \\
 &= log_{10}Q_0 + k(-1) \\
 &= log_{10}Q_0 - k \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$

Note: Euler's constant raised to the power of $-\infty$ equates to a value of 0. That is, $e^{-\infty}=\frac{1}{e^\infty}=\frac{1}{\infty} \approx 0$. This is has effect of making the value in parentheses equal to $-1$, which in turn results in the full subtraction of quantity $k$ from $log_{10}Q_0$.

## Koffarnus et al. (2015) Proofs

### $A_{UPPER}$ at $P = 0$

$$
\begin{aligned}
A_{Upper} &= Q_0 * 10^{k(e^{-\alpha * Q_0 * 0} - 1)}  \\
 &= Q_0 * 10^{k(e^{0} - 1)}  \\
 &= Q_0 * 10^{k(1 - 1)}  \\
 &= Q_0 * 10^{k(0)}  \\
 &= Q_0 * 10^{0}  \\
 &= Q_0 * 1  \\
 &= Q_0  \\
\end{aligned}
$$

### $A_{LOWER}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= Q_0 * 10^{k(e^{-\alpha * Q_0 * \infty} - 1)}  \\
 &= Q_0 * 10^{k(e^{-\infty} - 1)}  \\
 &= Q_0 * 10^{k(0 - 1)}  \\
 &= Q_0 * 10^{k(-1)}  \\
 &= Q_0 * 10^{-k}  \\
log_{10}A_{Lower} &= log_{10}Q_0 + (-k)  \\
 &= log_{10}Q_0 - k  \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$

