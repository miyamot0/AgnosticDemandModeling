---
title             : "Unnecessary Model Distinctions in the Operant Demand Framework: A Review and Simulations of Zero Values in Demand Curve Analysis"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn P. Gilroy"
    affiliation   : "1"
    corresponding : yes
    address       : "226 Audubon Hall Baton Rouge, Louisiana 70806"
    email         : "sgilroy1@lsu.edu"
    role:
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
 Shawn Patrick Gilroy is an Assistant Professor of School Psychology at Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu. The source code necessary to recreate this work is publicly hosted in a repository at: https://github.com/miyamot0/AgnosticDemandModeling 

abstract: |
 Operant translations of Behavioral Economic concepts and principles have enhanced the ability of researchers to characterize the effects of reinforcers under constraint and uncertainty. Operant Behavioral Economic models of choice (i.e., demand) have been particularly useful in evaluating how the consumption of reinforcers is affected by various ecological factors (e.g., price, limited resources). Prevailing perspectives in the Operant Demand Framework are derived from the framework presented in Hursh & Silberburg (2008). Few dispute the utility of this framework, though debate continues regarding how best to address the limitations associated with the logarithmic scale in this framework. At present, there are opposing views regarding the handling of zero values and under which situations alternative restatements of this framework are recommended, cf. Koffarnus et al. (2015). The purpose of this report was to review the relevance of asymptotes in the models derived from Hursh & Silberburg (2008) and the ways in which both the Hursh & Silberburg (2008) and Koffarnus et al. (2015) models may accomodate 0 consumption values. Simulations derived from those featured in Koffarnus et al. (2015) were used to determine whether the modifications to the Hursh & Silberburg (2008) would provide statistically equivalent estimates when controlling for the lower asymptotes and differences in error weighting. Simulations and proofs are provided to illustrate how neither the Hursh & Silberburg (2008) nor Koffarnus et al. (2015) cannot characterize zero and how both ultimately arrive at the same upper and lower asymptotes. Additional discussions are provided regarding whether competition and debate between models significantly enhances the generality and utility of methods in the Operant Demand Framework.

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : yes

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
---

\raggedbottom

```{r include = FALSE}
library(beezdemand)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(parameters)
library(tidyverse)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Contemporary methods for evaluating the consumption of goods and services using the Operant Demand Framework are heavily influenced by the methodology proposed in @hursh2008economic. This framework and methodology has evolved through several forms [@hursh1987linear] and the latest form takes a non-linear (i.e., "S"-type) shape that features an exponential decay process [@hursh2008economic]. This strategy for evaluating the effects of price on consumption had achieved widespread adoption and has inspired derivative models the evaluate consumption on either the linear [@koff2015expt] or some other log-like scale for consumption [@gilroy2021zbe].

The original implementation of the @hursh2008economic framework was modeled from the notion that the prototypical shape of the demand curve was an "S"-type form bounded at upper and lower limits. The original intent of @hursh2008economic was to have an upper asymptote defined at a price of zero, i.e. $\lim_{P \to - \infty}$, and a lower asymptote reached as prices approached infinity, $\lim_{P \to \infty}$ [@gilroy2021zbe]. Models derived from this framework (e.g., @hursh2008economic, @koff2015expt) all evaluate the demand for reinforcers with non-zero upper and lower asymptotes and these models are bounded at an upper limit (i.e., $Q_0$) and progress towards a lower asymptote in the "S"-type form. Non-zero upper and lower asymptotes in the @hursh2008economic framework make good sense because the original values of interest were positive real values (i.e., not 0). Such quantities are typical because the logarithmic representation of consumption is undefined at 0.

In response to the statistical and philosophical issues related to the omission of 0 consumption values, @koff2015expt introduced a restatement of the @hursh2008economic framework. This variant of the @hursh2008economic framework was adjusted to examine changes in observed consumption on the linear scale. Specifically, an exponentiation of terms was performed such that the LHS (left-hand side) of the @hursh2008economic model was reflected in the linear scale. In this way, the LHS of the model (i.e., observed consumption) need not be submitted to the log transformation that previously prevented the use of the @hursh2008economic model. This modification drew considerable attention, as one of the largest issues associated with the log scale can be avoided. However, it warrants noting that portions of the RHS (right-hand side) of the @koff2015expt model remain on the log scale. Specifically, the span of the demand curve and the rate of exponential decay remain reflected in the log scale [@gilroy2021zbe]. It is for this reason that the span of the demand curve in this restated model cannot reach a true 0 point, and thus, is also bound by non-zero upper and lower asymptotes despite accommodating 0 consumption values in model regression. Additionally, the regressive process for logarithmic and linear models differs with respect to how error weighting and this also introduces behavior that differs between models [@gilroy2021zbe].

## Same Model But Different Error

The challenges associated with fitting models of operant demand (i.e., minimizing residual error) are increasingly discussed by researchers applying the Operant Demand Framework [@gilroy2021zbe]. @gilroy2021zbe noted, among other things, that residual error is reflected differently in log and linear scales and that such differences can affect model optimization and resulting parameters. Briefly, changes in log space reflect relative differences while changes in the linear space reflect absolute differences. In most economic applications, relative error is typically preferred because the quantities of interest and their associated projections (i.e., $\hat{y}$) typically span across multiple orders and quantities observed at higher orders would naturally be weighted more heavily than those at lower orders in the linear scale (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). For this reason, relative difference is often the default because this balances how each piece of information is weighted in the regression.

As an alternative to relative differences, which are weighted against observed values, absolute differences are more straightforward. That is, absolute error is simply the difference from some observed quantity and $\hat{y}$ regardless of the order (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). @gilroy2021zbe discussed how the departure from relative error can affect model optimization when observed levels of consumption vary considerably across orders. This has led to occasional inconsistencies wherein model fits across the log and linear variants of @hursh2008economic framework lead to different conclusions (e.g., shared alpha across varying dose-response curves).

## Different Error But Same Asymptotes

There has been renewed attention regarding the upper and lower bounds of models in the @hursh2008economic framework. Specifically, @gilroy2021zbe highlighted how neither the @hursh2008economic nor the @koff2015expt model can characterize demand at 0. The topic of the zero-asymptote is not discussed at length here, but interested readers are encouraged to review @gilroy2021zbe for an exposition on why the inability to model demand at zero limits the utility of the Operant Demand Framework.

Revisiting asymptotes, two novel terms are introduced in this work: $A_{Upper}$ and $A_{Lower}$. The term $A_{Upper}$ refers to the upper bound of the demand curve. In models derived from the @hursh2008economic framework, this is simply the fitted parameter $Q_{0}$. This is the absolute upper limit to the demand curve as prices approach 0, i.e. $\displaystyle{f(0)= A_{Upper}}$. In contrast, the term $A_{Lower}$ refers the absolute lower bound of the demand curve. That is, this is the absolute lower limit to the demand curve as prices approach $\infty$, i.e.$\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}$. These two upper and lower extremes are separated by the span constant $\textit{k}$, which specify the distance in log units between these asymptotes [@gilroy2021zbe]. The notation of both $A_{Upper}$ and $A_{Lower}$ are noted below and these are proofed in greater detail in the Appendix of this work.

```{=tex}
\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}
```
Further inspection of the $A_{Lower}$ and its derivation naturally evoke questions regarding how these models could accommodate 0 consumption values. As noted in the bottom portion of *Equation 1*, both the @hursh2008economic and @koff2015expt models can never represent a value of 0 because such a value cannot exist between these bounds. Regarding the @koff2015expt model, this introduces a complex situation wherein 0 consumption values can be included in the regression but all predicted values could never reach this value.

## Same Asymptotes and Same Spans

Revisiting 0 consumption values in the @hursh2008economic requires a more thorough understanding of how the span parameter $\textit{k}$ influences the range of values that may be predicted (i.e., $\hat{y}$). In the original implementation of the @hursh2008economic framework, $\textit{k}$ represented the range of observed, non-zeroconsumption values. Since 0 consumption values were omitted from this implementation altogether, parameter $\textit{k}$ was directly linked to the upper and lower limits of the observed data. By all accounts, dropping 0 consumption values made the specification of this constant straightforward because parameter $\textit{k}$, $A_{Upper}$, and $A_{Lower}$ were directly linked to the highest and lowest observed non-zero levels of consumption, respectively. A visualization of parameter $\textit{k}$ is provided in \autoref{fig:fig1CurveSpanEmpirical} with respect to non-zero consumption values.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes") +
  geom_point() +
  annotate("segment", 
           x      = 0.1, 
           xend   = 0.1, 
           y      = 100, 
           yend   = lowerAsymptote,
           colour = "grey40") +
  annotate("text", 
           x     = 0.175, 
           y     = 55, 
           angle = 90,
           adj   = 0.5,
           col   = "grey40",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = lowerAsymptote, 
           yend   = lowerAsymptote,
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "grey40",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = max(preFit$Q), 
           yend   = max(preFit$Q),
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "grey40",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = c(preFit$P, 10000),
                labels = c(preFit$P, 10000),
                limits = c(0.1, 10000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

```

Whereas the determination of parameter $\textit{k}$ in the @hursh2008economic implementation is linked to observed (non-zero) consumption data, the determination of parameter $\textit{k}$ became more complicated in the implementation introduced by @koff2015expt. This added complexity emerged because parameter $\textit{k}$ was no longer be linked to non-zero consumption data because the intended $A_{Lower}$ was projected *beyond* the range of non-zero consumption data and towards a quantity nearer to 0. This represented a novel use of parameter $\textit{k}$ and various teams have constructed strategies to assist in driving $A_{Lower}$ *beyond* the range of non-zero consumption. For example, some have added a constant to parameter $\textit{k}$ (derived from non-zero consumption values) or fitted this as a free parameter [@kaplan2018understanding]. Regardless, the rationale for both was to inflate the span of the demand curve to and drive $A_{Lower}$ to a lower point. A visualization of this span-inflating behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE, fig.height=3.5}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  scale_color_manual(values = c(
    "K (Empirical)" = "grey80",
    "K + 0.5"       = "grey60",
    "K + 1"         = "grey40",
    "K + 2"         = "grey20",
    "K + 3"         = "grey0"
  )) +
  annotate("text",
           x     = 100,
           y     = 90,
           adj   = 0,
           col   = "grey80",
           parse = TRUE,
           label = expression("K (Empirical): " ~ A[LOWER] == 5.442)) +
  annotate("text",
           x     = 100,
           y     = 75,
           adj   = 0,
           col   = "grey60",
           parse = TRUE,
           label = expression("K + 0.5: " ~ A[LOWER] == 1.7209)) +
  annotate("text",
           x     = 100,
           y     = 60,
           adj   = 0,
           col   = "grey40",
           parse = TRUE,
           label = expression("K + 1: " ~ A[LOWER] == 0.5442)) +
  annotate("text",
           x     = 100,
           y     = 45,
           adj   = 0,
           col   = "grey20",
           parse = TRUE,
           label = expression("K + 2: " ~ A[LOWER] == 0.0544)) +
  annotate("text",
           x     = 100,
           y     = 30,
           adj   = 0,
           col   = "grey0",
           parse = TRUE,
           label = expression("K + 3: " ~ A[LOWER] == 0.0054)) +
  scale_x_log10(breaks = c(preFit$P, 10000),
                labels = c(preFit$P, 10000),
                limits = c(0.1, 10000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none"
  )

```

\autoref{fig:fig2CurveSpanEmpiricalMod} illustrates how inflating the span constant affects $A_{Lower}$ and this has three appreciable effects on the demand curve. First, the rate of change in elasticity is jointly reflected by parameter $\alpha$ and the span of the demand curve [@gilroy2020interpretation]. Given that $\alpha$ is a unitless quantity, it covaries inversely with the size of the span constant. That is, relatively greater $\alpha$ values reflect rapid changes in $\hat{y}$ across prices and relatively lesser values reflecting gradual changes in $\hat{y}$ across prices. Second, $\textit{k}$ values (i.e., $k < \frac{e}{log(10)}$) influence both the span of the demand curve as well as the range of elasticity and inelasticity possible [@newman2020improved; @gilroy2019exactsolve]. That is, $\textit{k}$ values that do not permit a span of 1 log unit necessarily would restrict the range of possible elasticity values (i.e., analytic solutions for $P_{MAX}$ are not possible). Third, and most relevant to the @koff2015expt model, inflated $\textit{k}$ values serve to lessen the difference between $A_{Lower}$ and 0. That is, the distance between $A_{Lower}$ and 0 is is *lessened*, but no $\textit{k}$ value could ever yield a span to reaches true 0. \autoref{fig:fig2CurveSpanEmpiricalMod} provides an elegant display of how this gap decreases proportionally with each unit increase in the span parameter $\textit{k}$.

## Unnecessary Model Distinctions

The sections above outline the various similarities and few differences between two of the most commonly-used forms of the @hursh2008economic framework--the @hursh2008economic model and the @koff2015expt restatement of the @hursh2008economic model. Specifically, these two modeling strategies differ in terms of optimization (i.e., minimization of residual error) but share the limitations related to asymptotes. Regarding the first point, residual error and optimization, the two models can provide functionally identical results when the handling of residual error is made comparable. That is, re-weighting the errors (i.e., relative to $\hat{y}$) in the @koff2015expt model yields fits and estimates that can be nearly identical to that of the @hursh2008economic model *in the absence of 0 consumption value.* Alternatively, the @hursh2008economic model can be made to provide estimates comparable to the @koff2015expt model by adjusting residual error to be sensitive to the order of predicted consumption (e.g., $E_i = E_i * 10^{\hat{y}}$, $E_i=10^{\hat{y}}-10^{y}$). A visualization of inter-related arrangements are illustrated in \autoref{fig:fig3ComparisonNonzeroRelativeError}.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error", fig.height=3, warning=FALSE}

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           weights = 1/(predict(EXPT)^2),
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  rename(`Exponential`   = explPred,
         `Exponentiated (Absolute Error)` = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred) 

# kable(preFit.trim) %>%
#   kable_styling(full_width = TRUE)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P) 

ggplot(preFit.trim, aes(P, Q)) +
  #ggtitle("TODO: Add title") +
  geom_point(color = "black") +
  xlab("Unit Price") +
  ylab("Consumption") +
  geom_line(aes(P, Prediction), color = "grey40") +
  facet_wrap(~Model, ncol = 3) +
  # scale_y_log10(breaks = c(1, 10, 100),
  #               labels = c(1, 10, 100),
  #               limits = c(1, 100)) +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom", 
    strip.background.x = element_blank()
  )

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

preFit.point.1 <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0),
  Model = "Exponentiated"
)

preFit.point.2 = preFit %>%
  select(P, Q) %>%
  mutate(Model = "Exponential (Absolute)")

preFit.point.full = rbind(preFit.point.1,
                          preFit.point.2)

```

Regarding the second point, few have discussed $A_{Lower}$ and how this aspect of the demand curve influences results in models derived from the @hursh2008economic framework. This is a complex topic, especially so in the @koff2015expt restatement, because consumption values observed at 0 are a quantity that cannot be *predicted* by models that reflect the range of consumption in log units. In attempting to accommodate observed 0 values, modeling based on the @hursh2008economic framework must minimize *two* sources of error instead of one. That is, the modeling must minimize residual error (as typical) and minimize the distance between $A_{LOWER}$ and 0. For instance, an application of the @koff2015expt model where $\textit{k}$ is included as a fitted parameter simultaneously optimizes demand intensity, rates of change in elasticity, and a span constant (i.e., $A_{LOWER}$). As noted above, $A_{LOWER}$ is driven lower by inflating the span constant towards some non-zero number that is *reasonably* proximate to 0. Pragmatically, one could and would likely argue that such a small amount of error calls for little concern and advocates of the @koff2015expt strategy would likely note that $A_{Lower}$ could be considered *close enough* of an approximation of 0 consumption values to enable analyses of the complete data set.

Revisiting logic around a *close enough* approximation of 0 consumption values, consider the following hypothetical. Let us say that the interpretation of a fitted @koff2015expt model optimizes such that values at $A_{Lower}$ are a *close enough* approximation of 0 to proceed with demand curve analyses using a complete data set that includes 0 consumption values. Following this logic (i.e., $A_{LOWER} \cong 0$), it stands to reason that treating sufficiently low $A_{LOWER}$ values and 0 consumption values *should* replicate the behavior of the @koff2015expt model. Assuming an inflated $\textit{k}$ parameter, similar or identical estimates should result because 1) $\hat{y}$ can be predicted beyond the range of observed non-zero levels and 2) and the resulting $A_{LOWER}$ should be *close enough* to 0 on the linear scale that differences between $A_{LOWER}$ and 0 would be considered negligible. Controlling for differences in terms of error representation, it stands to reason that the @hursh2008economic model would provide similar estimates had 0 consumption values been replaced by respective $A_{LOWER}$ values and error minimization been reflected in terms of absolute differences.

In a demonstration of this modified @hursh2008economic approach, the full data set from \autoref{fig:fig1CurveSpanEmpirical} can fitted with an inflated $\textit{k}$ parameter and 0 consumption values are replaced with respective $A_{Lower}$ values. Specifically, the most inflated span and corresponding $A_{Lower}$ from \autoref{fig:fig2CurveSpanEmpiricalMod} were used in this example demonstration, i.e. $A_{LOWER} = 10^{log{10}Q_0 - (k + 3)}$. The results of this modified @hursh2008economic approach are illustrated along with the @koff2015expt approach are illustrated in \autoref{fig:fig4ComparisonZeroRelative}.

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes", fig.height=3}

ggplot(subset(preFit.g, Model != "Exponential (Relative)"), aes(P, Q)) +
  geom_point(data = preFit.point.full) +
  geom_line(aes(P, Prediction), color = "grey40") +
  scale_x_log10(breaks = c(0.1, 10, 100, 1000, 10000),
                labels = c(0.1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  facet_wrap(~Model, ncol = 3) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none", 
    strip.background.x = element_blank()
  )

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  select(-c(`Exponential.Relative`)) %>%
  relocate(Exponentiated, .after = Q) %>%
  mutate(`Q.Mod` = Q) %>%
  mutate(Q = preFit.point.1$Q) %>%
  relocate(`Exponential.Absolute`, .after = `Q.Mod`) %>%
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

Across the fitted models, the @hursh2008economic model adjusted to accommodate 0 consumption values provided nearly identical fits to the @koff2015expt model, see \autoref{tab:table1ComparisonZeroAbsolute}. As such, this example highlights several details that are often unnoticed when using the @koff2015expt model. First, this model does not characterize demand at zero levels. Rather, an inflated $\textit{k}$ parameter to drives $A_{Lower}$ to a quantity *close enough* to 0 that the absolute difference between true 0 and $\hat{y}$ is negligible. This is the best that this approach can achieve because 0 values can never be predicted from this model. Second, this approach is functionally identical to the @hursh2008economic model when 0 values replaced with respective $A_{Lower}$ values and when residual errors are de-weighted (i.e., absolute). That is, controlling for the span, both the @hursh2008economic and @koff2015expt can achieve the same $A_{Lower}$ but neither can model zero.

## Planned Comparisons

The purpose of this review and report was to clarify and to evaluate the shared dimensions along which both the @hursh2008economic and @koff2015expt models vary. Specifically, the shared mathematical basis between the two should allow for modifications wherein both provide equivalent statistically estimates--even in the presence of 0 consumption values. The primary research question for this simulation study was to determine whether parameter estimates derived from the @hursh2008economic and @koff2015expt models would be statistically equivalent in a modified @hursh2008economic model that featured absolute error and were fitted when 0 consumption values were replaced with respective $A_{Lower}$ values.

# Methods

```{r }

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 500

sim <- SimulateDemand(nruns = nParticipants, setparams = setparams, sdindex = sdindex, x = x)

dataSet = sim$simvalues

passingFrame = beezdemand::CheckUnsystematic(dataSet)

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 0.5

```

## Data Generating Process

A total of `r nParticipants` hypothetical data series were simulated using using the R Statistical Program [@R-base]. The specific syntax used to generate was included in an R package that was submitted to peer-review [@kaplan2019r]. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to simulate hypothetical purchase task data that included a large composition of 0 consumption values. The seed values and variance used to generate these data were identical to those that were used in @koff2015expt. This specific data generating process was used as the basis for comparisons with the @hursh2008economic model given that the authors of the @koff2015expt study modeled their approach around the "messy" data frequently observed in "real-world" purchase tasks that are often conducted on crowdsourced platforms.

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all hypothetical series. Specifically, individual series were screened for *bounce*, *trend*, and *reversals from zero*. The first criterion, bounce, refers to local changes within an expected downward trend as a function of increasing price. That is, it would be unexpected to see consumption increases immediately following a price increase. The second criterion, trend, refers to the molar change in consumption from the lowest to the highest price. That is, there is a certain amount of decrease in consumption expected across the full domain of prices. Lastly, reversals from zero refer to the return of consumption at a higher price following the cessation of consumption at a lower price. Simulated data were carried forward into the final analysis so long as each series met all indicators of systematic hypothetical purchase task data.

## Modeling Strategies

A total of 3 modeling approaches were evaluated. Each of the 3 approaches was referenced as a specific strategy for conducting demand curve analysis when 0 consumption values were observed in the data. The core aim of each was to conduct pairwise comparisons between estimated parameters and determine whether the various strategies provided statistically equivalent estimates. Each of the strategies are listed below in greater detail.

### Strategy 1: @koff2015expt Model

The @koff2015expt model (absolute error) was fitted to simulated consumption data at the individual-level. The model was fit using the *optim* package included in the R Statistical Program [@R-base] due to its considerable flexibility in performing ordinary least squares regression. Initial starts were derived based on the respective data for parameter $Q_0$ and $\alpha$ was estimated on the log scale to support more comparable step sizes in the optimization. The span constant $k$ was derived from the empirical range of the full data set with an added constant (0.5) to allow the span of the demand curve to extend below the lowest non-zero point of consumption, as is common practice [@kaplan2018understanding]. The same span constant was used across all models to enable consistent comparisons between $Q_0$ and $\alpha$.

### TODO: Strategy 2 Micky Relative: 

### Strategy 2: @hursh2008economic Model (Relative Error)

The @hursh2008economic model (relative error) was fitted to simulated consumption data at the individual-level. During the fitting, 0 consumption values were replaced by an $A_{Lower}$ value that was generated dynamically based on parameters $Q_0$ and $\textit{k}$. That is, a customized loss function was prepared for use with the *optim* method. Consistent with efforts to maintain open and transparent science [@gilroy2019furthering], the source code necessary to reproduce this approach and this report has been posted for public review in a GitHub repository managed by the corresponding author, see Author Note. All other parameters were derived consistent with Strategy 1.

### Strategy 3: @hursh2008economic Model (Absolute Error)

The @hursh2008economic model (absolute error) was fitted to simulated consumption data at the individual-level as well. This strategy was identical to Strategy 2 with the exception of how loss (i.e., residual error) was interpreted during optimization. That is, a customized loss function was prepared and residual error was represented in terms of absolute differences, i.e. $(10^{\hat{y}} - 10^{y})^2$. All other parameters were derived consistent with that of Strategies 1 and 2.

## Analytical Strategy

Pairwise comparisons were conducted for parameters $Q_0$ and $\alpha$ resulting from each of the three strategies. Tests of equivalence were performed in a pairwise fashion (i.e., Strategy 1 vs. 2, Strategy 1 vs. 3) to evaluate estimates resulting from the @koff2015expt model and two modified forms of the @hursh2008economic model. The *tost* method in the *equivalence* R package [@robinson2016package] was used to perform two one-sided t-tests (TOSTs) with paired estimates resulting from each strategy. That is, the focus was not on determining *difference* between strategies but instead on determining *equivalence* between them. Across all tests, corrections were applied due to presence of repeated comparisons, i.e. $p=0.05/4=0.0125$.

# Results

```{r fig5scatterplots, fig.cap="Scatterplot Comparisons: Koffarnus et al. (Absolute)", fig.height=7}

dataFramePrep = data.frame(
  id       = 1:nParticipants,
  q0.HS    = numeric(length = nParticipants),
  q0.HSw   = numeric(length = nParticipants),
  q0.Koff  = numeric(length = nParticipants),
  q0.Koffw = numeric(length = nParticipants),
  a.HS     = numeric(length = nParticipants),
  a.HSw    = numeric(length = nParticipants),
  a.Koff   = numeric(length = nParticipants),
  a.Koffw  = numeric(length = nParticipants),
  K        = numeric(length = nParticipants)
)

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPT.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  
  subs = newData$y
  subs = ifelse(subs == 0, newAsymptote, subs)
  
  newData$ys   = getEXPT(par[1], kSet, 10^par[2], newData$x)
  newData$err  = ((newData$ys - subs)/newData$ys)^2
  #newData$err  = (log10(newData$ys) - log10(subs))^2
  #newData$err  = (newData$ys - newData$y)^2
  #newData$err  = newData$err * weights
  #newData$err  = newData$err * (1/newData$ys)

  sum(newData$err)
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

minQ0 = 0.01
maxQ0 = 200

#write.csv(dataSet, file = "dataSet.csv")

for (id in 1:nParticipants) {

  currentData = dataSet[dataSet$id == id, ]

  dataFramePrep[id, "K"] = kSet

  fit.Koff <- optim(par  = c(max(currentData$y), -3),
                    fn   = min.RSS.EXPT,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data = currentData)

  dataFramePrep[id, c("q0.Koff", "a.Koff")] = fit.Koff$par
  
  fit.HS   <- optim(par    = c(fit.Koff$par[1], fit.Koff$par[2]),
                    fn     = min.RSS.EXPL,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data   = currentData)

  dataFramePrep[id, c("q0.HS", "a.HS")] = fit.HS$par

  # ==================================================
  # Weighted sets
  # ==================================================
  
  fit.Koffa <- optim(par   = c(max(currentData$y), -3),
                     fn     = min.RSS.EXPT.aw,
                     method = "L-BFGS-B",
                     lower  = c(0.01, -6),
                     upper  = c(max(currentData$y) * 1.5, 0),
                     data   = currentData)

  dataFramePrep[id, c("q0.Koffw", "a.Koffw")] = fit.Koffa$par

  fit.HWa <- optim(par    = c(fit.HS$par[1], fit.HS$par[2]),
                   fn     = min.RSS.EXPL.aw,
                   method = "L-BFGS-B",
                   lower  = c(0.01, -5),
                   upper  = c(max(currentData$y) * 1.25, 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw", "a.HSw")] = fit.HWa$par
  
  
  
  

  # fit.Koffa = nls(y ~ Q0 * 10^(kSet*(exp(-a * Q0 * x)-1)),
  #          data = currentData,
  #          start = c(a  = 0.0001,
  #                    Q0 = 100),
  #          lower = c(a = -Inf,
  #                    Q0 = 1),
  #          upper = c(a = Inf,
  #                    Q0 = 125),
  #          algorithm = "port",
  #          weights = predict(fit.Koff),
  #          control = list(
  #            maxiter = 1000,
  #            warnOnly = TRUE
  #          ))
  # 
  # dataFramePrep[id, c("a.Koffw", "q0.Koffw")] = c(coef(fit.Koffa)["a"],
  #                                                 coef(fit.Koffa)["Q0"])
  


}

keepers = passingFrame %>%
  filter(TotalPass == 3) %>%
  select(id) %>%
  pull()

frameToAnalyze = dataFramePrep %>%
  filter(id %in% keepers)

par(mfrow = c(2, 2),
    oma = c(0, 0, 0, 0))

comps.base <- lm(q0.Koff ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HS,
     main = expression(A[LOWER]~ "with Relative Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.base)

comps.weight <- lm(q0.Koff ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main = expression(A[LOWER]~"with Absolute Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.weight)

comps.base <- lm(a.Koff ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HS,
     main = expression(A[LOWER]~"with Relative Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.base)

comps.weight <- lm(a.Koff ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main = expression(A[LOWER]~ "with Absolute Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.weight)

```

```{r fig5scatterplots2, fig.cap="Scatterplot Comparisons: Koffarnus et al. (Relative)", fig.height=7, eval=TRUE}

par(mfrow = c(2, 2),
    oma = c(0, 0, 0, 0))

comps.base <- lm(q0.Koffw ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koffw ~ frameToAnalyze$q0.HS,
     main = expression(A[LOWER]~ "with Relative Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.base)

comps.weight <- lm(q0.Koffw ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koffw ~ frameToAnalyze$q0.HSw,
     main = expression(A[LOWER]~"with Absolute Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.weight)

comps.base <- lm(a.Koffw ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koffw ~ frameToAnalyze$a.HS,
     main = expression(A[LOWER]~"with Relative Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.base)

comps.weight <- lm(a.Koffw ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koffw ~ frameToAnalyze$a.HSw,
     main = expression(A[LOWER]~ "with Absolute Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.weight)

```

The data generating process yielded a total of `r nrow(frameToAnalyze)` series that met all 3 indicators of systematic purchase task data (from *N*=`r nParticipants`; `r round((nrow(frameToAnalyze)/nParticipants) * 100, 2)`%). The correspondence between parameter estimates was visualized in scatter plots and these relationships are displayed in \autoref{fig:fig5scatterplots}. Overall, there was varying degrees of correspondence between each of the different strategies and the results of pairwise comparisons are presented below.

## Strategy 1 vs. Strategy 2

```{r strat1vsstrat2, include=TRUE}

library(equivalence)

tost.1v2.q0 = tost(frameToAnalyze$q0.Koff, 
                   frameToAnalyze$q0.HS, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.1v2.q0 = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HS)

tost.1v2.a = tost(frameToAnalyze$a.Koff, 
                  frameToAnalyze$a.HS, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.1v2.a = cor.test(frameToAnalyze$a.Koff, frameToAnalyze$a.HS)
```

An evaluation of the relationship between Strategy 1 and 2 revealed strong correlations for $Q_0$ (r=`r cor.1v2.q0$estimate`, t=`r cor.1v2.q0$statistic`, df=`r cor.1v2.q0$parameter`, `r scales::pvalue(cor.1v2.q0$p.value, accuracy = .0125, add_p = TRUE)`) as well as for $\alpha$ (r=`r cor.1v2.a$estimate`, t=`r cor.1v2.a$statistic`, df=`r cor.1v2.a$parameter`, `r scales::pvalue(cor.1v2.a$p.value, accuracy = .0125, add_p = TRUE)`). The results of TOSTs for $Q_0$ and $\alpha$ were non-significant (`r scales::pvalue(tost.1v2.q0$tost.p.value, accuracy = .0125, add_p = TRUE)`) and significant (`r scales::pvalue(tost.1v2.a$tost.p.value, accuracy = .0125, add_p = TRUE)`), respectively. That is, the results of equivalence testing indicated that both two strategies provided statistically equivalent estimates for $\alpha$ but not for $Q_0$.

## Strategy 1 vs. Strategy 3

```{r strat1vsstrat3, include=TRUE}

tost.1v3.q0 = tost(frameToAnalyze$q0.Koff, 
                   frameToAnalyze$q0.HSw, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.1v3.q0 = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

tost.1v3.a = tost(frameToAnalyze$a.Koff, 
                  frameToAnalyze$a.HSw, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.1v3.a = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

```

Evaluations of the relationship between Strategy 2 and 3 revealed perfect correlations for $Q_0$ (r=`r cor.1v3.q0$estimate`, t=`r cor.1v3.q0$statistic`, df=`r cor.1v3.q0$parameter`, `r scales::pvalue(cor.1v3.q0$p.value, accuracy = .0125, add_p = TRUE)`) as well as for $\alpha$ (r=`r cor.1v3.a$estimate`, t=`r cor.1v3.a$statistic`, df=`r cor.1v3.a$parameter`, `r scales::pvalue(cor.1v3.a$p.value, accuracy = .0125, add_p = TRUE)`). Similarly, the results of TOSTs for $Q_0$ (`r scales::pvalue(tost.1v3.q0$tost.p.value, accuracy = .0125, add_p = TRUE)`) and $\alpha$ (`r scales::pvalue(tost.1v3.a$tost.p.value, accuracy = .0125, add_p = TRUE)`) were significant overall. That is, the results of equivalence testing indicated that both strategies yielded statistically equivalent model parameters for both $Q_0$ and $\alpha$.

## Strategy 1 vs. Strategy 4

```{r strat1vsstrat4, include=TRUE}

library(equivalence)

tost.1v3.q0 = tost(frameToAnalyze$q0.Koffw, 
                   frameToAnalyze$q0.HS, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

print(tost.1v3.q0)

cor.1v3.q0 = cor.test(frameToAnalyze$q0.Koffw, frameToAnalyze$q0.HS)

print(cor.1v3.q0)

tost.1v3.a = tost(frameToAnalyze$a.Koffw, 
                  frameToAnalyze$a.HS, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

print(tost.1v3.a)

cor.1v3.a = cor.test(frameToAnalyze$a.Koffw, frameToAnalyze$a.HS)

print(cor.1v3.a)
```

# Discussion

The Operant Demand Framework has grown into a popular and well-regarded approach for evaluating choices and behavior of societal significance [@hursh2013behavioral; @reed2013behavioral]. Indeed, various labs and teams have shifted their focus from specific clinical questions towards issues of public policy @hursh2013behavioral; @roma2017progress]. Furthermore, specific modeling strategies in the Operant Demand Framework are increasingly represented in a range of scientific tools and packages [@kaplan2019r; @gilroy2018demand]. Despite an increasing range of scientific tools and resources, few firm guidelines exist with which to assist analysts in navigating between options for demand curve analyses. The purpose of this technical report was to review mathematical underpinnings of the two prevailing models derived from the framework of @hursh2008economic and present an argument why distinctions between these interchangeable approaches offers little to the advancement of the Operant Demand Framework.

This report provides an in-depth discussion and review of how 0 consumption values have, thus far, been included in models derive from the @hursh2008economic framework. As noted throughout this report, both the @hursh2008economic and the @koff2015expt approach are unable to model demand at 0 and both are restricted to the non-zero lower asymptote, $A_{Lower}$, in the same manner. This is the case regardless of whether 0 consumption values can be included in the regression. As such, the approach put forward in @koff2015expt is not a complete solution for 0 consumption values because it retains the same limitations of the original approach in this regard. This is because the span of the demand curve in the @hursh2008economic framework remains in the log scale and the log scale does not support 0 values. As an alternative, others have argued that a true solution to this issue would require deviating from the traditional log scale @gilroy2021zbe].

The simulations performed in this study were designed to facilitate comparisons between the @hursh2008economic and @koff2015expt models when controlling for the common $A_{LOWER}$ and differences in how residual errors were weighted. The goal of these comparisons were to advance the argument that the Exponential [@hursh2008economic] and Exponentiated [@koff2015expt] models should not be so strongly distinguished. Indeed, it is quite trivial to arrive at statistically equivalent estimates in both approaches when the role of the span constant and $A_{LOWER}$ is understood as they relate to 0 consumption values. The results of computer simulations confirmed that the two approach provide statistically equivalent estimates when controlling for such differences. This is because, mathematically, the behavior of $A_{LOWER}$ is identical regardless of which approach is used. Given that neither approach can characterize demand at zero, $A_{LOWER}$ is the *best* approximation of 0 in these circumstances. As such, replacing 0 consumption values with respective $A_{LOWER}$ values results in estimates that are statistically equivalent across the Exponential and Exponentiated models. This argument is not presented with the intent of favoring any specific approach as the de facto standard or recommended default when applying methods from the Operant Demand Framework. Rather, the intent of this work is in revealing how these (often) opposing strategies are much more similar than they are different. Indeed, they are so similar that distinguishing the two only serves to observe the shared mathematical bases for each. That said, each approach has utility and future efforts should be directed towards improving the understanding the properties of the @hursh2008economic framework rather than reinforcing any stance, position, or bias towards a specific implementation.

The final contribution of this work was in clarifying the common mathematical bases for the Exponential and Exponentiated models and highlight the ways in which the proponents of each approach have extended the Operant Demand Framework. That is, the proponents of each approach were successful in advancing both the utility and scope of the Operant Demand Framework. That is, the finding that the @hursh2008economic model can replicate the behavior of the @koff2015expt without exponentiation terms does not negate the contributions of the @koff2015expt approach. Indeed, the initiative of the @koff2015expt team warrants considerable praise even today because this led an initiative towards addressing the problematic issue of removing otherwise valid data. For decades, substantial portions of otherwise consumption data were never carried forward into analyses and it is unclear how these prior analyses would compare had these data been included. Regardless of whenever analysts have a preference for one approach or the other, it is clear that the methods featured in Operant Demand Framework are better equipped now that 0 consumption values are considered in the analysis.

## Future Directions in Operant Demand

The future is promising for the Operant Demand Framework. This perspective and this framework currently reflects a range of consumption (and non-consumption) and efforts are underway to leverage multilevel modeling as a future extension [@kaplanMlm]. Indeed, various labs are working toward increasing the applicability and generality of this approach. Towards this end, the intent and mission of the original @koff2015expt study regarding 0 consumption values is as valid and valuable today as it was when this work was first published. However, debates regarding model superiority (or inferiority) in the absence of formal tests does not enhance the Operant Demand Framework and its methods in any appreciable manner. That said, the two approaches are functionally interchangeable the reader is cautioned against thinking that any single model is inherently "true", "better", or otherwise superior in the absence of careful statistical evaluation. That said, it is unclear whether the prevailing approach in the Operant Demand Framework will remain based on the framework presented in @hursh2008economic. Indeed, it is possible that future research could lead to deviating from the use of log scale [@gilroy2021zbe] or towards a different framework altogether [@newman2020improved]. Regardless of the where the future takes the Operant Demand Framework, future approaches and advances should be met with cautious optimism and consideration rather than outright disregard in favor what has come before.

\newpage

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

\endgroup

\newpage

# Appendix

Several proofs are provided here to illustrate how the upper and lower asymptotes are determined. Despite the shared mathematical basis, derivations of each are provided below.

## Modified Hursh & Silberburg (2008) Optimization (Relative Error)

$$
e_i = \begin{cases} 
\hat{y}_i - log_{10}y_i       &    \mbox{if } y_i \not= 0 \\
\hat{y}_i - log_{10}A_{LOWER} &    \mbox{if } y_i = 0 \\
\end{cases}
$$

## Modified Hursh & Silberburg (2008) Optimization (Absolute Error)

$$
e_i = \begin{cases} 
10^{\hat{y}_i} - 10^{log_{10}y_i}       &    \mbox{if } y_i \not= 0 \\
10^{\hat{y}_i} - 10^{log_{10}A_{LOWER}} &    \mbox{if } y_i = 0 \\
\end{cases}
$$

## Hursh & Silberburg (2008) Proofs

### $A_{UPPER}$ at $P = 0$

$$
\begin{aligned}
log_{10}A_{Upper} &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * 0} - 1) \\
 &= log_{10}Q_0 + k(e^{0} - 1) \\
 &= log_{10}Q_0 + k(1 - 1) \\
 &= log_{10}Q_0 + k(0) \\
 &= log_{10}Q_0\\
 A_{Upper} &= Q_0
\end{aligned}
$$

Note: Euler's constant raised to the power of 0 is equal to a value of 1. This essentially zeroes out the $\textit{k}$ constant, leaving just the $Q_0$ parameter at 0 $P$.

### $A_{LOWER}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
log_{10}A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * \infty} - 1) \\
 &= log_{10}Q_0 + k(e^{-\infty} - 1) \\
 &= log_{10}Q_0 + k(0 - 1) \\
 &= log_{10}Q_0 + k(-1) \\
 &= log_{10}Q_0 - k \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$

Note: Euler's constant raised to the power of $-\infty$ equates to a value of 0. That is, $e^{-\infty}=\frac{1}{e^\infty}=\frac{1}{\infty} \approx 0$. This is has effect of making the value in parentheses equal to $-1$, which in turn results in the full subtraction of quantity $k$ from $log_{10}Q_0$.

## Koffarnus et al. (2015) Proofs

### $A_{UPPER}$ at $P = 0$

$$
\begin{aligned}
A_{Upper} &= Q_0 * 10^{k(e^{-\alpha * Q_0 * 0} - 1)}  \\
 &= Q_0 * 10^{k(e^{0} - 1)}  \\
 &= Q_0 * 10^{k(1 - 1)}  \\
 &= Q_0 * 10^{k(0)}  \\
 &= Q_0 * 10^{0}  \\
 &= Q_0 * 1  \\
 &= Q_0  \\
log_{10}A_{Upper} &= log_{10}Q_0  \\
\end{aligned}
$$

### $A_{LOWER}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= Q_0 * 10^{k(e^{-\alpha * Q_0 * \infty} - 1)}  \\
 &= Q_0 * 10^{k(e^{-\infty} - 1)}  \\
 &= Q_0 * 10^{k(0 - 1)}  \\
 &= Q_0 * 10^{k(-1)}  \\
 &= Q_0 * 10^{-k}  \\
log_{10}A_{Lower} &= log_{10}Q_0 + (-k)  \\
 &= log_{10}Q_0 - k  \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$
