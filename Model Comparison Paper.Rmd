---
title             : "False Dilemmas in the Operant Demand Framework: Zero Values and Demand Curve Analysis"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn P. Gilroy"
    affiliation   : "1"
    corresponding : yes
    address       : "226 Audubon Hall Baton Rouge, Louisiana 70806"
    email         : "sgilroy1@lsu.edu"
    role:
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
  Shawn Patrick Gilroy is an Assistant Professor of School Psychology at Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu. The source code necessary to recreate this work is publicly hosted in a repository at: https://github.com/miyamot0/AgnosticDemandModeling 

abstract: |
  Operant translations of Behavioral Economic concepts and principles have enhanced the ability of researchers to characterize individual choice under constraints and uncertainty. Operant Behavioral Economic models of concurrent choices (i.e., demand) have been particularly useful in evaluating how individual choices are affected by various ecological factors (e.g., price, limited resources). Prevailing perspectives in Operant Demand are derived from the framework of Hursh & Silberburg (2008). Few dispute the utility of this framework, though considerable debate continues regarding how best to address limitations associated with the logarithmic scale. At present, there are opposing views regarding the handling of zero values and under which situations alternative restatements of this framework are recommended, cf. Koffarnus et al. (2015). The purpose of this report is to review the importance of asymptotes in the Operant Demand Framework and the ways in which the Hursh & Silberburg (2008) and Koffarnus et al. (2015) models can accomodate 0 consumption values. A simulation study based on the original Koffarnus et al. (2015) study is provided as a demonstration of how results from the two models are statistically equivalent when addressing lower asymptotes and controlling for differences in error weighting. Additional discussion is provided on whether having competing models in the Hursh & Silberburg (2008) framework enhances the generality and precision of studies employing Operant Behavioral Economics.

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : yes

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
---

\raggedbottom

```{r include = FALSE}

library(beezdemand)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(parameters)
library(tidyverse)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# TODO: Strips need blanking

# TODO: Grayscale colors

# Introduction

Contemporary methods for evaluating the consumption of goods and services using the Operant Demand Framework draw heavily from the methodology proposed in @hursh2008economic. This framework and this methodology have passed through several iterations [@hursh1987linear], with the most recent form taking a non-linear shape and modeled around an exponential decay process [@hursh2008economic]. Beyond the @hursh2008economic model, specifically, more recent extensions of the this framework have explored variants that evaluate consumption on the linear [@koff2015expt] or other log-like scales [@gilroy2021zbe].

The original implementation of the @hursh2008economic framework was modeled around an "s-type" form as the prototypical shape of the demand curve. Indeed, the original intent of @hursh2008economic was to have an upper asymptote reached at a low (or free) price and a lower asymptote reached as prices approached infinity, $\lim_{P \to \infty}$ [@gilroy2021zbe]. Until recently, all models derived this framework [@hursh2008economic, @koff2015expt] evaluated the demand for reinforcers with non-zero upper and lower asymptotes. That is, both models were bounded at an upper limit (i.e., $Q_0$) and progressed towards a lower asymptote in the "s-type" form. The non-zero asymptotes in the @hursh2008economic framework make good sense because the traditional values of interest were positive real values (i.e., not 0). Such quantities were necessary because the logarithmic representation of consumption is naturally undefined at 0.

In response to the statistical and philosophical issues related to the omission of 0 consumption values, @koff2015expt introduced a restatement of the @hursh2008economic framework. This variant of the @hursh2008economic model was adjusted to examine changes in observed consumption on the linear scale. Specifically, an exponentiation of terms was performed such that the LHS (left-hand side) of the @hursh2008economic model were reflected in the linear scale. In this way, the LHS of the model (i.e., consumption) need not be submitted to the log transformation that previous prevented the use of the @hursh2008economic model. This modification drew considerable attention, as one of the largest issues associated with the log scale can be avoided. However, it warrants noting that portions of the RHS (right-hand side) of the @koff2015expt model remain on the log scale. Of particular importance, the span of the demand curve and the rate of exponential decay remain reflected in the log scale. It is for this reason that the span of the demand curve in this restated model cannot reach a true 0 point, and thus, is also bound by non-zero upper and lower asymptotes. Furthermore, the regressive process for logarithmic and linear models differs with respect to how error is weighted and this also introduces behavior that differs from that of the original @hursh2008economic framework [@gilroy2021zbe].

## Same Model, Different Error

The challenges associated with fitting models of operant demand (i.e., minimizing residual error) are increasingly discussed in research employing the the Operant Demand Framework [@gilroy2021zbe]. @gilroy2021zbe noted, among other things, that residual error is reflected differently in log and linear scales and that such differences affect model optimization. Briefly, changes in log space reflect relative differences while changes in the linear space reflect absolute differences. In most economic applications, relative error is typically preferred because the quantities of interest and their associated projections (i.e., $\hat{y}$) often span across multiple orders and quantities observed at higher orders would naturally be weighted more heavily than those at lower orders in the linear scale (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). For this reason, relative difference is often the default because this approach balances how each piece of information is weighted in the regression.

As an alternative to relative differences, which are weighted against observed values, absolute differences are more straightforward. That is, absolute error is simply the difference from some observed quantity and $\hat{y}$ regardless of the order (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). @gilroy2021zbe discussed how the departure from relative error can affect model optimization when observed levels of consumption vary considerably across orders. This has led to occasional inconsistencies wherein model fits across the log and linear variants of @hursh2008economic framework lead to different conclusions (e.g., shared alpha across varying dose-response curves).

## Different Error, Same Asymptotes

Apart from differences in how error is minimized across models in the Operant Demand Framework, there has been renewed attention regarding the upper and lower asymptotes of these models [@gilroy2021zbe]. Specifically, @gilroy2021zbe highlighted how neither the @hursh2008economic nor @koff2015expt models can characterize demand at 0. The topic of the zero-asymptote is not discussed at length here, but interested readers are encouraged to review @gilroy2021zbe for an exposition on why the inability to model demand at zero limits the utility of the Operant Demand Framework.

Revisiting the topic of asymptotes, two novel terms are introduced in this work: $A_{Upper}$ and $A_{Lower}$. The term $A_{Upper}$ refers to the upper non-zero bound of the demand curve. In models derived from the @hursh2008economic framework, this is simply the fitted parameter $Q_{0}$. That is, this is the absolute upper limit to the demand curve as prices approach 0, $\displaystyle{\lim_{P \to 0} f(x)= A_{Upper}}$. In contrast, the term $A_{Lower}$ refers the absolute lower bound of the demand curve. That is, this is the absolute lower limit to the demand curve as prices approach $\infty$, $\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}$. These two upper and lower extremes are separates by the span constant $\textit{k}$, which specify the distance in log units between these asymptotes. The derivation of both $A_{Upper}$ and $A_{Lower}$ are noted below and reviewed in greater detail in the Appendix.

\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}

Further inspection of the $A_{Lower}$ and its derivation naturally evoke questions regarding how 0 consumption values could ever be evaluated in a model derived from the @hursh2008economic framework. As noted in the bottom portion of *Equation 1*, both the @hursh2008economic and @koff2015expt models can never take a value of 0 because both asymptotes exist in a space where such a value cannot exist. As such, all predictions from these model must take values that emerge from within these bounds, $\hat{y} \in [ A_{Lower}, A_{Upper} ]$.

## Same Asymptotes, Same Spans

Revisiting *Equation 1*, this notation highlights the heavy influence of the span parameter $\textit{k}$ and how it affects the values that can be predicted (i.e., $\hat{y}$). In the original implementation of the @hursh2008economic framework, parameter $\textit{k}$ was specified based on the range of *observed consumption*. Since 0 consumption values were omitted from this implementation altogether, parameter $\textit{k}$ was directly linked to the observed data. Indeed, dropping the 0 values made the specification of this constant straightforward and unremarkable. That is, both parameter $\textit{k}$, $A_{Upper}$, and $A_{Lower}$ could be directly linked to the highest and lowest observed non-zero levels of consumption, respectively. A visualization of the parameter $\textit{k}$ is provided in \autoref{fig:fig1CurveSpanEmpirical} with respect to non-zero consumption values.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes") +
  geom_point() +
  annotate("segment", 
           x      = 0.1, 
           xend   = 0.1, 
           y      = 100, 
           yend   = lowerAsymptote,
           colour = "red") +
  annotate("text", 
           x     = 0.175, 
           y     = 55, 
           angle = 90,
           adj   = 0.5,
           col   = "red",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = lowerAsymptote, 
           yend   = lowerAsymptote,
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "red",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = max(preFit$Q), 
           yend   = max(preFit$Q),
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "red",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

```

Whereas the determination of parameter $\textit{k}$ in the @hursh2008economic implementation is essentially linked to observed (non-zero) consumption data, the determination of parameter $\textit{k}$ became more abstract in the implementation introduced by @koff2015expt. That is, parameter $\textit{k}$ could not longer be linked to observed (non-zero) consumption data because $A_{Lower}$ would have to extend *beyond* the range of non-zero consumption. Anecdotally, various labs and teams using this model have opted to either add a small constant to parameter $\textit{k}$ or fit this as a free parameter--both with the intent of *increasing* the span beyond the observed (non-zero) data, and thus, driving $A_{Lower}$ to a point closer to 0 on the linear scale. A visualization of this span-extending behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE, fig.height=3.5}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  annotate("text",
           x     = 100,
           y     = 90,
           adj   = 0,
           col   = "red",
           parse = TRUE,
           label = expression("K (Empirical): " ~ A[LOWER] == 5.442)) +
  annotate("text",
           x     = 100,
           y     = 75,
           adj   = 0,
           col   = "orange",
           parse = TRUE,
           label = expression("K + 0.5: " ~ A[LOWER] == 1.7209)) +
  annotate("text",
           x     = 100,
           y     = 60,
           adj   = 0,
           col   = "green",
           parse = TRUE,
           label = expression("K + 1: " ~ A[LOWER] == 0.5442)) +
  annotate("text",
           x     = 100,
           y     = 45,
           adj   = 0,
           col   = "blue",
           parse = TRUE,
           label = expression("K + 2: " ~ A[LOWER] == 0.0544)) +
  annotate("text",
           x     = 100,
           y     = 30,
           adj   = 0,
           col   = "purple",
           parse = TRUE,
           label = expression("K + 3: " ~ A[LOWER] == 0.0054)) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom"
  )

```

\autoref{fig:fig2CurveSpanEmpiricalMod} illustrates how inflating the span constant affects $A_{Lower}$ and this has two effects on the demand curve. First, the rate of change in elasticity is jointly reflected by parameter $\alpha$ and the span of the demand curve [@gilroy2020interpretation]. Given that $\alpha$ is unitless, it covaries inversely with the size of the span constant--with relatively greater values reflecting rapid changes in $\hat{y}$ across prices and relatively lesser values reflecting gradual changes in $\hat{y}$ across prices [@gilroy2019exactsolve]. Second, and specific to the @koff2015expt model, larger $\textit{k}$ values serve to lessen the absolute difference between $A_{Lower}$ and true 0--a point that cannot actually be characterized in the model. \autoref{fig:fig2CurveSpanEmpiricalMod} elegantly displays how this gap decreases proportionally with each unit increase in the span parameter $\textit{k}$.

## Unnecessary Distinctions

The areas above serve to highlight the minimal ways in which the @hursh2008economic and @koff2015expt models differ. Indeed, these approaches differ in terms of optimization but ultimately share the same limitations with respect to non-zero asymptotes. Regarding the first point, error representation and optimization, the two models can provide functionally identical results when error representation is made comparable. That is, re-weighting the errors (i.e., relative to $\hat{y}$) in the @koff2015expt model yields fits and estimates that are nearly identical to that of the @hursh2008economic model in the absence of 0 consumption value, see \autoref{fig:fig3ComparisonNonzeroRelativeError}. That is, in the presence of purely non-zero data, there is little need to extend beyond the original implementation of the @hursh2008economic framework.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error", fig.height=3, warning=FALSE}

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           weights = 1/(predict(EXPT)^2),
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  rename(`Exponential`   = explPred,
         `Exponentiated (Absolute Error)` = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred) 

# kable(preFit.trim) %>%
#   kable_styling(full_width = TRUE)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P) 

ggplot(preFit.trim, aes(P, Q, color = Model)) +
  #ggtitle("TODO: Add title") +
  geom_point(color = "black") +
  geom_line(aes(P, Prediction)) +
  facet_wrap(~Model, ncol = 3) +
  scale_y_log10(breaks = c(1, 10, 100),
                labels = c(1, 10, 100),
                limits = c(1, 100)) +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom"
  )

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

```

Regarding the second point, few researchers have explored $A_{Lower}$ and how this feature of the demand curve differs across series with and without 0 consumption values. This is a complex topic, especially in the linear restatement, because 0 is an observed quantity that cannot be characterized by either model. Evaluations of the @koff2015expt model and its the behavior have revealed that the optimization of the @koff2015expt model minimizes differences between 0 consumption values and $\hat{y}$ by using a $\textit{k}$ parameter that extends well beyond the non-zero range of consumption values in an effort to minimize the impassible absolute distance between $A_{Lower}$ and 0. Pragmatically, one could and would argue that such a small amount of error calls for little concern and that values at $A_{Lower}$ are *close enough* to approximate 0 consumption in demand curve analysis.

Revisiting this point, consider the following logic. For the sake of argument, let us say that the @koff2015expt model considers values at $A_{Lower}$ to be a *close enough* approximation of 0 to reasonably model consumption at such levels. That is, $A_{Lower}$ is considered sufficiently approximate to 0 in the @koff2015expt model. Following this logic, it stands to reason that treating 0 consumption values as $A_{Lower}$ is equally appropriate because 1) this allows demand to be projected beyond observed non-zero levels (i.e., accommodates 0 values) and 2) it is close enough to 0 that the absolute differences are considered negligible. In this arrangement, it stands to reason that the @koff2015expt model should perform in the same ways as the @hursh2008economic model, were errors also treated in terms of absolute difference and 0 values as $A_{Lower}$. Using the full data displayed in \autoref{fig:fig1CurveSpanEmpirical}, let us apply the @koff2015expt model using the full data set and the @hursh2008economic model with the 0 values replaced by the lowest $A_{Lower}$ illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}, i.e. $10^{log{10}Q_0 - (k + 3)}$. The predictions from these fits are illustrated in \autoref{fig:fig4ComparisonZeroRelative}. Across these series, the @hursh2008economic model (with and without relative error weighting) provide nearly identical fits to the @koff2015expt model. Additionally, de-weighting the @hursh2008economic to minimize the absolute difference between residuals produces an even closer approximation between the two, see \autoref{tab:table1ComparisonZeroAbsolute}.

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes", fig.height=3}

ggplot(preFit.g, aes(P, Q, color = Model)) +
  geom_point() +
  geom_line(aes(P, Prediction)) +
  scale_x_log10(breaks = c(0.1, 10, 100, 1000, 10000),
                labels = c(0.1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  scale_y_log10(breaks = c(0.001, 0.01, 0.1, 1, 10, 100),
                labels = c(0.001, 0.01, 0.1, 1, 10, 100),
                limits = c(0.001, 100)) +
  facet_wrap(~Model, ncol = 3) +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom"
  )

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

This brief example serves to highlight a core limitation of the @koff2015expt model--it cannot characterize demand at zero and is functionally identical to the @hursh2008economic model with 0 values replaced with a common $A_{Lower}$ limit and de-weighting residuals. Said another way, the approach put forward in @koff2015expt implicitly uses an inflated $\textit{k}$ parameter to drive $A_{Lower}$ down to a region close enough to 0 that wherein the absolute difference between true 0 and $\hat{y}$ is trivial. That is, 0 values cannot emerge from this model, i.e. $0 \not\in [ A_{Lower}, A_{Upper} ]$.

## Planned Comparisons

The purpose of this technical report was to clarify how prevailing models in the @hursh2008economic framework can be adjusted to functional similarly in the presence of 0 consumption values by addressing $A_{LOWER}$ and error representation. The primary question of interest was whether parameter estimates derived from the @hursh2008economic and @koff2015expt models could be made statistically equivalent by addressing $A_{Lower}$ with the @hursh2008economic model.

# Methods

```{r }

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 500

sim <- SimulateDemand(nruns = nParticipants, setparams = setparams, sdindex = sdindex, x = x)

dataSet = sim$simvalues

passingFrame = beezdemand::CheckUnsystematic(dataSet)

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 0.5

```

## Data Generating Process

A total of `r nParticipants` hypothetical data series were simulated using peer-reviewed software previously submitted to peer-review [@kaplan2019r]. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to generate hypothetical series of hypothetical purchase task data. The seed values and variance used to generate these hypothetical purchase task data were identical to those that were used in @koff2015expt study. This specific data generating process was used as the basis for comparisons with the @hursh2008economic model given that the @koff2015expt simulation was designed to be analogous to the "messy" data often observed in "real-world" purchase tasks.

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all hypothetical series. Specifically, individual series were screened for bounce, trend, and reversals from zero. The first criterion, bounce, refers to local changes within the *expected* downward trend across increasing prices. That is, it would be unexpected to see consumption increases immediately following a price increase. The second criterion, trend, refers to the molar change in consumption from the lowest to the highest price. That is, there is a certain amount of decrease expected across the full run of consumption across prices. Lastly, reversals from zero refer to the return of consumption at a higher price following the cessation of consumption at a lower price. Simulated data were carried forward into the final analysis so long as each series met all indicators of systematic hypothetical purchase task data.

## Modeling Strategies

A total of three different demand curve modeling strategies were evaluated. Each of the three approaches referred to as a different strategy for conducting demand curve analysis when 0 values were observed in the data. The core aim was to evaluate estimated parameters across each strategy to determine whether the varying strategies provided statistically equivalent results. Each of the modeling strategies is listed below in greater detail.

### Strategy 1: @koff2015expt Model

The @koff2015expt model (absolute error) was applied to simulated consumption data at the individual-level. The model was fit using the *optim* package in the R Statistical Program [@R-base] due to its considerable flexibility in performing ordinary least squares regression. Initial starts were derived based on the respective data for parameter $Q_0$ and $\alpha$ was estimated on the log scale to support more equal step sizes in the optimization steps. The span constant $k$ was derived from the empirical range of the full data set with an added constant (0.5) to extend below the lowest non-zero point of consumption, as is common practice [@kaplan2018understanding]. The same span constant was used across all models to ensure comparable fits.

### Strategy 2: @hursh2008economic Model (Relative Error)

The @hursh2008economic model (relative error) was applied to simulated consumption data at the individual-level. During the fitting, 0 consumption values were replaced by a $A_{Lower}$ value generated dynamically based on parameter $Q_0$ and $\textit{k}$. That is, a customized loss function was prepared for use with the *optim* method. In efforts to maintain complete transparency, the source code necessary to reproduce this approach and these calculations has been posted for public review in a GitHub repository managed by the corresponding author, see Author Note. All other parameters were derived consistent with that of Strategy 1.

### Strategy 3: @hursh2008economic Model (Absolute Error)

The @hursh2008economic (absolute error) was applied to simulated consumption data at the individual-level. This strategy was identical to Strategy 2 with the exception of how loss was interpreted during optimization. That is, a customized loss function was prepared and residual error was represented in terms of absolute differences, i.e. $(10^{\hat{y}} - 10^{y})^2$. All other parameters were derived consistent with that of Strategies 1 and 2.

## Analytical Strategy

Individual comparisons were prepared for both parameters $Q_0$ and $\alpha$ resulting from each of the models. Given that both the @hursh2008economic and @koff2015expt models emerge from the same framework, tests of equivalence were performed in a pairwise fashion between Strategies 1 and 2 and 1 and 3. The *tost* method in the *equivalence* R package [@robinson2016package] was used to perform two one-sided t-tests (TOST) with paired estimates resulting from each strategy. That is, the focus was not on determining difference but on determining equivalence. Across all tests, corrections were applied due to presence of repeated comparisons, $p=0.05/4=0.0125$.

# Results

```{r fig4, fig.cap="Scatterplot Comparisons Resulting from Modeling Strategies", fig.height=7}

dataFramePrep = data.frame(
  id      = 1:nParticipants,
  q0.HS   = numeric(length = nParticipants),
  q0.HSw  = numeric(length = nParticipants),
  q0.Koff = numeric(length = nParticipants),
  a.HS    = numeric(length = nParticipants),
  a.HSw   = numeric(length = nParticipants),
  a.Koff  = numeric(length = nParticipants),
  K       = numeric(length = nParticipants)
)

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

minQ0 = 0.01
maxQ0 = 200

for (id in 1:nParticipants) {

  currentData = dataSet[dataSet$id == id, ]

  dataFramePrep[id, "K"] = kSet

  fit.Koff <- optim(par  = c(max(currentData$y), -3),
                    fn   = min.RSS.EXPT,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data = currentData)

  dataFramePrep[id, c("q0.Koff", "a.Koff")] = fit.Koff$par

  fit.HS   <- optim(par    = c(fit.Koff$par[1], fit.Koff$par[2]),
                    fn     = min.RSS.EXPL,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data   = currentData)

  dataFramePrep[id, c("q0.HS", "a.HS")] = fit.HS$par

  fit.HWa <- optim(par    = c(fit.HS$par[1], fit.HS$par[2]),
                   fn     = min.RSS.EXPL.aw,
                   method = "L-BFGS-B",
                   lower  = c(0.01, -5),
                   upper  = c(max(currentData$y) * 1.25, 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw", "a.HSw")] = fit.HWa$par
}

keepers = passingFrame %>%
  filter(TotalPass == 3) %>%
  select(id) %>%
  pull()

frameToAnalyze = dataFramePrep %>%
  filter(id %in% keepers)

par(mfrow = c(2, 2),
    oma = c(0, 0, 0, 0))

comps.base <- lm(q0.Koff ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HS,
     main = expression(A[LOWER]~ "with Relative Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.base)

comps.weight <- lm(q0.Koff ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main = expression(A[LOWER]~"with Absolute Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.weight)

comps.base <- lm(a.Koff ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HS,
     main = expression(A[LOWER]~"with Relative Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.base)

comps.weight <- lm(a.Koff ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main = expression(A[LOWER]~ "with Absolute Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.weight)

```

The data generating process produced a total of `r nrow(frameToAnalyze)` series that met all 3 indicators of systematic purchase task data (from *N*=`r nParticipants`; `r round((nrow(frameToAnalyze)/nParticipants) * 100, 2)`%). The correspondence between parameter estimates was visualized using scatterplots and these are displayed in \autoref{fig:fig4}. Overall, the relationships illustrated in \autoref{fig:fig4} indicated varying degrees of correspondence between each of the different strategies. These results are presented in greater detail below.

## Strategy 1 vs. Strategy 2

```{r strat1vsstrat2, include=TRUE}

library(equivalence)

tost.1v2.q0 = tost(frameToAnalyze$q0.Koff, 
                   frameToAnalyze$q0.HS, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.1v2.q0 = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HS)

tost.1v2.a = tost(frameToAnalyze$a.Koff, 
                  frameToAnalyze$a.HS, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.1v2.a = cor.test(frameToAnalyze$a.Koff, frameToAnalyze$a.HS)
```

An evaluation of the relationship between Strategy 1 and 2 revealed strong correlations for $Q_0$ (r=`r cor.1v2.q0$estimate`, t=`r cor.1v2.q0$statistic`, df=`r cor.1v2.q0$parameter`, `r scales::pvalue(cor.1v2.q0$p.value, accuracy = .0125, add_p = TRUE)`) as well as for $\alpha$ (r=`r cor.1v2.a$estimate`, t=`r cor.1v2.a$statistic`, df=`r cor.1v2.a$parameter`, `r scales::pvalue(cor.1v2.a$p.value, accuracy = .0125, add_p = TRUE)`). The results of TOSTs for $Q_0$ and $\alpha$ were non-significant (`r scales::pvalue(tost.1v2.q0$tost.p.value, accuracy = .0125, add_p = TRUE)`) and significant (`r scales::pvalue(tost.1v2.a$tost.p.value, accuracy = .0125, add_p = TRUE)`), respectively. That is, the results of equivalence testing indicated that both two strategies provided statistically equivalent estimates for $\alpha$ but not for $Q_0$.

## Strategy 1 vs. Strategy 3

```{r strat1vsstrat3, include=TRUE}

tost.1v3.q0 = tost(frameToAnalyze$q0.Koff, 
                   frameToAnalyze$q0.HSw, 
                   epsilon = 1, 
                   conf.level = 0.9875,
                   paired = TRUE)

cor.1v3.q0 = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

tost.1v3.a = tost(frameToAnalyze$a.Koff, 
                  frameToAnalyze$a.HSw, 
                  epsilon = 1, 
                  conf.level = 0.9875,
                  paired = TRUE)

cor.1v3.a = cor.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

```

Evaluations of the relationship between Strategy 2 and 3 revealed perfect correlations for $Q_0$ (r=`r cor.1v3.q0$estimate`, t=`r cor.1v3.q0$statistic`, df=`r cor.1v3.q0$parameter`, `r scales::pvalue(cor.1v3.q0$p.value, accuracy = .0125, add_p = TRUE)`) as well as for $\alpha$ (r=`r cor.1v3.a$estimate`, t=`r cor.1v3.a$statistic`, df=`r cor.1v3.a$parameter`, `r scales::pvalue(cor.1v3.a$p.value, accuracy = .0125, add_p = TRUE)`). Similarly, the results of TOSTs for $Q_0$ (`r scales::pvalue(tost.1v3.q0$tost.p.value, accuracy = .0125, add_p = TRUE)`) and $\alpha$ (`r scales::pvalue(tost.1v3.a$tost.p.value, accuracy = .0125, add_p = TRUE)`) were significant overall. That is, the results of equivalence testing indicated that both strategies yielded statistically equivalent model parameters across both $Q_0$ and $\alpha$.

# Discussion

The Operant Demand Framework is increasingly applied to various issues of clinical and societal significance [@hursh2013behavioral; @reed2013behavioral]. Furthermore, formal models of operant demand are increasingly supported across a range of statistical packages and tools [@kaplan2019r; @gilroy2018demand]. Within these applications and tools, several modeling options are currently available but few firm guidelines exist to guide researchers in navigating among the available options. The purpose of this technical report was to review the pragmatic and mathematical underpinnings of the two prevailing models derived from the framework of @hursh2008economic--the @hursh2008economic model (Exponential) and the @koff2015expt model (Exponentiated). The goals of this report were three-fold: First, one of the goals of this report was to review the behavior of the lower limit ($A_{LOWER}$) for both the Exponential and Exponentiated as its relates to the presence and absence of 0 consumption values. Second, and building from the first, the two models are largely interchangeable when the $A_{LOWER}$ limit is known and differences in error weighting are negated. Third, this report was written to clarify the shared mathematical basis for each model and affirm the ways in which each have extended the Operant Demand Framework.

Regarding the first point, this report provides an in-depth discussion and review of how 0 consumption values have, thus far, been included in models directly derived from the @hursh2008economic framework. In short, neither the Exponential nor the Exponentiated models characterize demand at 0. Both are equally restricted to the same non-zero lower asymptote, $A_{Lower}$, which can never be 0. Regardless of whether 0 consumption values are included in the regression, this mathematically cannot succeed as currently specified. As such, it warrants stating that the approach put forward in @koff2015expt is not a true solution to the absence of a true zero-asymptote in the @hursh2008economic framework. This is due to the underlying log scale and alternative scales have been suggested if a true-zero asymptote is necessary to answer specific research questions [@gilroy2021zbe].

The second aim of this study was to provide a mathematical basis and justification for why the two different models should not be so strongly distinguished. That is, controlling for differences in scale and $A_{LOWER}$, it is quite trivial to arrive at the same results for both model regardless of whether 0 consumption values are included. The results of computer simulation confirmed that the two models provide statistically equivalent estimates when controlling for the inherent differences between each implementation. That is, it is $A_{LOWER}$ being modeled and not 0 in each approach. However, this is not stated with the intent of favoring any specific model as the de facto standard for use in the Operant Demand Framework. Rather, the true boon here is in finding that these two historically opposing perspectives are more alike than they are different. As such, the value of each is strongly stated and future efforts should be directed towards better understanding the properties of each strategy rather than towards reinforcing any prior biases towards or against any particular model.

Lastly, the final goal of this report was to affirm the contributions that emerged from each variant of @hursh2008economic framework. Stated plainly, both proponents were successful in using their strategy to advance the utility and scope of the Operant Demand Framework. Although the @hursh2008economic model can mimic the behavior of the @koff2015expt with minor modifications, the initiative of the @koff2015expt study warrants considerable praise regardless. That is, @koff2015expt led the charge towards addressing the historically problematic issue of removing data not occurring at random. For decades, substantial portions of otherwise valid consumption data were never carried forward into formal analyses for study. Regardless of which model is preferable in any particular case, the Operant Demand Framework is improved now that multiple ways of addressing 0 consumption values are now available.

Future Directions in Operant Demand

In closing, future efforts in the Operant Demand Framework should be directed towards advancing the applicability and generality of an Operant Behavioral Economic perspective. Towards this end, the intent and mission of the original @koff2015expt study regarding 0 consumption values is as true today as it was then. However, further debate regarding model superiority (or inferiority) in the absence of formal tests does not enhance the Operant Behavioral Economic perspective in any appreciable manner. As such, the reader should be cautioned against thinking that any one model is "true" or "better" in the absence of statistical evaluation. Indeed, future contributions may drop the reliance on the log scale [@gilroy2021zbe] or choose not to extend the framework altogether [@newman2020improved]. Regardless, such approaches and advances should be met with scientific scrutiny and cautious optimism rather than outright disregard.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\newpage

# Appendix

Several proofs are provided here to illustrate how the upper and lower asymptotes are determined. Despite the shared mathematical basis, derivations of each are provided below.

## Modified Hursh & Silberburg (2008) Optimization (Relative Error)

$$
e(n) = \begin{cases} 
\hat{y} - log_{10}y         &    \mbox{if } y \not= 0 \\
\hat{y} - log_{10}A_{LOWER} &    \mbox{if } y = 0 \\
\end{cases}
$$

## Modified Hursh & Silberburg (2008) Optimization (Absolute Error)

$$
e(n) = \begin{cases} 
10^{\hat{y}} - 10^{log_{10}y}         &    \mbox{if } y \not= 0 \\
10^{\hat{y}} - 10^{log_{10}A_{LOWER}} &    \mbox{if } y = 0 \\
\end{cases}
$$

## Hursh & Silberburg (2008) Proofs

### $A_{UPPER}$ at $P = 0$

$$
\begin{aligned}
log_{10}A_{Upper} &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * 0} - 1) \\
 &= log_{10}Q_0 + k(e^{0} - 1) \\
 &= log_{10}Q_0 + k(1 - 1) \\
 &= log_{10}Q_0 + k(0) \\
 &= log_{10}Q_0\\
 A_{Upper} &= Q_0
\end{aligned}
$$

Note: Euler's constant raised to the power of 0 is equal to a value of 1. This essentially zeroes out the $\textit{k}$ constant, leaving just the $Q_0$ parameter at 0 $P$ and below. Naturally, this value may be raised to the linear scale as follows: $10^{log_{10}Q_0} = Q_0$.

### $A_{LOWER}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
log_{10}A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * \infty} - 1) \\
 &= log_{10}Q_0 + k(e^{-\infty} - 1) \\
 &= log_{10}Q_0 + k(0 - 1) \\
 &= log_{10}Q_0 + k(-1) \\
 &= log_{10}Q_0 - k \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$

Note: Euler's constant raised to the power of $-\infty$ equates to a value of 0. That is, $e^{-\infty}=\frac{1}{e^\infty}=\frac{1}{\infty} \approx 0$. This is has effect of making the value in parentheses equal to $-1$, which in turn results in the full subtraction of quantity $k$ from $log_{10}Q_0$. Similar to $A_{UPPER}$, this value may be raised to the linear scale as follows: $10^{log_{10}Q_0 - k}$.

## Koffarnus et al. (2015) Proofs

### $A_{UPPER}$ at $P = 0$

$$
\begin{aligned}
A_{Upper} &= Q_0 * 10^{k(e^{-\alpha * Q_0 * 0} - 1)}  \\
 &= Q_0 * 10^{k(e^{0} - 1)}  \\
 &= Q_0 * 10^{k(1 - 1)}  \\
 &= Q_0 * 10^{k(0)}  \\
 &= Q_0 * 10^{0}  \\
 &= Q_0 * 1  \\
 &= Q_0  \\
\end{aligned}
$$

Note: Euler's constant raised to the power of 0 is equal to a value of 1. This essentially zeroes out the $\textit{k}$ constant, leaving just the $Q_0$ parameter at 0 $P$ and below. Given that this value exists in the linear scale, the result is simply $Q_0$.

### $A_{LOWER}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= Q_0 * 10^{k(e^{-\alpha * Q_0 * \infty} - 1)}  \\
 &= Q_0 * 10^{k(e^{-\infty} - 1)}  \\
 &= Q_0 * 10^{k(0 - 1)}  \\
 &= Q_0 * 10^{k(-1)}  \\
 &= Q_0 * 10^{-k}  \\
log_{10}A_{Lower} &= log_{10}Q_0 + (-k)  \\
 &= log_{10}Q_0 - k  \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$

Note: Euler's constant raised to the power of $-\infty$ equates to a value of 0. That is, $e^{-\infty}=\frac{1}{e^\infty}=\frac{1}{\infty} \approx 0$. This is has effect of making the value in parentheses equal to $-1$, which in turn results in the full subtraction of quantity $k$ from $log_{10}Q_0$. Similar to $A_{UPPER}$, this value may be raised to the linear scale as follows: $10^{log_{10}Q_0 - k}$.
