---
title             : "Hidden Equivalence in the Operant Demand Framework: A Review and Evaluation of Multiple Methods for Evaluating Non-Consumption"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn P. Gilroy"
    affiliation   : "1"
    corresponding : yes
    address       : "220 Audubon Hall Baton Rouge, Louisiana 70803"
    email         : "sgilroy1@lsu.edu"
    role:
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
 Shawn Patrick Gilroy is an Assistant Professor of School Psychology at Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu. The source code necessary to recreate this work is publicly hosted in a repository at: https://github.com/miyamot0/AgnosticDemandModeling 

abstract: |
 Operant translations of behavioral economic concepts and principles have enhanced the ability of researchers to characterize the effects of reinforcers on behavior. Operant behavioral economic models of choice (i.e., Operant Demand) have been particularly useful in evaluating how the consumption of reinforcers is affected by various ecological factors (e.g., price, limited resources). Prevailing perspectives in the Operant Demand Framework are derived from the framework presented in Hursh and Silberberg (2008). Few dispute the utility of this framework and model, though debate continues regarding how to address the challenges associated with logarithmic scaling. At present, there are competing views regarding the handling of non-consumption (i.e., 0 consumption values) and under which situations that alternative restatements of this framework are recommended. The purpose of this report was to review the shared mathematical bases for the Hursh and Silberberg (2008) and Koffarnus et al. (2015) models and how each can accommodate non-consumption values. Simulations derived from those featured in Koffarnus et al. (2015) were used to conduct tests of equivalence between modeling strategies while controlling for interpretations of residual error as well as the absolute lower asymptote. Simulations and proofs were provided to illustrate how neither the Hursh and Silberberg (2008) nor Koffarnus et al. (2015) models can characterize demand at 0 and how both ultimately arrive at the same upper and lower asymptotes. These findings are discussed and recommendations are provided to build consensus related to zero consumption values in the Operant Demand Framework.  

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "5284"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : yes

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
#bibliography: r-references.bib
---

\raggedbottom

```{r include = FALSE}
library(beezdemand)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(parameters)
library(scales)
library(tidyverse)
library(tidyr)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(65535)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Contemporary methods for evaluating the consumption of goods and services using the Operant Demand Framework are heavily influenced by the methodology proposed by @hursh2008. This framework and methodology have evolved through several forms [e.g., @hursh1987linear] and the latest iteration takes a non-linear (i.e., "S"-type) shape and is driven by an exponential decay process [@hursh2008]. This framework for evaluating the effects of unit price on the consumption of reinforcers has achieved widespread adoption and has also inspired derivatives that model consumption using varying scales, e.g., linear [@koff2015expt], "log-like" [@gilroy2021]. Furthermore, this framework and manner of analysis has supported both basic and applied research, across a variety of real and hypothetical goods, within and across species [e.g., human and non-human animals; @hursh2016]. Although this framework has been effective in evaluating behavior across a range of applications, various modeling strategies are available and there is little consensus at present regarding which strategies are most appropriate for certain compositions of data.

The original implementation of the @hursh2008 framework was modeled from the notion that the prototypical shape of the demand curve was an "S"-type form bounded by upper and lower limits. The original intent of @hursh2008 was to have an upper asymptote defined at a price of zero (i.e., $\lim_{P \to - \infty}$) and a lower asymptote reached as prices approached infinity [i.e., $\lim_{P \to - \infty}$; @gilroy2021]. The upper asymptote is interpreted as the intensity of demand for a particular reinforcer (i.e., consumption [*Q*] at a price of 0 = *Q~0~*), and the rate by which demand progresses from the upper to lower asymptote refers to the overall sensitivity to price (i.e., rate of change in elasticity = $\alpha$).[^1] This approach to characterizing the effects of reinforcers has been effective for understanding patterns of choices related to abuse liability for drugs as reinforcers [@mackillop2018] as well as substance use and abuse [@acuff2020; @aston2016; @gonzálezroz2019]. Furthermore, this approach also provides a means of evaluating reinforcer efficacy in behavioral interventions [@gilroy2018; @gilroy2021a] as well as various other initiatives, e.g. environmental conservation [@kaplan2018], consumption of evidence-based therapies [@gilroy], and COVID-19 vaccination [@hursh2020].

[^1]: Beyond the fitted estimates resulting from the framework of Hursh and Silberberg (2008), indicators of price elasticity of demand (e.g., *P~MAX~*, *O~MAX~*) are of primary interest in the Operant Demand Framework.

Models derived from this framework, such as @hursh2008 and @koff2015expt, characterize the demand for reinforcers with non-zero upper and lower asymptotes. Both models are bounded at an upper limit (i.e., *Q~0~*) and progress towards a non-zero lower limit in an "S"-type form. Non-zero upper and lower asymptotes in these models make good sense because the original values of interest in the framework of @hursh2008 were positive real values (i.e., not including 0). The exclusion of such quantities is expected because the logarithmic representation of consumption is undefined at 0.

In response to the statistical and philosophical issues related to the omission of 0 consumption values, @koff2015expt introduced a restatement of the @hursh2008 model that accommodated these values during non-linear regression. This procedure was made possible by exponentiatingterms such that the LHS (left-hand side) of the original @hursh2008 model reflected changes in consumption using the linear scale. In this restatement, the LHS of the model (i.e., observed consumption) need not be submitted to the log transformation that prevented the use of the original @hursh2008 model with non-consumption values.

The approach presented in @koff2015expt drew considerable attention, as one of the largest issues associated with the log scale could be avoided during nonlinear regression. However, it warrants noting that most of the RHS (right-hand side) of the @koff2015expt restatement remained on the log scale. Specifically, the span of the demand curve as well as the rate of exponential decay remained in the log scale [@gilroy2021]. It is for this reason that the span of the demand curve in this restated model cannot characterize 0, despite including such quantities in non-linear regression. Additionally, it is also relevant to note that the regressive process for logarithmic and linear implementations of the model differs with respect to how residual error is interpreted and this introduces behavior that varies between implementations [see @gilroy2021].

## Same Model But Different Error

The challenges associated with fitting models of operant demand (i.e., minimizing residual error) are increasingly reviewed by researchers applying the Operant Demand Framework [@gilroy2020interpretation; @gilroy2021]. @gilroy2021 noted, among other things, that residual error is reflected differently in log and linear scales and that such differences affect model optimization, the resulting parameters, and even the interpretations of such parameters. For example, changes in log scale represent relative differences while changes in linear scale represent absolute differences. In most economic applications, relative error is preferred because the usual quantities of interest and their associated projections (i.e., $\hat{y}$) often span across multiple orders of magnitude (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). In these situations, the quantities observed at higher orders would be weighted more heavily in absolute least squares regression (linear scale) than those at lower orders unless some form of correction was applied (i.e., weighting). It is for this reason that relative difference is often the default in these applications.

Whereas relative differences reference another quantity (e.g., predicted values, weights), absolute differences are straightforward. That is, absolute error is simply the difference from some observed quantity and $\hat{y}$ regardless of the order of magnitude. Although more straightforward, the use of the linear scale in the Operant Demand Framework introduces some variability in how parameters are optimized in these models. For example, @gilroy2021 noted how a departure from relative difference has led to occasional inconsistencies wherein estimates across implementations of the @hursh2008 framework have led to different conclusions (e.g., shared vs. respective $\alpha$ values across varying dose-response curves). In addition to fitted estimates, reflecting residual error in terms of relative differences tends to yield more normalized patterns of error variance. As such, differences in how residual error is handled represent one dimension along which the two implementations of the @hursh2008 framework differ.

## Different Error but Same Asymptotes

There has been renewed attention regarding the asymptotes of models based upon the framework of @hursh2008. As currently designed, neither the @hursh2008 nor the @koff2015expt model can characterize demand at 0 and this is because both reflect the span of the demand curve in log units [@gilroy2021]. To address this fundamental issue, @gilroy2021 presented a "log-like" alternative to the log scale that preserves many of the desirable qualities of the log scale while accommodating 0 consumption values in the Operant Demand Framework. For example, this alternative (i.e., Inverse Hyperbolic Sine transformation) replicates the behavior of logarithms across a range of values (e.g., 10, 100), normalizes residual error variance, and supports a true lower bound at 0. This approach is not discussed at length in this report, though interested readers are encouraged to consult @gilroy2021 for a discussion and demonstration of this "log-like" scale and its benefits (e.g., de-coupling of $\alpha$ from the span of the curve, separate span parameter not necessary).

Revisiting the topic of asymptotes, two novel terms are introduced in this work: $A_{Upper}$ and $A_{Lower}$. The term $A_{Upper}$ is used to refer to the absolute upper bound of the demand curve. In models derived from the @hursh2008 framework, this is simply the fitted parameter $Q_{0}$. This is because parameter $Q_{0}$ is the absolute upper limit to the demand curve when prices equals 0, i.e. $\displaystyle{f(0)= A_{Upper}}$. In contrast, the term $A_{Lower}$ refers the absolute lower bound of the demand curve and this is not reflected by any *single* parameter. Mathematically, this absolute lower limit refers to the level of demand as price approaches $\infty$, i.e.$\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}$. These two upper and lower extremes are separated by the span constant $\textit{k}$, which specifies the distance in log units between these asymptotes [@gilroy2021]. The notation of both $A_{Upper}$ and $A_{Lower}$ are noted below and are proofed in greater detail across models in the Appendix of this work.

```{=tex}
\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}
```
Further inspection of $A_{Lower}$ and its derivation evokes questions regarding how any model based on the @hursh2008 framework could characterize non-consumption values. As noted in the bottom portion of *Equation 1*, neither the @hursh2008 and @koff2015expt models could represent a value of 0 because this value does not fall within the interval between these upper and lower extremes. This introduces a complex situation wherein 0 consumption values could be included in non-linear regression, but the predicted levels of demand could never characterize this value. As such, the issue with non-zero lower asymptotes is one dimension along which derivatives of the @hursh2008 framework are the same.

## Same Asymptotes and Same Spans

Understanding non-consumption in the @hursh2008 framework requires an appreciation of how the span parameter $\textit{k}$ influences the range of values that may be predicted (i.e., $\hat{y}$). In the original implementation of the @hursh2008 framework, $\textit{k}$ represented the range of observed, non-zero consumption values. That is, $\textit{k}$ was derived in log units from the upper and lower extremes of all positive, real numbers. Since 0 consumption values were not included in the original implementation, parameter $\textit{k}$ was directly linked to the upper and lower limits of the observed data. Indeed, the specification of this constant was straightforward and parameter $\textit{k}$, $A_{Upper}$, and $A_{Lower}$ were all directly linked to positive real numbers. A visualization of parameter $\textit{k}$ is provided in \autoref{fig:fig1CurveSpanEmpirical} with respect to positive real consumption values.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  geom_point() +
  annotate("segment",
           x      = 0.1,
           xend   = 0.1,
           y      = 99.54077086,
           yend   = lowerAsymptote,
           colour = "grey40") +
  annotate("text",
           x     = 0.175,
           y     = 55,
           angle = 90,
           adj   = 0.5,
           col   = "grey40",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment",
           x      = 0.1,
           xend   = 5000,
           y      = lowerAsymptote,
           yend   = lowerAsymptote,
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "grey40",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment",
           x      = 0.1,
           xend   = 5000,
           y      = max(preFit$Q),
           yend   = max(preFit$Q),
           colour = "grey40",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "grey40",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = c(preFit$P),
                labels = c(preFit$P),
                limits = c(0.1, 5000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )

```

Whereas the determination of parameter $\textit{k}$ in the @hursh2008 implementation is linked to positive real numbers, the interpretation of parameter $\textit{k}$ became more complicated in the implementation introduced by @koff2015expt. This added complexity emerged because parameter $\textit{k}$ was still based on positive real numbers but had to be inflated to project $A_{Lower}$ *beyond* the range of positive real values to a quantity nearer to 0. This represented novel behavior for parameter $\textit{k}$ and various teams have constructed strategies to assist in driving $A_{Lower}$ *beyond* the range of non-zero consumption. For example, some have added a constant to parameter $\textit{k}$ (derived from positive real values) or allowed this parameter to vary as a fitted parameter [@kaplan2018understanding]. Regardless of the method, the rationale was to inflate the span of the demand curve to and drive $A_{Lower}$ to a lower point. A visualization of this span-inflating behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE, fig.height=3.5}


preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  scale_color_manual(values = c(
    "K (Empirical)" = "grey60",
    "K + 0.5"       = "grey50",
    "K + 1"         = "grey40",
    "K + 2"         = "grey20",
    "K + 3"         = "grey0"
  )) +
  annotate("text",
           x     = 100,
           y     = 90,
           adj   = 0,
           col   = "grey60",
           parse = TRUE,
           label = expression("K (Empirical): " ~ A[Lower] == 5.442)) +
  annotate("text",
           x     = 100,
           y     = 75,
           adj   = 0,
           col   = "grey50",
           parse = TRUE,
           label = expression("K + 0.5: " ~ A[Lower] == 1.7209)) +
  annotate("text",
           x     = 100,
           y     = 60,
           adj   = 0,
           col   = "grey40",
           parse = TRUE,
           label = expression("K + 1: " ~ A[Lower] == 0.5442)) +
  annotate("text",
           x     = 100,
           y     = 45,
           adj   = 0,
           col   = "grey20",
           parse = TRUE,
           label = expression("K + 2: " ~ A[Lower] == 0.0544)) +
  annotate("text",
           x     = 100,
           y     = 30,
           adj   = 0,
           col   = "grey0",
           parse = TRUE,
           label = expression("K + 3: " ~ A[Lower] == 0.0054)) +
  scale_x_log10(breaks = c(preFit$P),
                labels = c(preFit$P),
                limits = c(0.1, 5000)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none"
  )

```

\autoref{fig:fig2CurveSpanEmpiricalMod} illustrates how inflating the span affects $A_{Lower}$ and this has three appreciable effects on the demand curve. First, the rate of change in elasticity is jointly reflected by parameter $\alpha$ and the span of the demand curve [@gilroy2020interpretation]. Given that $\alpha$ is a unit-less quantity, it co-varies inversely with the size of the span constant. For example, relatively greater $\alpha$ values reflect rapid changes in $\hat{y}$ across prices and relatively lesser values reflecting gradual changes in $\hat{y}$ across prices. Second, $\textit{k}$ values (i.e., $k < \frac{e}{log(10)}$) influence both the span of the demand curve as well as the range of elasticity and inelasticity observed in models derived from the @hursh2008 framework [@newman2020improved; @gilroy2019exactsolve]. That is, *k* values below 1 log unit restrict the range of elasticity values and render analytic solutions for unit elasticity impossible. Third, and most relevant to the @koff2015expt model, inflated $\textit{k}$ values serve to lessen the absolute difference between $A_{Lower}$ and 0. That is, the distance between $A_{Lower}$ and 0 is is lessened, but no $\textit{k}$ value will drive the span to 0. \autoref{fig:fig2CurveSpanEmpiricalMod} provides an elegant display of how this gap decreases proportionally with each unit increase in the span parameter *k* but will never reach 0.

## Hidden Model Equivalence

The sections above outline the few ways in which two of the most popular derivatives of the @hursh2008 framework differ. These two modeling strategies differ in terms of optimization (i.e., minimization of residual error) but share the limitations related to asymptotes. Regarding the first point, residual error and optimization, the two models can provide equivalent results when the handling of residual error is made *comparable*. That is, re-weighting the errors (i.e., relative to $\hat{y}$) in the @koff2015expt model can yield fits and estimates approximate to those resulting from the @hursh2008 model *in the absence of non-consumption.* Alternatively, the @hursh2008 model can be adjusted to yield estimates comparable to those from the @koff2015expt model by adjusting residual error to be interpreted in terms of absolute difference, i.e. $E_i=10^{\hat{y}}-10^{y}$. A visualization of inter-related model fits are illustrated in \autoref{fig:fig3ComparisonNonzeroRelativeError}.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error", fig.height=2, warning=FALSE}


preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 125),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
              data = preFit.trim,
              start = c(a  = 0.0001,
                        Q0 = 100),
              lower = c(a = -Inf,
                        Q0 = 1),
              upper = c(a = Inf,
                        Q0 = 125),
              algorithm = "port",
              weights = 1/(predict(EXPT)),
              control = list(
                maxiter = 1000,
                warnOnly = TRUE
              ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  dplyr::rename(`Exponential`                    = explPred,
         `Exponentiated`                  = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P)

ggplot(preFit.trim, aes(P, Q, colour = Model, lty = Model)) +
  geom_point(color = "black") +
  geom_line(aes(P, Prediction)) +
  xlab("Unit Price") +
  ylab("Consumption") +
  ylim(0, 100) +
  scale_colour_grey(start = .0, end = .8) +
  scale_linetype_manual(values = c(1,1,2)) +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom",
    strip.background.x = element_blank()
  )

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * Q0 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * Q0 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

preFit.point.1 <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0),
  Model = "Exponentiated"
)

preFit.point.2 = preFit %>%
  select(P, Q) %>%
  mutate(Model = "Exponential (Absolute)")

preFit.point.full = rbind(preFit.point.1,
                          preFit.point.2)

```

Regarding the second point, $A_{Lower}$ is seldom discussed in Operant Demand and this has considerable influence on models derived from the @hursh2008 framework. This is an inherently complex topic, especially so in the @koff2015expt restatement, because consumption values observed at 0 are a quantity that cannot be predicted by models that reflect the range of consumption in log units. In attempts to accommodate non-consumption, modeling based on the @hursh2008 framework must minimize *two* sources of error instead of one. That is, the non-linear regression must minimize residual error as well as the distance between $A_{Lower}$ and 0 (i.e., *k*) in an attempt to produce an $A_{Lower}$ that *approximates* 0. For instance, an application of the @koff2015expt model where $\textit{k}$ is included as a fitted parameter simultaneously optimizes demand intensity, rates of change in elasticity, and a span constant (i.e., $A_{Lower}$). As noted above, $A_{Lower}$ is driven lower by inflating the span constant towards some non-zero quantity that is *reasonably* close to 0. Pragmatically, proponents of the @koff2015expt approach would likely argue that such a small amount of error calls for little concern and that $A_{Lower}$ could be considered *close enough* of an approximation of 0 to enable analyses using the complete data set (non-consumption values included).

Revisiting the argument for a *close enough* approximation of non-consumption, let us consider the following hypothetical. Let us say that the interpretation of a fitted @koff2015expt model optimizes such that values at $A_{Lower}$ are a close enough approximation of 0 to proceed with demand curve analyses using a complete data set. Following this logic (i.e., $A_{Lower} \cong 0$), it stands to reason that treating sufficiently low $A_{Lower}$ values and 0 consumption values as the same *should* replicate the behavior of the @koff2015expt model in the @hursh2008 model. Assuming an inflated $\textit{k}$ parameter, equivalent estimates should result because $\hat{y}$ can be predicted beyond the range of observed non-zero levels and the resulting $A_{Lower}$ should be *close enough* to 0 on the linear scale that differences between $A_{Lower}$ and 0 would be considered negligible. Controlling for differences in terms of error representation, it stands to reason that the @hursh2008 model would provide equivalent estimates had non-consumption been replaced by respective $A_{Lower}$ values and error minimization been reflected in terms of absolute differences.

In a demonstration of this modified @hursh2008 approach, the full data set from \autoref{fig:fig1CurveSpanEmpirical} was fitted with an inflated $\textit{k}$ parameter and non-consumption values replaced with respective $A_{Lower}$ values. Specifically, the most inflated span and corresponding $A_{Lower}$ from \autoref{fig:fig2CurveSpanEmpiricalMod} were used in this example demonstration, i.e. $A_{Lower} = 10^{log{10}Q_0 - (k + 3)}$. The results of this modified @hursh2008 approach are illustrated along with the @koff2015expt approach are illustrated in \autoref{fig:fig4ComparisonZeroRelative}.

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes", fig.height=2}

ggplot(subset(preFit.g, Model != "Exponential (Relative)"), aes(P, Q)) +
  geom_point(data = preFit.point.full) +
  geom_line(aes(P, Prediction), color = "grey40") +
  scale_x_log10(breaks = c(0.1, 1, 10, 100, 1000, 10000),
                labels = c(0.1, 1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  facet_wrap(~Model, ncol = 3) +
  xlab("Unit Price") +
  ylab("Consumption") +
  theme_bw() +
  theme(
    axis.line = element_line(colour = "black"),
    panel.border = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none", 
    strip.background.x = element_blank()
  )

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  select(-c(`Exponential.Relative`)) %>%
  relocate(Exponentiated, .after = Q) %>%
  mutate(`Q.Mod` = Q) %>%
  mutate(Q = preFit.point.1$Q) %>%
  relocate(`Exponential.Absolute`, .after = `Q.Mod`) %>%
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

Controlling for differences in error handling (absolute difference) and $A_{Lower}$ values (non-consumption replaced by lower asymptotes in the @hursh2008 model), the model fits are functionally equivalent, see \autoref{tab:table1ComparisonZeroAbsolute}. This short example highlights several details that often go unnoticed when using the @koff2015expt model. First, this model does not characterize demand at 0. Rather, an inflated $\textit{k}$ parameter to drives $A_{Lower}$ to a quantity *close enough* to 0 that the absolute difference between 0 and $\hat{y}$ is negligible. This is the best that this approach can achieve because 0 does not fall within the interval between $A_{Upper}$ and $A_{Lower}$. Second, this approach is functionally equivalent to the @hursh2008 model when non-consumption values are replaced with by the respective $A_{Lower}$ values and when residual errors are de-weighted (i.e., absolute). As such, both functional almost identically and differ in largely trivial aspects.

## Research Questions

The Operant Demand Framework has achieved high regard as a robust approach for evaluating choices and behavior of societal significance [@hurshRoma2013; @reed2013behavioral]. Various labs and teams have been working towards expanding the scale and scope of this approach, moving from questions specific to individuals and groups to society-at-large [@hurshRoma2013; @roma2017progress]. This approach and its methods are increasingly represented in a range of scientific tools and packages as well [@gilroy2018; @kaplan2019r]. Despite increasing popularity and accessibility, few resources provide the mathematical details necessary to support researchers in navigating between the available options for performing demand curve analyses.

The purpose of this technical report was to review the mathematical underpinnings of two prevailing models derived from the framework of @hursh2008 and present an argument as to why distinctions between such models create more confusion than consensus.

Specifically, the shared mathematical bases between the two should allow for modifications wherein both provide statistically equivalent estimates---even when non-consumption values are present. The primary questions for the simulation study was whether estimates resulting from the @hursh2008 and @koff2015expt models would be statistically equivalent when controlling for differences in handling residual error (i.e., absolute, relative) and treating non-consumption values as respective $A_{Lower}$ values.

# Methods

```{r simulationParams}

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 20000

if (file.exists("sim-full.rds")) {
  sim = readRDS("sim-full.rds")

} else {
  sim           = SimulateDemand(nruns     = nParticipants,
                                 setparams = setparams,
                                 sdindex   = sdindex,
                                 x         = x)

  saveRDS(sim, file = "sim-full.rds")
}

dataSet = sim$simvalues

if (file.exists("sim-full-passingFrame.rds")) {
  passingFrame = readRDS("sim-full-passingFrame.rds")

} else {
  passingFrame  = beezdemand::CheckUnsystematic(dataSet,
                                                ncons0    = 1,
                                                reversals = 0)

  saveRDS(passingFrame, "sim-full-passingFrame.rds")
}

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 3

```

## Data Generating Process

A total of `r nParticipants` hypothetical data series were simulated using using the R Statistical Program [@R-base]. The specific syntax used to generate was featured in an R package that was submitted to peer-review [@kaplan2019r]. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to simulate hypothetical purchase task data that included a large composition of non-consumption values. The seed values and variance used to generate these data were identical to those that were used in @koff2015expt. This specific data generating process was used as the basis for comparisons with the @hursh2008 model given that the authors of the @koff2015expt study modeled their approach around "messy" data frequently observed in "real-world" purchase tasks that are often conducted on "crowdsourced" platforms, e.g. Amazon's Mechanic Turk (mTurk).

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all generated demand data. Specifically, individual series were screened for *bounce*, *trend*, and *reversals from zero*. The first criterion, bounce, refers to local changes within an expected downward trend as a function of increasing price. That is, it is unexpected to see consumption increases immediately following a price increase. The second criterion, trend, refers to the molar change in consumption from the lowest to the highest price. That is, there is a certain amount of decrease in consumption expected across the full domain of price increases. Lastly, reversals from zero refer to the return of consumption at a higher price following the cessation of consumption at a lower price. Such trends are inconsistent with expected patterns of consumption. Simulated data were carried forward into the final analysis if each series met all indicators of systematic hypothetical purchase task data.

## Modeling Strategies

A total of 4 modeling approaches were evaluated (2 models, 2 error interpretations). Each approach was referenced as a specific strategy for conducting demand curve analysis when non-consumption values were observed in the data. This facilitated two pairwise comparisons when both models shared a comparable approach for handling residual error. These comparisons were used to determine whether the various strategies provided statistically equivalent estimates when asymptotes and error differences were comparable. Consistent with efforts to maintain open and transparent science [@gilroy2019furthering], the source code necessary to reproduce these strategies and this report has been posted for public review in a GitHub repository managed by the corresponding author, see Author Note. Each of the strategies used in these comparisons are presented below in greater detail.

### Strategy 1: @koff2015expt Model (Absolute Error)

The @koff2015expt model (absolute error difference) was fitted to simulated consumption data at the individual-level. The model was fit using the *optim* package included in the R Statistical Program [@R-base] due to its considerable flexibility in performing ordinary least squares regression. Initial starts were derived based on the respective data for parameter $Q_0$ and both $Q_0$ and $\alpha$ were estimated on the log scale to 1) support more comparable step sizes in the optimization and 2) facilitate pairwise comparisons across strategies. The span constant $k$ was derived from the empirical range of the full data set with an added constant (i.e., *k* = *k +* 3) to allow the span of the demand curve to extend below the lowest non-zero point of consumption, as is common practice [@kaplan2018understanding]. The same span constant was used across all models to enable consistent comparisons between $Q_0$ and $\alpha$. Non-consumption values remained at a value of 0 in this approach.

### Strategy 2: @koff2015expt Model (Percentage Error)

The @koff2015expt model (percentage error difference) was evaluated consistent with Strategy 1 with the exception of how differences in residual error were reflected. In this approach, the absolute residuals simulated relative error by referencing $\hat{y}$, i.e. $e_i = (\hat{y}- y) * \frac{1}{\hat{y}} = \frac{\hat{y}- y}{\hat{y}}$. It warrants noting that this manner of weighting error is not identical to log difference. That is, the weighting of the absolute error difference against $\hat{y}$ is equivalent to reflecting residual error as percentage difference and this corresponds with log difference only certain circumstances, i.e. $ln\frac{Y_1}{Y_2} \approx \frac{Y_2 - Y_1}{Y_1}$. Briefly, percentage difference is nearly identical to log difference with very small differences (e.g., 1% change) but the two diverge once the degree of difference between values grows larger (e.g., 50% change). As such, the varying approaches to reflecting relative differences are expected to vary and this source of error between the approaches is described more thoroughly in the Appendix. Regardless, the two approaches are expected to behave comparably but are not expected to be equivalent. All other parameters were estimated consistent with Strategy 1.

### Strategy 3: @hursh2008 Model (Log Difference Error)

The @hursh2008 model (log error difference) was fitted to simulated consumption data at the individual-level. During the fitting, non-consumption values were replaced by an $A_{Lower}$ value that was generated dynamically based on parameters $Q_0$ and $\textit{k}$ during parameter estimation. That is, a customized loss function was prepared for use with the *optim* method. As noted in Strategy 2, both Strategy 2 and Strategy 3 reflected relative difference in different ways. All other parameters were estimated consistent with the other strategies.

### Strategy 4: @hursh2008 Model (Absolute Error)

The @hursh2008 model (absolute error difference) was fitted to simulated consumption data at the individual-level as well. This strategy was identical to that of Strategy 3 with the exception of how residual error was interpreted during optimization. Consistent with Strategy 3, non-consumption values were replaced by an $A_{Lower}$ value that was generated dynamically based on parameters $Q_0$ and $\textit{k}$ during parameter estimation. A customized loss function was used to represent residual error in terms of absolute differences, i.e. $e_i = 10^{\hat{y}} - 10^{y}$. All other parameters were estimated consistent with that of the other strategies.

## Analytical Strategy

Pairwise comparisons were conducted for parameters $Q_0$ and $\alpha$ resulting from each of the four strategies while controlling for differences in how residual error was interpreted during optimization. T-tests and tests of equivalence were used to compare estimates resulting from the @koff2015expt model and the @hursh2008 model with and without modified error terms. T-tests were calculated using the base methods in R and the *tost* method in the *equivalence* R package was used to perform two one-sided t-tests [TOSTs, @robinson2016package]. Specifically, t-tests were used first in each comparison to test whether a significant difference was observed between estimates. Tests of equivalence were performed if a non-significant difference was observed. The emphasis here was not on determining a lack of *difference* between strategies but instead on determining whether these were practically *equivalent*. The Smallest Effect Size of Interest (SESOI) was set to 0.01 (i.e., \~1% difference in log scale) and differences below this threshold were not considered practically meaningful. Across all tests, corrections were applied due to presence of repeated comparisons, i.e. $p=0.05/2=0.025$.

# Results

```{r coreAnalyses}
keepers = passingFrame %>%
  dplyr::filter(TotalPass == 3 & NumPosValues > 5) %>%
  dplyr::select(id) %>%
  dplyr::pull()

nPassing = length(keepers)

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

R2.EXPT <- function(data, par) {
  RSS = with(data, sum((getEXPT(10^par[1], kSet, 10^par[2], x) - y)^2))

  1.0 - (RSS/sum((data$y - mean(data$y))^2))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(10^par[1], kSet, 10^par[2], x) - y)^2))
}

R2.EXPT.aw <- function(data, par) {
  newData = data

  newData$ys   = getEXPT(10^par[1], kSet, 10^par[2], newData$x)
  newData$err  = (newData$y - newData$ys)/newData$ys
  newData$err  = newData$err^2

  RSS = sum(newData$err)

  1.0 - (RSS/sum((newData$y - mean(newData$y))^2))
}

min.RSS.EXPT.aw <- function(data, par) {
  newData = data

  newData$ys   = getEXPT(10^par[1], kSet, 10^par[2], newData$x)
  newData$err  = (newData$y - newData$ys)/newData$ys
  newData$err  = newData$err^2

  sum(newData$err)
}

R2.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(10^par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  RSS = with(newData, sum((getEXPL(10^par[1], kSet, 10^par[2], x) - y)))

  1.0 - (RSS/sum((newData$y - mean(newData$y))^2))
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(10^par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(10^par[1], kSet, 10^par[2], x) - y)^2))
}

R2.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(10^par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y    = log10(newData$y)
  newData$ys   = getEXPL(10^par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  RSS = sum(newData$err)

  1.0 - (RSS/sum((10^newData$y - mean(10^newData$y))^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(10^par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y    = log10(newData$y)
  newData$ys   = getEXPL(10^par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

min.RSS.EXPL.aw2 <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(10^par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(10^par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$y - 10^newData$ys)/10^newData$ys
  newData$err  = newData$err^2

  sum(newData$err)
}

lowerBounds = c(0.5,
                -6)

if (file.exists("sim-full-frameToAnalyze.rds")) {
  frameToAnalyze = readRDS("sim-full-frameToAnalyze.rds")

} else {
  dataFramePrep = data.frame(
    id       = 1:nPassing,
    tag      = numeric(length = nPassing),
    q0.HS    = numeric(length = nPassing),
    q0.HSw   = numeric(length = nPassing),
    q0.Koff  = numeric(length = nPassing),
    q0.Koffw = numeric(length = nPassing),
    a.HS     = numeric(length = nPassing),
    a.HSw    = numeric(length = nPassing),
    a.Koff   = numeric(length = nPassing),
    a.Koffw  = numeric(length = nPassing),
    r2.HS    = numeric(length = nPassing),
    r2.HSw   = numeric(length = nPassing),
    r2.Koff  = numeric(length = nPassing),
    r2.Koffw = numeric(length = nPassing),
    K        = numeric(length = nPassing),
    c.HS     = numeric(length = nPassing),
    c.HSw    = numeric(length = nPassing),
    c.Koff   = numeric(length = nPassing),
    c.Koffw  = numeric(length = nPassing)
  )

  for (id in 1:nPassing) {

    if (id %% 100 == 0) {
      message(paste("Iteration #", id, "of", nPassing))
    }

    currentData              = dataSet[dataSet$id == as.numeric(keepers[id]), ]
    dataFramePrep[id, "tag"] = keepers[id]
    dataFramePrep[id, "K"]   = kSet

    startQ = log10(max(currentData$y))
    startA = -3
    ctrlClass = list(
      maxit = 1000
    )

    methodSetting = "L-BFGS-B"

    upperBounds = c(log10(max(currentData$y) * 2),
                    -1)

    # ==================================================
    # Un-Weighted sets
    # ==================================================

    fit.HS   <- optim(par    = c(startQ, startA),
                      fn     = min.RSS.EXPL,
                      method = methodSetting,
                      lower  = lowerBounds,
                      upper  = upperBounds,
                      data   = currentData,
                      control = ctrlClass)

    fit.Koff <- optim(par    = c(startQ, startA),
                      fn     = min.RSS.EXPT,
                      method = methodSetting,
                      lower  = lowerBounds,
                      upper  = upperBounds,
                      data   = currentData,
                      control = ctrlClass)

    # ==================================================
    # Weighted sets
    # ==================================================

    fit.Koffa <- optim(par    = c(startQ, startA),
                       fn     = min.RSS.EXPT.aw,
                       method = methodSetting,
                       lower  = lowerBounds,
                       upper  = upperBounds,
                       data   = currentData,
                       control = ctrlClass)

    fit.HWa <- optim(par    = c(startQ, startA),
                     fn     = min.RSS.EXPL.aw,
                     method = methodSetting,
                     lower  = lowerBounds,
                     upper  = upperBounds,
                     data   = currentData,
                     control = ctrlClass)

    if (length(unique(currentData$y[currentData$y > 0])) > 2) {
      dataFramePrep[id, c("q0.HS", "a.HS")]       = fit.HS$par
      dataFramePrep[id, "r2.HS"]                  = R2.EXPL(currentData, fit.HS$par)
      dataFramePrep[id, "c.HS"]                   = fit.HS$convergence

      dataFramePrep[id, c("q0.Koffw", "a.Koffw")] = fit.Koffa$par
      dataFramePrep[id, "r2.Koffw"]               = R2.EXPT.aw(currentData, fit.Koffa$par)
      dataFramePrep[id, "c.Koffw"]                = fit.Koffa$convergence

      dataFramePrep[id, c("q0.HSw", "a.HSw")]     = fit.HWa$par
      dataFramePrep[id, "r2.HSw"]                 = R2.EXPL.aw(currentData, fit.HWa$par)
      dataFramePrep[id, "c.HSw"]                  = fit.HWa$convergence

      dataFramePrep[id, c("q0.Koff", "a.Koff")]   = fit.Koff$par
      dataFramePrep[id, "r2.Koff"]                = R2.EXPT(currentData, fit.Koff$par)
      dataFramePrep[id, "c.Koff"]                 = fit.Koff$convergence
    } else {
      dataFramePrep[id, c("q0.HS", "a.HS")]       = NA
      dataFramePrep[id, "r2.HS"]                  = NA
      dataFramePrep[id, "c.HS"]                   = NA

      dataFramePrep[id, c("q0.Koffw", "a.Koffw")] = NA
      dataFramePrep[id, "r2.Koffw"]               = NA
      dataFramePrep[id, "c.Koffw"]                = NA

      dataFramePrep[id, c("q0.HSw", "a.HSw")]     = NA
      dataFramePrep[id, "r2.HSw"]                 = NA
      dataFramePrep[id, "c.HSw"]                  = NA

      dataFramePrep[id, c("q0.Koff", "a.Koff")]   = NA
      dataFramePrep[id, "r2.Koff"]                = NA
      dataFramePrep[id, "c.Koff"]                 = NA
    }
  }

  frameToAnalyze = dataFramePrep

  # Filter out poor convergence
  frameToAnalyze = frameToAnalyze %>%
    mutate(ConvergenceSum = c.HS + c.Koffw + c.HSw  + c.Koff) %>%
    filter(ConvergenceSum == 0)

  frameToAnalyze = frameToAnalyze %>%
    mutate(FitSum = r2.HS + r2.Koffw + r2.HSw  + r2.Koff) %>%
    arrange(-FitSum) %>%
    dplyr::top_n(1000)

  saveRDS(frameToAnalyze, "sim-full-frameToAnalyze.rds")
}

```

The data generating process was used to produce a total of `r nParticipants` distinct consumption series that simulated hypothetical purchase task data. A range of series was simulated but was restricted to those that met all indices of systematic purchase task data, contained 50% or more non-zero consumption, and featured at least two unique positive real consumption values (i.e., non-step data). Within these series, the *R^2^* metric was used as the basis for selecting the 1,000 series that best represented the optimal performance across all fitted models. The results of specific pairwise comparisons across these 1,000 cases are presented below.

## Strategy 1 vs. Strategy 4 (Absolute Error)

```{r s1vs4fig, fig.cap="Comparisons of Strategy 1 and 4 (Absolute Error)", fig.height=4}

library(equivalence)

t.1v4.q0 = t.test(frameToAnalyze$q0.Koff, frameToAnalyze$q0.HSw)

tost.1v4.q0 = tost(frameToAnalyze$q0.Koff, 
                   frameToAnalyze$q0.HSw,  
                   epsilon = 0.01, 
                   conf.level = 0.975,
                   paired = TRUE)

cor.1v4.q0 = cor.test(frameToAnalyze$q0.Koff, 
                      frameToAnalyze$q0.HSw)

t.1v4.a = t.test(frameToAnalyze$a.Koff, frameToAnalyze$a.HSw)

tost.1v4.a = tost(frameToAnalyze$a.Koff, 
                  frameToAnalyze$a.HSw, 
                  epsilon = 0.01, 
                  conf.level = 0.975,
                  paired = TRUE)

cor.1v4.a = cor.test(frameToAnalyze$a.Koff, 
                     frameToAnalyze$a.HSw)

par(mfrow = c(1, 2),
    oma   = c(0, 0, 0, 0))

plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main  = expression(Q[0] ~ "Estimates"),
     col   = alpha("black", 0.4),
     ylab  = "Strategy 1",
     xlab  = "Strategy 4",
     xaxt  = "n",
     xlim  = c(0, 3),
     yaxt  = "n",
     ylim  = c(0, 3),
     frame = FALSE)

axis(1, at = seq(0, 3, 1))
axis(2, at = seq(0, 3, 1))

box(bty = "l")

plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main  = expression(alpha ~ "Estimates"),
     col   = alpha("black", 0.4),
     ylab  = "Strategy 1",
     xlab  = "Strategy 4",
     xaxt = "n",
     xlim  = c(-6, -2),
     yaxt = "n",
     ylim  = c(-6, -2),
     frame = FALSE)

axis(1, at = seq(-6, -2, 1))
axis(2, at = seq(-6, -2, 1))

box(bty = "l")

```

The primary comparison of interest in this report was between Strategy 1 and Strategy 4 This comparison evaluated the correspondence between the @hursh2008 and @koff2015expt models when error differences were represented in terms of absolute difference and when non-consumption was treated as $A_{Lower}$ for the @hursh2008 model. Given the shared mathematical basis for each, the estimates resulting from each were expected to be equivalent.

An evaluation of the relationship between Strategy 1 and 4 revealed perfect correlations for both $Q_0$ (r=`r cor.1v4.q0$estimate`, t=`r cor.1v4.q0$statistic`, df=`r cor.1v4.q0$parameter`, `r scales::pvalue(cor.1v4.q0$p.value, accuracy = .025, add_p = TRUE)`) and for $\alpha$ (r=`r cor.1v4.a$estimate`, t=`r cor.1v4.a$statistic`, df=`r cor.1v4.a$parameter`, `r scales::pvalue(cor.1v4.a$p.value, accuracy = .025, add_p = TRUE)`). That is, a perfect rank ordering was observed across strategies and across parameters. T-test comparisons were non-significant for $Q_0$ (t=`r t.1v4.q0$statistic`, df=`r t.1v4.q0$parameter`, `r scales::pvalue(t.1v4.q0$p.value, accuracy = .025, add_p = TRUE)`) and for $\alpha$ (t=`r t.1v4.a$statistic`, df=`r t.1v4.a$parameter`, `r scales::pvalue(t.1v4.a$p.value, accuracy = .025, add_p = TRUE)`). Subsequent TOSTs were significant for $Q_0$ (`r scales::pvalue(tost.1v4.q0$tost.p.value, accuracy = .025, add_p = TRUE)`) and for $\alpha$ (`r scales::pvalue(tost.1v4.a$tost.p.value, accuracy = .025, add_p = TRUE)`). Specifically, results of equivalence testing rejected the null hypothesis of statistical difference for both parameters and this indicated that estimates resulting from each strategy were statistically equivalent. A visualization of these corresponding estimates is illustrated in \autoref{fig:s1vs4fig}.

## Strategy 2 vs. Strategy 3 (Relative Error)

```{r s2vs3fig, fig.cap="Comparisons of Strategy 2 and 3 (Relative Error)", fig.height=4}

t.2v3.q0 = t.test(frameToAnalyze$q0.Koffw, frameToAnalyze$q0.HS)

cor.2v3.q0 = cor.test(frameToAnalyze$q0.Koffw, 
                      frameToAnalyze$q0.HS)

t.2v3.a = t.test(frameToAnalyze$a.Koffw, frameToAnalyze$a.HS)

cor.2v3.a = cor.test(frameToAnalyze$a.Koffw, 
                     frameToAnalyze$a.HS)

par(mfrow = c(1, 2),
    oma   = c(0, 0, 0, 0))

plot(frameToAnalyze$q0.Koffw ~ frameToAnalyze$q0.HS,
     main  = expression(Q[0] ~ "Estimates"),
     col   = alpha("black", 0.4),
     ylab  = "Strategy 2",
     xlab  = "Strategy 3",
     xaxt  = "n",
     xlim  = c(0, 3),
     yaxt  = "n",
     ylim  = c(0, 3),
     frame = FALSE)

axis(1, at = seq(0, 3, 1))
axis(2, at = seq(0, 3, 1))

box(bty = "l")

plot(frameToAnalyze$a.Koffw ~ frameToAnalyze$a.HS,
     main  = expression(alpha ~ "Estimates"),
     col   = alpha("black", 0.4),
     ylab  = "Strategy 2",
     xlab  = "Strategy 3",
     xaxt = "n",
     xlim  = c(-6, -2),
     yaxt = "n",
     ylim  = c(-6, -2),
     frame = FALSE)

axis(1, at = seq(-6, -2, 1))
axis(2, at = seq(-6, -2, 1))

box(bty = "l")

```

The secondary comparison of interest in this report was between Strategy 2 and Strategy 3. Comparisons between Strategy 2 and Strategy 3 evaluated the correspondence between the @hursh2008 and @koff2015expt models when error differences were interpreted in terms of relative difference and when non-consumption was treated as $A_{Lower}$ for the @hursh2008 model. Specifically, the @hursh2008 model evaluated error using log difference and the @koff2015expt model evaluated error using percentage difference. Given the varying methods of representing residual error as relative, the estimates resulting from each were not expected to be equivalent.

An evaluation of the relationship between Strategy 2 and Strategy 3 revealed strong, but not perfect correlations for $Q_0$ (r=`r cor.2v3.q0$estimate`, t=`r cor.2v3.q0$statistic`, df=`r cor.2v3.q0$parameter`, `r scales::pvalue(cor.2v3.q0$p.value, accuracy = .025, add_p = TRUE)`) and for $\alpha$ (r=`r cor.2v3.a$estimate`, t=`r cor.2v3.a$statistic`, df=`r cor.2v3.a$parameter`, `r scales::pvalue(cor.2v3.a$p.value, accuracy = .025, add_p = TRUE)`). T-test comparisons were significant for $Q_0$ (t=`r t.2v3.q0$statistic`, df=`r t.2v3.q0$parameter`, `r scales::pvalue(t.2v3.q0$p.value, accuracy = .025, add_p = TRUE)`) as well as for $\alpha$ (t=`r t.2v3.a$statistic`, df=`r t.2v3.a$parameter`, `r scales::pvalue(t.2v3.a$p.value, accuracy = .025, add_p = TRUE)`). No TOSTs were performed given that t-tests indicated significant differences between estimates resulting from each strategy. A visualization of these relationships are illustrated in \autoref{fig:s2vs3fig}.

# Discussion

This report provided an in-depth review of how non-consumption values (i.e., 0) have, thus far, been incorporated in models derived from the @hursh2008 framework. As noted throughout this report, both the @hursh2008 and the @koff2015expt approaches are unable to model demand at 0 and both are bounded by the non-zero lower asymptote, $A_{Lower}$. This is the case regardless of whether non-consumption values are included in the regression. As such, the approach put forward in @koff2015expt is not a complete solution for non-consumption values because the same limitations of the original approach remain in this regard. This is because the span of the demand curve in the @hursh2008 framework remains in the log scale, despite LHS exponentiation, and the span in log scale cannot support 0. As an alternative to this issue with span, others have argued that a true solution to this issue would require deviating from the log scale altogether [@gilroy2021].

The proofs and simulations featured in this study facilitated comparisons between the @hursh2008 and @koff2015expt models when controlling for the common $A_{Lower}$ and differences in how residual errors are interpreted during optimization. The goal of these comparisons was to advance the argument that the Exponential [@hursh2008] and Exponentiated [@koff2015expt] models should not be so strongly distinguished. Indeed, it is quite trivial to arrive at statistically equivalent estimates in both approaches when the role of the span constant and $A_{Lower}$ and the method of representing residual error are held perfectly constant. The results of planned comparisons confirmed that the two models provide statistically equivalent estimates when controlling for such differences perfectly---even when non-consumption values are included. However, this is not the case when different methods of addressing residual error are used. That is, similar methods for representing relative differences are closely correlated but not statistically equivalent. This difference is mostly due to how percentage and log difference diverge as differences grow larger (see Appendix).

Given that neither approach can characterize demand at 0, $A_{Lower}$ is the *best* approximation of 0 possible for models derived from the @hursh2008 framework. Following this logic, replacing non-consumption values with respective $A_{Lower}$ values often result in the Exponential model providing estimates that are at least highly correlated with (potentially statistically equivalent to) the Exponentiated model. In advancing this argument, it is necessary to state clearly that this claim is not presented with the intent of favoring any specific approach as a de facto standard or a recommended default when applying methods from the Operant Demand Framework. Rather, this work intended to reveal how these supposedly opposing strategies are functionally interchangeable under specific conditions. Indeed, they are so similar that distinguishing the two only serves to obscure the many shared mathematical bases of each. That said, each approach has common utility and future efforts should be directed towards improving the understanding of the properties of the @hursh2008 framework overall rather than reinforcing any stance, position, or bias towards a specific implementation.

The final aim of this work was to reiterate the ways in which the proponents of each approach have extended the Operant Demand Framework. That is, the proponents of each approach were successful in advancing both the utility and scope of the Operant Demand Framework. For example, the finding that the @hursh2008 model can replicate the behavior of the @koff2015expt without exponentiation terms in no way detracts from the contributions of the @koff2015expt implementation of the framework. Indeed, the @koff2015expt team led the charge towards addressing the problematic issue of removing otherwise valid research data. For decades, substantial portions of otherwise valid consumption data were never carried forward into analyses and it is unclear how these prior analyses would compare had these data been included. Regardless of whether analysts have an established preference for one approach or another, it is clear that the methods included in Operant Demand Framework are better equipped now that non-consumption values can now be considered in the analysis.

## Future Directions in Operant Demand

This perspective and this framework currently reflect a range of consumption (and non-consumption) and efforts are underway to leverage multilevel modeling as a methodological extension [@kaplanMlm]. Indeed, various labs are working toward increasing the applicability and generality of this approach. Towards this end, the intent and mission of the original @koff2015expt study regarding non-consumption values is as valid and valuable today as it was when this work was first published. However, debates and conjecture regarding model superiority (or inferiority) in the absence of formal tests and mathematical proofing do not enhance the Operant Demand Framework in any appreciable manner. That said, the two approaches are functionally interchangeable (even in the presence of non-consumption) and the reader is cautioned against thinking that any single model is inherently "true," "better," or otherwise superior in the absence of careful and individualized statistical evaluation. That said, it is unclear whether the prevailing approach in the Operant Demand Framework will remain based on the framework presented in @hursh2008 well into the future. Indeed, it is possible that future research could explore to deviations from the log scale [@gilroy2021] or adopt a different framework altogether [@newman2020improved]. Regardless of the where the future takes the Operant Demand Framework, future approaches and advances should be met with cautious optimism and consideration rather than disregard in favor for what is preferred or familiar.

\newpage

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

\endgroup

\newpage

# Appendix

Several proofs are provided here to illustrate how the upper and lower asymptotes are determined. Despite the shared mathematical basis, derivations of each are provided below.

## Modified Hursh & Silberburg (2008) Optimization (Relative Error)

$$
e_i = \begin{cases} 
\hat{y}_i - log_{10}y_i       &    \mbox{if } y_i \not= 0 \\
\hat{y}_i - log_{10}A_{Lower} &    \mbox{if } y_i = 0 \\
\end{cases}
$$

## Modified Hursh & Silberburg (2008) Optimization (Absolute Error)

$$
e_i = \begin{cases} 
10^{\hat{y}_i} - 10^{log_{10}y_i}       &    \mbox{if } y_i \not= 0 \\
10^{\hat{y}_i} - 10^{log_{10}A_{Lower}} &    \mbox{if } y_i = 0 \\
\end{cases}
$$

## Hursh & Silberburg (2008) Proofs

### $A_{Upper}$ at $P = 0$

$$
\begin{aligned}
log_{10}A_{Upper} &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * 0} - 1) \\
 &= log_{10}Q_0 + k(e^{0} - 1) \\
 &= log_{10}Q_0 + k(1 - 1) \\
 &= log_{10}Q_0 + k(0) \\
 &= log_{10}Q_0\\
 A_{Upper} &= Q_0
\end{aligned}
$$

Note: Euler's constant raised to the power of 0 is equal to a value of 1. This essentially zeroes out the $\textit{k}$ constant, leaving just the $Q_0$ parameter at 0 $P$.

### $A_{Lower}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
log_{10}A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= log_{10}Q_0 + k(e^{-\alpha * Q_0 * \infty} - 1) \\
 &= log_{10}Q_0 + k(e^{-\infty} - 1) \\
 &= log_{10}Q_0 + k(0 - 1) \\
 &= log_{10}Q_0 + k(-1) \\
 &= log_{10}Q_0 - k \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$

Note: Euler's constant raised to the power of $-\infty$ equates to a value of 0. That is, $e^{-\infty}=\frac{1}{e^\infty}=\frac{1}{\infty} \approx 0$. This is has effect of making the value in parentheses equal to $-1$, which in turn results in the full subtraction of quantity $k$ from $log_{10}Q_0$.

## Koffarnus et al. (2015) Proofs

### $A_{Upper}$ at $P = 0$

$$
\begin{aligned}
A_{Upper} &= Q_0 * 10^{k(e^{-\alpha * Q_0 * 0} - 1)}  \\
 &= Q_0 * 10^{k(e^{0} - 1)}  \\
 &= Q_0 * 10^{k(1 - 1)}  \\
 &= Q_0 * 10^{k(0)}  \\
 &= Q_0 * 10^{0}  \\
 &= Q_0 * 1  \\
 &= Q_0  \\
log_{10}A_{Upper} &= log_{10}Q_0  \\
\end{aligned}
$$

### $A_{Lower}$ at $\displaystyle{\lim_{P \to \infty} f(x)}$

$$
\begin{aligned}
A_{Lower} = \displaystyle{\lim_{P \to \infty} f(x) } &= Q_0 * 10^{k(e^{-\alpha * Q_0 * \infty} - 1)}  \\
 &= Q_0 * 10^{k(e^{-\infty} - 1)}  \\
 &= Q_0 * 10^{k(0 - 1)}  \\
 &= Q_0 * 10^{k(-1)}  \\
 &= Q_0 * 10^{-k}  \\
log_{10}A_{Lower} &= log_{10}Q_0 + (-k)  \\
 &= log_{10}Q_0 - k  \\
 &= log_{10}A_{Upper} - k \\
 A_{Lower} &= 10^{log_{10}A_{Upper} - k}
\end{aligned}
$$ \newpage

## Differences between Log and Percentage Difference

### Logarithmic Difference

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 90
\end{aligned}
$$

$$
\begin{aligned}
ln(\frac{V_2}{V_1})      & = -1 * ln(\frac{V_1}{V_2}) \\
ln(\frac{90}{100})       & = -1 * ln(\frac{100}{90}) \\
ln(0.9)                  & = -1 * ln(1.11) \\
-0.1053                  & = -1 * 0.1053 \\
-0.1053                  & = -0.1053
\end{aligned}
$$

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 50
\end{aligned}
$$

$$
\begin{aligned}
ln(\frac{V_2}{V_1})      & = -1 * ln(\frac{V_1}{V_2}) \\
ln(\frac{50}{100})       & = -1 * ln(\frac{100}{50}) \\
ln(0.5)                  & = -1 * ln(2) \\
-0.6931                  & = -1 * 0.6931 \\
-0.6931                  & = -0.6931
\end{aligned}
$$

\newpage

### Percentage Difference

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 90
\end{aligned}
$$

$$
\begin{aligned}
\frac{ V_2 - V_1}{V_1} & \approx -1 *\frac{ V_1 - V_2}{V_2} \\
\frac{ 90 - 100}{100}  & \approx -1 *\frac{ 100 - 90}{90} \\
\frac{ -10}{100}        & \approx -1 *\frac{ 10}{90} \\
-0.1                    & \approx -1 * 0.11 \\
-0.1                    & \approx -0.11
\end{aligned}
$$

$$
\begin{aligned}
V_1 &= 100 \\
V_2 &= 50
\end{aligned}
$$

$$
\begin{aligned}
\frac{ V_2 - V_1}{V_1} & \approx -1 *\frac{ V_1 - V_2}{V_2} \\
\frac{ 50 - 100}{100}  & \approx -1 *\frac{ 100 - 50}{50} \\
\frac{ -50}{100}       & \approx -1 *\frac{ 50}{50} \\
-0.5                   & \approx -1 * 1 \\
-0.5                   & \approx -1
\end{aligned}
$$ Note: The examples provided above illustrate how log difference $(-0.1053)$ and percentage difference $(-0.1)$ are quite close for small differences. However, the difference between log difference $(-0.6931)$ and percentage difference $(-0.5)$ begins to differ considerably with larger changes. As such, the two approaches to reflecting relative differences are unlikely to be perfectly related outside of optimal conditions.
