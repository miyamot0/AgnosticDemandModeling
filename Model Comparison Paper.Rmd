---
title             : "Asymptotes and Models of Operant Demand: Re-uniting the Hursh & Silberburg Framework"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn P. Gilroy"
    affiliation   : "1"
    corresponding : yes
    address       : "226 Audubon Hall Baton Rouge, Louisiana 70806"
    email         : "sgilroy1@lsu.edu"
    role:
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
  Shawn Patrick Gilroy is an Assistant Professor of School Psychology at Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu. The source code necessary to recreate this work is publicly hosted in a repository at: https://github.com/miyamot0/AgnosticDemandModeling 

abstract: |
  Operant translations of Behavioral Economic concepts and principles have enhanced the ability of researchers to characterize individual choice under constraints and uncertainty. Operant Behavioral Economic models of concurrent choices (i.e., demand) have been particularly useful in evaluating how individual choices are affected by various ecological factors (e.g., price, limited resources). Prevailing perspectives in Operant Demand are derived from the framework of Hursh & Silberburg (2008). Few dispute the utility of this framework, though considerable debate continues regarding how best to address limitations associated with the logarithmic scale. At present, there are opposing views regarding the handling of zero values and under which situations alternative restatements of this framework are warranted, c.f. Koffarnus et al. (2015). The purpose of this report is to review the importance of asymptotes in the Operant Demand Framework and the relatively minor ways in which the Hursh & Silberburg (2008) and Koffarnus et al. (2015) differ. A simulation study is provided as a demonstration of how results from the two models can be largely interchangeable when addressing asymptote and error differences. Additional discussion is provided on whether having multiple models extends the Operant Demand Framework.

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
---

\raggedbottom

```{r include = FALSE}

library(beezdemand)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(tidyverse)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Contemporary methods for evaluating the consumption of goods and services in an Operant Demand Framework draws heavily from the methodology proposed in @hursh2008economic. This framework and methodology has grown through several iterations [@hursh1987linear] with the most recent extensions of the @hursh2008economic framework adopting the linear [@koff2015expt] or another log-like scale [@gilroy2021zbe].

The original implementation of the @hursh2008economic framework was constructed to generate an "s-type" to the demand curve. The original intent was to have an upper asymptote reached at a low (or free) price and a lower asymptote reached as prices approached infinity [@gilroy2021zbe]. Until most recently, this framework [@hursh2008economic, @koff2015expt] modeled demand for reinforcers with non-zero upper and lower asymptotes. That is, these models were bounded at an upper limit (i.e., $Q_0$) and progressed towards a lower asymptote in a roughly S-type form. The non-zero asymptotes in the @hursh2008economic framework make good sense because the consumption values of interest were positive real values. Such quantities were necessary because the logarithmic representation of consumption is undefined.

In response to the omission of zero-valued data, @koff2015expt introduced a restatement of the @hursh2008economic framework but with changes in observed consumption reflected on the linear scale. Specifically, an exponentiation of terms was performed such that the LHS of the @hursh2008economic model were in the linear scale. In this way, the LHS of the model (i.e., consumption) need not be submitted to the log transformation that previous prevented the use of the @hursh2008economic. The draw of this approach is that one of the largest issues associated with the log scale can be avoided. However, it warrants noting that portions of the RHS of the @koff2015expt model remain on the log scale. Specifically, the span of the demand curve and the rate of exponential decay remain in the log scale. The span of the demand curve in this restated model cannot reach a zero point, and thus, is also bound to non-zero upper and lower asymptotes. Additionally, the regressive process for logarithmic and linear models differs in how error is weighted and this introduces behavior novel to the @hursh2008economic framework.

Issues associated with varying methods of minimizing model error are increasingly visible in the Operant Demand Framework [@gilroy2021zbe]. @gilroy2021zbe noted that residual error is reflected different in log and linear scales. That is, changes in log space reflect relative difference and changes in the linear space are absolute. In most economic applications, relative error is typically because projections (i.e., $\hat{y}$) may span multiple order and observed values at higher orders would be weighted more heavily than those at lower orders (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). In this way, relative difference is emphasized and this feature balances how each piece of information is weighted in the model. 

As an alternative to relative difference, absolute difference is more straightforward. That is, absolute error is difference from $\hat{y}$ regardless of scale. @gilroy2021zbe discussed how the departure from relative error handling can affect the modeling of demand that differs across orders. For example, error may become incomparable when fitting a model to sets of data that vary considerably across orders. This has led to occasionally inconsistencies between model fits between the log and linear forms of @hursh2008economic framework (e.g., shared alpha across varying dose-response curves).

Apart from difference in how residual errors are weighted, the issue of zero and non-zero asymptotes has become a topic of discussion [@gilroy2021zbe]. The @gilroy2021zbe work is not discussed at length here; however, readers are encouraged to review this work for an exposition on how the lack of a demand model asymptotic at zero limits the applicability of the framework. Regardless, @gilroy2021zbe noted that both @hursh2008economic and @koff2015expt models cannot characterize demand at zero levels. 

Revisiting the topic of asymptotes, two novel terms are noted here: $A_{Upper}$ and $A_{Lower}$. The term $A_{Upper}$ refers to the upper, typically non-zero bound of the demand curve. In models derived from the @hursh2008economic framework, this is simply the fitted parameter $Q_{0}$. The term $A_{Lower}$ refers the lower bound of the demand curve, extending downward from $A_{Upper}$ at a distance equal to the span constant $\textit{k}$ [@gilroy2021zbe]. These limits are also noted below.

\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}

The notation of $A_{Lower}$ make the difficulties characterizing demand 0 zero more apparent. That is, $\hat{y}$ in both the @hursh2008economic and @koff2015expt models can *never* take a value of 0 because the asymptotes are derived from a span cast into logarithmic space. As such, even as prices approach infinity $\hat{y}$ will never advance beyond $A_{Lower}$, i.e. $\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}$. As such, fitted demand curve values can only take values between these two absolute limits, i.e. $\hat{y} \in [ A_{Lower}, A_{Upper} ]$.

Mention of the $A_{Lower}$ and $A_{Upper}$ limits highlight the relevance of the span parameter $\textit{k}$. Indeed, the @hursh2008economic framework uses a span parameter based on the range of *non-zero* consumption values because the range must exist in log units (i.e., why non-zero values were excluded). This has the effect of halting the demand curve at the lowest observed *non-zero* consumption value unless extended further by adding a constant (or fitting as a parameter). A visualization of the most basic behavior for the span parameter is provided in \autoref{fig:fig1CurveSpanEmpirical}.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes") +
  geom_point() +
  annotate("segment", 
           x      = 0.1, 
           xend   = 0.1, 
           y      = 100, 
           yend   = lowerAsymptote,
           colour = "red") +
  annotate("text", 
           x     = 0.175, 
           y     = 55, 
           angle = 90,
           adj   = 0.5,
           col   = "red",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = lowerAsymptote, 
           yend   = lowerAsymptote,
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "red",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = max(preFit$Q), 
           yend   = max(preFit$Q),
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "red",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw()

```

Although the determination of the span constant is straightforward in the @hursh2008economic implementation, the role of the span constant grew more abstract when 0 values were included in the regression on the linear scale. That is, various teams fit the span parameter as a constant because the observed data alone to not project the span below the lowest non-zero value. As such, the span constant is often increased (e.g., a small constant is added) in an attempt to *drive* the $A_{Lower}$ closer to 0. This behavior was originally noted in @gilroy2019exactsolve, whereby modifications to the span constant are be performed in instances when the *observed* range of non-zero consumption value in log-units does not reflect the full range of demand being modeled. For instance, the range of the demand curve in log units may differ from the observed range when demand is not evaluated at $Q_{FREE}$ (i.e., $Q_0$ > highest observed consumption). Alternatively, researchers using the @koff2015expt model may fit $\textit{k}$ as a parameter in order to drive $A_{Lower}$ to a quantity nearer to zero. A visualization of this behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

#kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))
#lowerAsymptote = 10^(log10(100) - kSet)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes (Varying Spans)") +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  annotate("text",
           x     = 5000,
           y     = 90,
           adj   = 1,
           col   = "red",
           label = paste("K (Empirical):", round(10^(log10(max(preFit$Q)) - kSet1),4))) +
  annotate("text",
           x     = 5000,
           y     = 80,
           adj   = 1,
           col   = "yellow",
           label = paste("K + 0.5:", round(10^(log10(max(preFit$Q)) - kSet2),4))) +
  annotate("text",
           x     = 5000,
           y     = 70,
           adj   = 1,
           col   = "green",
           label = paste("K + 1:", round(10^(log10(max(preFit$Q)) - kSet3),4))) +
  annotate("text",
           x     = 5000,
           y     = 60,
           adj   = 1,
           col   = "blue",
           label = paste("K + 2:", round(10^(log10(max(preFit$Q)) - kSet4),4))) +
  annotate("text",
           x     = 5000,
           y     = 50,
           adj   = 1,
           col   = "purple",
           label = paste("K + 3:", round(10^(log10(max(preFit$Q)) - kSet5),4))) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw() +
  theme(legend.position = "bottom")

```

As illustrated in this figure, increasing the span (i.e., distance between $A_{Lower}$ and $A_{Upper}$) has two principal effects on the demand curve. First, the rate of change in elasticity is reflected by parameter $\alpha$ in conjunction with the span constant. As such, the rate of change in the model is predicated on these upper and lower limits and this quantity represents the rate of change, as a function of $P$, by which $\hat{y}$ advances from $A_{Upper}$ to $A_{Lower}$. Second, and specific to the @koff2015expt model, a gap exists between $A_{Lower}$ and zero. This gap decreases proportionally with each unit increase in the span parameter $\textit{k}$. This behavior is illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}, whereby larger $\textit{k}$ values result in a $A_{Lower}$ that more closely approaches zero. However, again, this curve can never reach true zero in this approach.

## Same Model, Different Error

Although the varying implementations of the @hursh2008economic framework differ in how residual error is interpreted, both models function in the same manner. That is, the $A_{Upper}$ and $A_{Lower}$ exist and function in the same fashion because both the @hursh2008economic and @koff2015expt represent the span of the demand curve in log units. In cases where no zero-valued consumption is included in the regression, fits between the log and linear interpretations of the @hursh2008economic model provide similar estimates so long as error handling is comparable (i.e., both are relative). That is, re-weighting of the the errors in the linear restatement (i.e., relative to $\hat{y}$) yields fits and estimates that are nearly identical to that of the log form. Example fits are illustrated in \autoref{fig:fig3ComparisonNonzeroRelativeError}.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error"}

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * 100 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * 100 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * 100 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           weights = 1/(predict(EXPT)^2),
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  rename(`Exponential`   = explPred,
         `Exponentiated (Absolute Error)` = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred) 

# kable(preFit.trim) %>%
#   kable_styling(full_width = TRUE)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P) 

ggplot(preFit.trim, aes(P, Q, color = Model)) +
  #ggtitle("TODO: Add title") +
  geom_point(color = "black") +
  geom_line(aes(P, Prediction)) +
  facet_wrap(~Model, ncol = 3) +
  scale_y_log10(breaks = c(1, 10, 100),
                labels = c(1, 10, 100),
                limits = c(1, 100)) +
  theme_bw() +
  theme(legend.position = "bottom")

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * 100 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * 100 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * 100 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

```

Although the literature better characterizes the impact of relative and absolute error, few researchers have focused on $A_{Lower}$ and its role in models derived from the @hursh2008economic framework. This is a particularly complex topic in the application of the linear restatement because 0 consumption values are typically included in the regression despite an inability to characterize demand at 0 levels. The implementation put forward in @koff2015expt minimizes the impact of zeroes on the log scale by capitalizing on the relatively small difference between $A_{Lower}$ and 0 on the linear scale when $\textit{k}$ values exceed that of the observed range. Pragmatically, most care little for such a small amount of error, this raises an interesting argument regarding the management of 0 values. That is, what is being considered *close enough* when accomodating 0 consumption values in demand curve analyses.

For the sake of argument, let us say that the model introduced in @koff2015expt considers values at $A_{Lower}$ to be *close enough* to 0 to reasonably approximate consumption at such levels. That is, $A_{Lower}$ is considered sufficiently approximate to 0. Following this logic (i.e., this degree of difference is negligible), it stands to reason that treating 0 values as $A_{Lower}$ is appropriate because 1) it allows demand to be projected beyond observed non-zero levels and 2) it is close enough to 0 that absolute differences are minimal. As such, it stands to reason that one could simply replace the 0 values with $A_{Lower}$, a quantity easily derived from $Q_0$ and $\textit{k}$. 

Reconsidering the full data displayed \autoref{fig:fig2CurveSpanEmpiricalMod}, only the @koff2015expt model would proceed under these conditions because of how 0 values interact with log transformation. However, as noted above, 0 values can be replaced by an individually-set $A_{Lower}$ value and both the @koff2015expt and @hursh2008economic models converge quite closely in this case, see \autoref{fig:fig4ComparisonZeroRelative}. Furthermore, re-weighting the @hursh2008economic to absolute difference error produces an even closer approximation between the two that renders almost identical, see \autoref{fig:fig4ComparisonZeroRelative} and see \autoref{tab:table1ComparisonZeroAbsolute}.

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes"}

ggplot(preFit.g, aes(P, Q, color = Model)) +
  geom_point() +
  geom_line(aes(P, Prediction)) +
  scale_x_log10(breaks = c(0.1, 10, 100, 1000, 10000),
                labels = c(0.1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  scale_y_log10(breaks = c(0.001, 0.01, 0.1, 1, 10, 100),
                labels = c(0.001, 0.01, 0.1, 1, 10, 100),
                limits = c(0.001, 100)) +
  facet_wrap(~Model, ncol = 3) +
  theme_bw() +
  theme(legend.position = "bottom")

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

The hypothetical data series modeled in \autoref{fig:fig4ComparisonZeroRelative} demonstrate highly similar fits when controlling for $A_{Lower}$ (i.e., a common lower bound). That is, the approach put forward in @koff2015expt uses $A_{Lower}$ as a *close enough* proxy to zero and functionally treats 0 values as $A_{Lower}$. When adapting this same logic to the @hursh2008economic implementation, this results in nearly the same fit. As indicated in \autoref{tab:table1ComparisonZeroAbsolute}, the projections between each of these implementations becomes even closer when the @hursh2008economic implementation is amended to treat differences in error as absolute.

## Different Model, Same Limitations

 



The considerable influence of near-zero values and relative error are described well in @koff2015expt. As noted in that work, the replacement of zero values with *arbitrary* constants led to wildly varying effects on fits using the @hursh2008economic framework. Indeed, most agree that constructing a common $A_{Lower}$ (e.g., for all demand series) with a small constant (e.g., 0.1, 0.01) is unfavorable. The authors highlight issues with applying an *arbitrary* constant to zero-valued consumption and used this logic to make a case for a restatement of the @hursh2008economic model in linear terms. The desire to include all data, zero-valued or not, is admirable and statistically desirable but two issues warrant noting in this approach. 

First, the evaluation of *arbitrary* constants is fundamentally at odds with how individual demand curves and span constants interact to determine the demand curve, see \autoref{fig:fig1CurveSpanEmpirical}. That is, $Q_{0}$ forms the basis for $A_{Upper}$ and the arbitrary constant (in log scale) differ from $A_{Lower}$ by several magnitudes. This is because $A_{Upper}$ and $A_{Lower}$ are jointly determined by individual-level parameters and the global span constant $k$. As such, replacing *all* zero values with a common constant simply does not provide a reasonable approximation of $A_{Upper}$.

Second, neither the @hursh2008economic nor the @koff2015expt models characterize demand at zero. Although the @hursh2008economic fails for obvious reasons, the limitations of the @koff2015expt approach are less self-evident. Specifically, regression in the @koff2015expt approach succeeds in the case of 0 value consumption because 0 values *are treated as* $A_{Lower}$, which is a bound determined by individual-level $Q_{0}$ values and the span parameter $k$. As such, this strategy succeeds because 0 values are approximated by individual-level $A_{Upper}$ bounds and because the absolute difference $A_{Upper}$ decreases by an order of magnitude for each unit increase in parameter $k$. As such, the @koff2015expt approach is essentially equivalent to using the @hursh2008economic with individually-derived constants and normally distributed error in the linear scale. However, this claim is not trivial and simulation and analysis is warranted to whether the two models differ in an appreciable manner.

# Methods

## Data Generating Process

A total of 500 hypothetical data series were simulated using software submitted to peer-review. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to generate demand series data. The seed values and variance were consistent with those used in the @koff2015expt publication as the basis for this approach over the earlier @hursh2008economic model.

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all hypothetical series. Specifically, individual series were screened for 1) bounce (i.e., local changes within an expected downward trend), 2) trend (i.e., molar changes within an expected downward trend), and reversals from zero (i.e., return of consumption at a higher price from 0 consumption at a lower price). Data were included in this methodological comparison so long as they met all three indicators of systematic demand data.

## Modeling Strategies

Three modeling strategies were evaluated in this simulation study. The core aim of these comparisons was to evaluate how both replacing 0 values with an individually-generated $A_{Lower}$ constant and altering the weighting of error influenced the correspondence between the @hursh2008economic and @koff2015expt models. The individual modeling strategies are listed below:

### Strategy 1 : @koff2015expt

The @koff2015expt model was applied to eligible data series at the individual level. This model was fit using the *optim* package in the R Statistical Program [@R-base]. Initial starts were derived based on the respective data and parameters $Q_0$ and $\alpha$ were carried forward into the final comparisons. The span constant $k$ was estimated based on the empirical range of the data with an added constant (0.5) to extend slightly below the lowest non-zero point of consumption. The same span constant was used across all models.

### Strategy 2: @hursh2008economic Model (Relative Error)

The @hursh2008economic was adjusted to dynamically determine the $A_{Lower}$ bound during the optimization procedure. Specifically, a customized loss function was prepared for use with the *optim* method whereby all values of 0 were replaced by $A_{Lower}$. The source code necessary to reproduce this approach and calculation has been provided for public review by the author at [https://github.com/miyamot0/AgnosticDemandModeling](https://github.com/miyamot0/AgnosticDemandModeling). Respective parameters were derived consistent with that of the @koff2015expt model.

### Strategy 3 : @hursh2008economic Model (Absolute Error)

The @hursh2008economic was adjusted further to dynamically determine the $A_{Lower}$ as a replacement for zero and also represented error (i.e., loss) in the linear scale. That is, residual error was reflected by ($10^{\hat{y}} - 10^{y}$). This model and resulting parameters were handled in the same manner as the others.

## Analytical Strategy

Given that both the @hursh2008economic and @koff2015expt models emerge from the same framework...

TODO: Equivalence test

# Results

```{r fig4, fig.cap="Comparisons of Modeling Strategies", fig.height=9}

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 100

sim <- SimulateDemand(nruns = nParticipants, setparams = setparams, sdindex = sdindex, x = x)

dataSet = sim$simvalues

passingFrame = beezdemand::CheckUnsystematic(dataSet)

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 0.5

dataFramePrep = data.frame(
  id      = 1:nParticipants,
  q0.HS   = numeric(length = nParticipants),
  q0.HSw  = numeric(length = nParticipants),
  q0.Koff = numeric(length = nParticipants),
  a.HS    = numeric(length = nParticipants),
  a.HSw   = numeric(length = nParticipants),
  a.Koff  = numeric(length = nParticipants),
  K       = numeric(length = nParticipants)
)

###

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

###

minQ0 = 0.01
maxQ0 = 200

for (id in 1:nParticipants) {

  currentData = dataSet[dataSet$id == id, ]

  dataFramePrep[id, "K"] = kSet

  fit.Koff <- optim(par  = c(max(currentData$y), -3),
                    fn   = min.RSS.EXPT,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data = currentData)

  dataFramePrep[id, c("q0.Koff", "a.Koff")] = fit.Koff$par

  fit.HS   <- optim(par    = c(fit.Koff$par[1], fit.Koff$par[2]),
                    fn     = min.RSS.EXPL,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data   = currentData)

  dataFramePrep[id, c("q0.HS", "a.HS")] = fit.HS$par

  fit.HWa <- optim(par    = c(fit.HS$par[1], fit.HS$par[2]),
                   fn     = min.RSS.EXPL.aw,
                   method = "L-BFGS-B",
                   lower  = c(0.01, -5),
                   upper  = c(max(currentData$y) * 1.25, 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw", "a.HSw")] = fit.HWa$par
}

keepers = passingFrame %>%
  filter(TotalPass == 3) %>%
  select(id) %>%
  pull()

frameToAnalyze = dataFramePrep %>%
  filter(id %in% keepers)

par(mfrow = c(2, 2),
    oma = c(0, 0, 0, 0))

comps.base <- lm(q0.Koff ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HS,
     main = expression(A[LOWER]~ "with Relative Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.base)

comps.weight <- lm(q0.Koff ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main = expression(A[LOWER]~"with Absolute Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.weight)

comps.base <- lm(a.Koff ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HS,
     main = expression(A[LOWER]~"with Relative Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.base)

comps.weight <- lm(a.Koff ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main = expression(A[LOWER]~ "with Absolute Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.weight)


```

The simulated data revealed a total of `r nrow(frameToAnalyze)` that met all criteria for systematic hypothetical purchase task data (from *N*=100). Scatterplot comparisons across each of the three modeling strategies are illustrated in \autoref{fig:fig4}. As illustrated in this figure, each of the three strategies differed with respect to their correspondence. 

## Strategy 1 vs. Strategy 2

Comparisons between Strategy 1 and 2 revealed a significant correlation for $Q_0$ (r = 0.8955, t = 17.313, df = 74, p-value < 2.2e-16). Similarly, there was a significant correlation for $\alpha$ (r = 0.9344, t = 22.515, df = 74, p-value < 2.2e-16). Scatterplot visualization of these two strategie indicated that, overall, the two approaches generally corresponded to a non-trivial degree.

## Strategy 1 vs. Strategy 3

Comparisons between Strategy 2 and 3 revealed a perfect correlation for $Q_0$ (r = 1, t = 82678, df = 74, p-value < 2.2e-16). Similarly, there was also a near perfect correlation for $\alpha$ (r = 0.9999, t = 22.515, df = 74, p-value < 2.2e-16). Scatterplot visualization of these two strategie indicated that, overall, the two approaches corresponded near perfectly.

# Discussion

The purpose of this technical report was to further explore the role of zero and non-zero asymptotes in two popular models of operant demand. Indeed, debate as to whether 0 consumption values contribute to the characterization of demand continue to date. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
