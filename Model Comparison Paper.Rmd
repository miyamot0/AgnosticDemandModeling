---
title             : "The Exponential Model's New Clothes"
shorttitle        : "Exponential Model"

author: 
  - name          : "Shawn Gilroy"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "226 Audubon Hall Baton Rouge, Louisiana 70806"
    email         : "sgilroy1@lsu.edu"
    role:         
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Louisiana State University"

authornote: |
  Shawn Gilroy. Louisiana State University. Correspondence should be directed to sgilroy1@lsu.edu

abstract: |
  Operant translations of Behavioral Economic concepts and principles have successfully enhanced the ability to predict and characterize individual choice. Specifically, Behavioral Economic models of choices (e.g., demand) have been particularly useful in evaluating how decision-makers arrives at choices under constraint (e.g., limited price, resources). With respect to operant demand, the prevailing approach in the Behavior Analytic literature draws from the framework of Hursh & Silberburg (2008). Although few would argue the merits of this framework, debate continues with regard to how to address limitations of the logarithmic scale. Specifically, there are opposing opinions regarding the handling of zero values and which restatement of Hursh & Silberburg (2008) fares best under certain situations. The purpose of this technical report is to review the limited ways in which the original and exponentiated forms differ and how slight manipulations in the application of each lead to equivalent results.

keywords          : "behavioral economics, operant demand, consumption, zero asymptotes"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa7"
csl               : "apa.csl"
classoption       : "man"
output            : papaja::apa6_pdf
---

\raggedbottom

```{r include = FALSE}

library(beezdemand)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(knitr)
library(papaja)
library(tidyverse)

r_refs("r-references.bib")
```

```{r }
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

Contemporary methods for evaluating the consumption of goods and services in an operant framework draw heavily from the methodology proposed in @hursh2008economic. This evolving strategy and methodology has taken various forms [@hursh1987linear] and the latest iterations of this framework use either a two- [@gilroy2021zbe] or three-parameter variant [@gilroy2021zbe, @koff2015expt] of the contemporary framework.

## Zero and Non-Zero Asymptotes

Until most recently, all derivatives of the @hursh2008economic framework modeled demand non-zero upper and lower asymptotes. That is, these models were bounded at an upper ($Q_0$) and lower asymptote in a roughly S-type form. The lower bound in the original @hursh2008economic model represent the lower limit of observed non-zero consumption. The lower asymptote took a non-zero value because the log transformations of consumption produced undefined values.

@koff2015expt provided a restatement of the @hursh2008economic framework with consumption evaluated in the linear scale. That is, exponentiation of the order of consumption was performed prevent the log transformation of observed values (which could be take a quantity of zero). The span of the demand curve, along with its associated rate of exponential decay, reflects changes in log scale before such projections are returned to the linear scale (the order of non-transformed consumption). The draw of this approach is that some of the issues with the log scale are avoided (i.e., consumption values observed at zero). However, two issues remain: error representation and the non-zero lower asymptote.

## Error Representation

Issues with various methods of error representation are reviewed in @gilroy2021zbe. Briefly, changes in the log and linear scale are reflected different. That is, changes in log space are relative while changes in the linear space are absolute. Relative error is typically more suited to evaluating changes across varying in an equal fashion (e.g., $\hat{y} = 1000$, $\hat{y} = 10$, $\hat{y} = 0.1$). In this way, relative difference is emphasized and this feature specifies how each piece of information is weighted in the model.

Absolute differences are much more straightforward and are not made relative. That is, error is difference from prediction regardless of scale. @gilroy2021zbe discussed this difference in the context of evaluating instances of demand that vary across orders. Specifically, error may become incomparable when fitting models to data that vary wildly across orders. For instance, data observed at high values (i.e., low prices) would theoretically have greater influences than those observed at low values (i.e, high prices) because differences close to zero have negligible influence in the absolute sense. This has led to occasionally inconsistencies between model fits between the log and linear forms of @hursh2008economic framework.

## Zero Consumption and Lower Asymptotes

Recent discussions on the inclusion of zero consumption values in operant demand have highlight issues with the lower asymptote in contemporary models [@gilroy2021zbe]. Specifically, both of the models described in @hursh2008economic and @koff2015expt cannot explicitly characterize demand at zero levels. Rather, each of these models has an upper asymptote defined by $Q_{0}$ and a lower asymptote defined by $Q_{0}$ and the span constant, $\textit{k}$. Regardless of the model, the asymptotes are defined as the following:

\begin{equation}
\begin{split} 
A_{Upper} &= 10^{log_{10}Q_0 }  \\
A_{Lower} &= 10^{log_{10}Q_0 - k}
\end{split} 
\end{equation}

Given that these limits are defined on the log scale, it will not be possible to explicitly model zero and the lowest possible $\hat{y}$ will always at the non-zero lower asymptote. See the example notation below:

\begin{equation} 
\displaystyle{\lim_{P \to \infty} f(x)= A_{Lower}}
\end{equation} 

In this framework, which applies equally to both the log and linear restatement of the @gilroy2021zbe, the predicted values can only fall within the interval between upper and lower limits, see below:

\begin{equation} 
\hat{y}\in [{A_{Lower}},{A_{Upper}}] 
\end{equation} 

The span parameter $\textit{k}$ in the original implementation was derived from range of non-zero consumption values. As such, the $A_{Upper}$ and $A_{Lower}$ limits reflect the observed, non-zero consumption at upper and lower extremes, respectively. This was necessarily the case in the original implementation of the @hursh2008economic framework because non-zero values were dropped from the analysis. This has the effect of halting the demand curve at the lowest observed non-zero consumption value. A visualization of this behavior is provided in \autoref{fig:fig1CurveSpanEmpirical}.

```{r fig1CurveSpanEmpirical, fig.cap="Span of Demand Curve with Non-zero Consumption", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))

lowerAsymptote = 10^(log10(100) - kSet)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes") +
  geom_point() +
  annotate("segment", 
           x      = 0.1, 
           xend   = 0.1, 
           y      = 100, 
           yend   = lowerAsymptote,
           colour = "red") +
  annotate("text", 
           x     = 0.175, 
           y     = 55, 
           angle = 90,
           adj   = 0.5,
           col   = "red",
           label = "Range of Non-zero Observed Consumption\r\nFrom Upper to Lower Asymptotes") +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = lowerAsymptote, 
           yend   = lowerAsymptote,
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 10,
           adj   = 0.5,
           col   = "red",
           parse = TRUE,
           label = paste("'Non-Zero Lower Bound at: ' * 10^{log[10]*Q[0]-k} * ' : ' *",
                         round(lowerAsymptote,2))) +
  annotate("segment", 
           x      = 0.1, 
           xend   = 5000, 
           y      = max(preFit$Q), 
           yend   = max(preFit$Q),
           colour = "red",
           lty    = 2) +
  annotate("text",
           x     = 25,
           y     = 95,
           adj   = 0.5,
           col   = "red",
           label = paste("Non-Zero Upper Bound at Q0 : ",
                         round(max(preFit$Q),2))) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw()

```

Although this behavior is straightforward in the initial implementation, the span of the demand curve became a more complex issue when observed consumption was restated in the linear scale. That is, the span constant can be enlarged in an attempt to *approach* an $A_{Lower}$ limit slightly closer to 0. Originally noted in @gilroy2019exactsolve, manipulations to the span constant can be performed in instances when the observed range of non-zero consumption value in log-units does not reflect the range of consumption values regressed upon. For instance, such is the case when demand is not evaluated at $Q_{FREE}$ or when non-zero consumption values are included in the regression. In these circumstances, $A_{Upper}$ and $A_{Lower}$ are not well characterized by the upper and lower extremes of the non-zero consumption data. Focusing on the latter issue, extending the span to more closely approach 0, it is common practice to include a fixed constant. These increasingly large values increase the span of the demand curve in log units and their varying degrees of influence are illustrated in \autoref{fig:fig2CurveSpanEmpiricalMod}.

```{r fig2CurveSpanEmpiricalMod, fig.cap="Span of Empirical Demand Curve with Vary Spans", warning=FALSE}

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

#kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))
#lowerAsymptote = 10^(log10(100) - kSet)

kSet1 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 0
kSet2 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 0.5
kSet3 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 1
kSet4 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 2
kSet5 = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q>0]))) + 3

asymptoteFrame = data.frame(
  Y = c(rep(10^(log10(max(preFit$Q)) - kSet1), 2),
        rep(10^(log10(max(preFit$Q)) - kSet2), 2),
        rep(10^(log10(max(preFit$Q)) - kSet3), 2),
        rep(10^(log10(max(preFit$Q)) - kSet4), 2),
        rep(10^(log10(max(preFit$Q)) - kSet5), 2)),
  X = c(rep(c(0.1, 5000), 5)),
  K = c(rep("K (Empirical)", 2),
        rep("K + 0.5", 2),
        rep("K + 1", 2),
        rep("K + 2", 2),
        rep("K + 3", 2))
)

ggplot(preFit, aes(P, Q)) +
  #ggtitle("Demand Data with Zeroes (Varying Spans)") +
  geom_point() +
  geom_line(data = asymptoteFrame,
            aes(X, Y, color = K),
            inherit.aes = FALSE) +
  annotate("text",
           x     = 5000,
           y     = 90,
           adj   = 1,
           col   = "red",
           label = paste("K (Empirical):", round(10^(log10(max(preFit$Q)) - kSet1),4))) +
  annotate("text",
           x     = 5000,
           y     = 80,
           adj   = 1,
           col   = "yellow",
           label = paste("K + 0.5:", round(10^(log10(max(preFit$Q)) - kSet2),4))) +
  annotate("text",
           x     = 5000,
           y     = 70,
           adj   = 1,
           col   = "green",
           label = paste("K + 1:", round(10^(log10(max(preFit$Q)) - kSet3),4))) +
  annotate("text",
           x     = 5000,
           y     = 60,
           adj   = 1,
           col   = "blue",
           label = paste("K + 2:", round(10^(log10(max(preFit$Q)) - kSet4),4))) +
  annotate("text",
           x     = 5000,
           y     = 50,
           adj   = 1,
           col   = "purple",
           label = paste("K + 3:", round(10^(log10(max(preFit$Q)) - kSet5),4))) +
  scale_x_log10(breaks = preFit$P,
                labels = preFit$P) +
  theme_bw() +
  theme(legend.position = "bottom")

```

The increasing of the span has two primary effects. First, the rate of change in elasticity is jointly reflected by parameter $\alpha$ and the span constant. As such, the rate of change in the model is predicated on the upper and lower limits. That is, the span constant reflects the rate of change, as a function of $P$, as $\hat{y}$ advances from $A_{Upper}$ to $A_{Lower}$. Second, the gap between $A_{Lower}$ and true zero decreases proportionally with a larger $\textit{k}$. As shown in \autoref{fig:fig2CurveSpanEmpiricalMod}, larger $\textit{k}$ more closely approach zero but $A_{Lower}$ can never be true zero in this approach.

## Same Model, Different Error

As noted in earlier works [@gilroy2021zbe, @koff2015expt], the various implementations of the @hursh2008economic framework differ primarily in how residual error is interpreted when optimizing model parameters. For all intents and purposes, both models function in the same manner. That is, the $A_{Upper}$ and $A_{Lower}$ exist in the same fashion because both the @hursh2008economic and @koff2015expt represent the span of the demand curve in log units. In cases where no zero-valued consumption is included in the regression, fits between the log and linear interpretations of the @hursh2008economic model provide similar estimates. Further, re-weighting of the the errors in the linear restatement (i.e., relative to $\hat{y}$) yields fits and estimates that are essentially identical to that of the log form. These fits are illustrated in an example data set in \autoref{fig:fig3ComparisonNonzeroRelativeError}.

```{r fig3ComparisonNonzeroRelativeError, fig.cap="Comparable Model Fits with Comparable Error"}

preFit.trim = preFit[preFit$Q > 0, ]

kSet = log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))

### HS2008

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * 100 * P)-1),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$explPred = 10^predict(EXPL)

### Koff2015

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * 100 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptPred = predict(EXPT)

### Koff2015 Relative Error

EXPTrel = nls(Q ~ Q0 * 10^(kSet*(exp(-a * 100 * P)-1)),
           data = preFit.trim,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           weights = 1/(predict(EXPT)^2),
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit.trim$exptRelPred = predict(EXPTrel)

preFit.trim = preFit.trim %>%
  rename(`Exponential`   = explPred,
         `Exponentiated (Absolute Error)` = exptPred,
         `Exponentiated (Relative Error)` = exptRelPred) 

# kable(preFit.trim) %>%
#   kable_styling(full_width = TRUE)

preFit.trim = preFit.trim %>%
  gather(Model, Prediction, -Q, -P) 

ggplot(preFit.trim, aes(P, Q, color = Model)) +
  #ggtitle("TODO: Add title") +
  geom_point(color = "black") +
  geom_line(aes(P, Prediction)) +
  facet_wrap(~Model, ncol = 3) +
  scale_y_log10(breaks = c(1, 10, 100),
                labels = c(1, 10, 100),
                limits = c(1, 100)) +
  theme_bw() +
  theme(legend.position = "bottom")

```

```{r }

preFit <- data.frame(
  P = c(0.1, 1,  2,  5,  10, 50, 100, 500, 1000, 5000),
  Q = c(99.54077086,
        95.52117752,
        91.28456421,
        79.88382452,
        64.51709973,
        16.3328254,
        5.442004322,
        0,
        0,
        0)
)

kSet = (log10(max(preFit$Q)) - log10(min(preFit$Q[preFit$Q > 0]))) + 3

newAsymptote = 10^(log10(max(preFit$Q)) - kSet)

EXPT = nls(Q ~ Q0 * 10^(kSet*(exp(-a * 100 * P)-1)),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

preFit[preFit$Q == 0, "Q"] = newAsymptote

preFit$exptPred = predict(EXPT)

EXPL = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * 100 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

prePreds = predict(EXPL)

preFit$explPred = 10^prePreds

#print(prePreds)

EXPL2 = nls(log(Q)/log(10) ~ log(Q0)/log(10) + kSet*(exp(-a * 100 * P)-1),
           data = preFit,
           start = c(a  = 0.0001,
                     Q0 = 100),
           lower = c(a = -Inf,
                     Q0 = 1),
           upper = c(a = Inf,
                     Q0 = 100),
           weights = (10^prePreds)^2,
           algorithm = "port",
           control = list(
             maxiter = 1000,
             warnOnly = TRUE
           ))

postPreds = predict(EXPL2)

#preFit$explPred = 10^prePreds

preFit$explPred2 = 10^postPreds

displayFrame = data.frame(
    P                          = preFit$P,
    Q                          = preFit$Q,
    `Exponential-Relative`     = preFit$explPred,
    `Exponential-Absolute`     = preFit$explPred2,
    `Exponentiated`            = preFit$exptPred
  )

preFit = preFit %>%
  rename(`Exponential (Relative)`   = explPred,
         `Exponential (Absolute)`   = explPred2,
         `Exponentiated`            = exptPred) 

# kable(preFit) %>%
#   kable_styling(full_width = TRUE)

preFit.g = preFit %>%
  gather(Model, Prediction, -Q, -P) %>%
  mutate(Model = factor(Model,
                        levels = c(
                          "Exponentiated",
                          "Exponential (Relative)",
                          "Exponential (Absolute)"
                        )))

```

Although the field is now more aware to the differences between relative vs. absolute error, relatively few researchers have discussed the importance of $A_{Lower}$ in the @hursh2008economic framework. This is particularly complex in the linear restatement, as zero valued consumption is often included in the regression despite an inability to characterize demand at zero. Rather, the implementation put forward in @koff2015expt minimizes the impact of zeroes on the log scale by capitalizing on the relatively small difference between $A_{Lower}$ and 0 on the linear scale. Pragmatically, most researchers care little for such a small amount of error, this raises an interesting argument of what is being considered "close enough" when handling 0 consumption values. 

For the sake of argument, let us say that the restatement of the @hursh2008economic framework put forward in @koff2015expt considers values at $A_{Lower}$ to be "close enough" to 0 to reasonably resemble consumption at such levels. That is, $A_{Lower}$ is consider approximate to 0 in this instance. Given that $A_{Lower}$ is considered close enough to zero (i.e., the difference is negligible), it should be no matter of significance to treat the observed 0 values as $A_{Lower}$. The result of the exponentiation of the terms of the @hursh2008economic model allows the regression to proceed (foregoing problematic log transforms) by treating consumption values at high prices (i.e., likely at or near zero) as if they took the value of $A_{Lower}$.

Consider the data in \autoref{fig:fig2CurveSpanEmpiricalMod}. In entirety, only the @koff2015expt would proceed under these conditions because of the problematic log transformation. However, holding the span parameter $k$ constant and replacing 0 values with $A_{Lower}$, the projections between @koff2015expt and the @hursh2008economic converge quite closely, see \autoref{fig:fig4ComparisonZeroRelative}. Even further, re-weighting the @hursh2008economic to absolute difference error produces an even closer approximation between the two, see \autoref{fig:fig4ComparisonZeroRelative} and see \autoref{tab:table1ComparisonZeroAbsolute}.

```{r fig4ComparisonZeroRelative, warning=FALSE, fig.cap="Comparable Model Fits with Comparable Asymptotes"}

ggplot(preFit.g, aes(P, Q, color = Model)) +
  geom_point() +
  geom_line(aes(P, Prediction)) +
  scale_x_log10(breaks = c(0.1, 10, 100, 1000, 10000),
                labels = c(0.1, 10, 100, 1000, 10000),
                limits = c(0.1, 10000)) +
  scale_y_log10(breaks = c(0.001, 0.01, 0.1, 1, 10, 100),
                labels = c(0.001, 0.01, 0.1, 1, 10, 100),
                limits = c(0.001, 100)) +
  facet_wrap(~Model, ncol = 3) +
  theme_bw() +
  theme(legend.position = "bottom")

```

```{r table1ComparisonZeroAbsolute}

displayFrame %>% 
  apa_table(., 
            digits  = 3,
            caption = "Comparison of Exponentiated and Absolute-Weighted Exponential Models")

```

In the hypothetical data series modeled in \autoref{fig:fig4ComparisonZeroRelative}, fits with the original (relative and absolute error representations) and exponentiated forms of the @hursh2008economic framework provide highly similar fits when controlling for the $A_{Lower}$. That is, the approach put forward in @koff2015expt uses $A_{Lower}$ as a "close enough" proxy to zero and simply replacing 0 with $A_{Lower}$ in the @hursh2008economic implementation provides essentially the same fit. As indicated in \autoref{tab:table1ComparisonZeroAbsolute}, the projections between each of these implementations becomes even closer when the @hursh2008economic implementation is amended to treat differences in error as absolute. This is particularly relevant as when low values that differ slightly in absolute value differ greatly in relative value.

The considerable influence of near-zero values and relative error are described well in @koff2015expt. As noted in that work, the replacement of zero values with *arbitrary* constants led to wildly varying effects on fits using the @hursh2008economic framework. Indeed, most agree that constructing a common $A_{Lower}$ (e.g., for all demand series) with a small constant (e.g., 0.1, 0.01) is unfavorable. The authors highlight issues with applying an *arbitrary* constant to zero-valued consumption and used this logic to make a case for a restatement of the @hursh2008economic model in linear terms. The desire to include all data, zero-valued or not, is admirable and statistically desirable but two issues warrant noting in this approach. 

First, the evaluation of *arbitrary* constants is fundamentally at odds with how individual demand curves and span constants interact to determine the demand curve, see \autoref{fig:fig1CurveSpanEmpirical}. That is, $Q_{0}$ forms the basis for $A_{Upper}$ and the arbitrary constant (in log scale) differ from $A_{Lower}$ by several magnitudes. This is because $A_{Upper}$ and $A_{Lower}$ are jointly determined by individual-level parameters and the global span constant $k$. As such, replacing *all* zero values with a common constant simply does not provide a reasonable approximation of $A_{Upper}$.

Second, neither the @hursh2008economic nor the @koff2015expt models characterize demand at zero. Although the @hursh2008economic fails for obvious reasons, the limitations of the @koff2015expt approach are less self-evident. Specifically, regression in the @koff2015expt approach succeeds in the case of 0 value consumption because 0 values *are treated as* $A_{Lower}$, which is a bound determined by individual-level $Q_{0}$ values and the span parameter $k$. As such, this strategy succeeds because 0 values are approximated by individual-level $A_{Upper}$ bounds and because the absolute difference $A_{Upper}$ decreases by an order of magnitude for each unit increase in parameter $k$. As such, the @koff2015expt approach is essentially equivalent to using the @hursh2008economic with individually-derived constants and normally distributed error in the linear scale. However, this claim is not trivial and simulation and analysis is warranted to whether the two models differ in an appreciable manner.

# Methods

## Data Generating Process

A total of 500 hypothetical data series were simulated using software submitted to peer-review. Specifically, the *SimulateDemand* method included in the *beezdemand* R package [@kaplan2019r] was used to generate demand series data. The seed values and variance were consistent with those used in the @koff2015expt publication as the basis for this approach over the earlier @hursh2008economic model.

## Screening of Non-systematic Data Series

The three criteria for systematic data outlined in @stein2015identification were applied to all hypothetical series. Specifically, individual series were screened for 1) bounce (i.e., local changes within an expected downward trend), 2) trend (i.e., molar changes within an expected downward trend), and reversals from zero (i.e., return of consumption at a higher price from 0 consumption at a lower price). Data were included in this methodological comparison so long as they met all three indicators of systematic demand data.

## Modeling Strategies

Three modeling strategies were evaluated in this simulation study. The core aim of these comparisons was to evaluate how both replacing 0 values with an individually-generated $A_{Lower}$ constant and altering the weighting of error influenced the correspondence between the @hursh2008economic and @koff2015expt models. The individual modeling strategies are listed below:

### Strategy 1 : @koff2015expt

The @koff2015expt model was applied to eligible data series at the individual level. This model was fit using the *optim* package in the R Statistical Program [@R-base]. Initial starts were derived based on the respective data and parameters $Q_0$ and $\alpha$ were carried forward into the final comparisons. The span constant $k$ was estimated based on the empirical range of the data with an added constant (0.5) to extend slightly below the lowest non-zero point of consumption. The same span constant was used across all models.

### Strategy 2: @hursh2008economic Model (Relative Error)

The @hursh2008economic was adjusted to dynamically determine the $A_{Lower}$ bound during the optimization procedure. Specifically, a customized loss function was prepared for use with the *optim* method whereby all values of 0 were replaced by $A_{Lower}$. The source code necessary to reproduce this approach and calculation has been provided for public review by the author at [https://github.com/miyamot0/AgnosticDemandModeling](https://github.com/miyamot0/AgnosticDemandModeling). Respective parameters were derived consistent with that of the @koff2015expt model.

### Strategy 3 : @hursh2008economic Model (Absolute Error)

The @hursh2008economic was adjusted further to dynamically determine the $A_{Lower}$ as a replacement for zero and also represented error (i.e., loss) in the linear scale. That is, residual error was reflected by ($10^{\hat{y}} - 10^{y}$). This model and resulting parameters were handled in the same manner as the others.

## Analytical Strategy

Given that both the @hursh2008economic and @koff2015expt models emerge from the same framework...

TODO: Equivalence test

# Results

```{r fig4, fig.cap="Comparisons of Modeling Strategies", fig.height=9}

setparams <- vector(length = 4)
setparams <- c(-2.5547, .702521, 1.239893, .320221, 3.096, 1.438231)
names(setparams) <- c("alphalm", "alphalsd", "q0lm", "q0lsd", "k", "yvalssd")
sdindex <- c(2.1978, 1.9243, 1.5804, 1.2465, 0.8104, 0.1751, 0.0380, 0.0270)
x <- c(.1, 1, 3, 10, 30, 100, 300, 1000)

set.seed(65535)

nParticipants = 100

sim <- SimulateDemand(nruns = nParticipants, setparams = setparams, sdindex = sdindex, x = x)

dataSet = sim$simvalues

passingFrame = beezdemand::CheckUnsystematic(dataSet)

kSet = log10(max(dataSet$y)) - log10(min(dataSet$y[dataSet$y > 0])) + 0.5

dataFramePrep = data.frame(
  id      = 1:nParticipants,
  q0.HS   = numeric(length = nParticipants),
  q0.HSw  = numeric(length = nParticipants),
  q0.Koff = numeric(length = nParticipants),
  a.HS    = numeric(length = nParticipants),
  a.HSw   = numeric(length = nParticipants),
  a.Koff  = numeric(length = nParticipants),
  K       = numeric(length = nParticipants)
)

###

getEXPL <- function(Q, K, A, P) {
  log(Q)/log(10) + K * (exp(-A * Q * P) - 1)
}

getEXPT <- function(Q, K, A, P) {
  Q * 10^(K*(exp(-A * Q * P) - 1))
}

min.RSS.EXPT <- function(data, par) {
  with(data, sum((getEXPT(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote
  newData$y <- log10(newData$y)

  with(newData, sum((getEXPL(par[1], kSet, 10^par[2], x) - y)^2))
}

min.RSS.EXPL.aw <- function(data, par) {
  newData = data
  newAsymptote = 10^(log10(par[1]) - kSet)
  newData[newData$y == 0, "y"] <- newAsymptote

  newData$y <- log10(newData$y)
  newData$ys   = getEXPL(par[1], kSet, 10^par[2], newData$x)
  newData$err  = (10^newData$ys - 10^newData$y)^2

  sum(newData$err)
}

###

minQ0 = 0.01
maxQ0 = 200

for (id in 1:nParticipants) {

  currentData = dataSet[dataSet$id == id, ]

  dataFramePrep[id, "K"] = kSet

  fit.Koff <- optim(par  = c(max(currentData$y), -3),
                    fn   = min.RSS.EXPT,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data = currentData)

  dataFramePrep[id, c("q0.Koff", "a.Koff")] = fit.Koff$par

  fit.HS   <- optim(par    = c(fit.Koff$par[1], fit.Koff$par[2]),
                    fn     = min.RSS.EXPL,
                    method = "L-BFGS-B",
                    lower  = c(0.01, -5),
                    upper  = c(max(currentData$y) * 1.25, 0),
                    data   = currentData)

  dataFramePrep[id, c("q0.HS", "a.HS")] = fit.HS$par

  fit.HWa <- optim(par    = c(fit.HS$par[1], fit.HS$par[2]),
                   fn     = min.RSS.EXPL.aw,
                   method = "L-BFGS-B",
                   lower  = c(0.01, -5),
                   upper  = c(max(currentData$y) * 1.25, 0),
                   data   = currentData)

  dataFramePrep[id, c("q0.HSw", "a.HSw")] = fit.HWa$par
}

keepers = passingFrame %>%
  filter(TotalPass == 3) %>%
  select(id) %>%
  pull()

frameToAnalyze = dataFramePrep %>%
  filter(id %in% keepers)

par(mfrow = c(2, 2),
    oma = c(0, 0, 0, 0))

comps.base <- lm(q0.Koff ~ q0.HS, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HS,
     main = expression(A[LOWER]~ "with Relative Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.base)

comps.weight <- lm(q0.Koff ~ q0.HSw, data = frameToAnalyze)
plot(frameToAnalyze$q0.Koff ~ frameToAnalyze$q0.HSw,
     main = expression(A[LOWER]~"with Absolute Error"),
     ylab = expression(Q[0] ~ "(Koffarnus et al., 2015)"),
     ylim = c(0,100),
     xlab = expression(Q[0] ~ "(Hursh et al., 2008)"),
     xlim = c(0,100))
abline(comps.weight)

comps.base <- lm(a.Koff ~ a.HS, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HS,
     main = expression(A[LOWER]~"with Relative Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.base)

comps.weight <- lm(a.Koff ~ a.HSw, data = frameToAnalyze)
plot(frameToAnalyze$a.Koff ~ frameToAnalyze$a.HSw,
     main = expression(A[LOWER]~ "with Absolute Error"),
     ylab = expression(alpha~"(Koffarnus et al., 2015)"),
     ylim = c(-5,0),
     xlab = expression(alpha~"(Hursh et al., 2008)"),
     xlim = c(-5,0))
abline(comps.weight)


```

The simulated data revealed a total of `r nrow(frameToAnalyze)` that met all criteria for systematic hypothetical purchase task data (from *N*=100). Scatterplot comparisons across each of the three modeling strategies are illustrated in \autoref{fig:fig4}. As illustrated in this figure, each of the three strategies differed with respect to their correspondence. 

## Strategy 1 vs. Strategy 2

Comparisons between Strategy 1 and 2 revealed a significant correlation for $Q_0$ (r = 0.8955, t = 17.313, df = 74, p-value < 2.2e-16). Similarly, there was a significant correlation for $\alpha$ (r = 0.9344, t = 22.515, df = 74, p-value < 2.2e-16). Scatterplot visualization of these two strategie indicated that, overall, the two approaches generally corresponded to a non-trivial degree.

## Strategy 1 vs. Strategy 3

Comparisons between Strategy 2 and 3 revealed a perfect correlation for $Q_0$ (r = 1, t = 82678, df = 74, p-value < 2.2e-16). Similarly, there was also a near perfect correlation for $\alpha$ (r = 0.9999, t = 22.515, df = 74, p-value < 2.2e-16). Scatterplot visualization of these two strategie indicated that, overall, the two approaches corresponded near perfectly.

# Discussion

The purpose of this technical report was to further explore the role of zero and non-zero asymptotes in two popular models of operant demand. Indeed, debate as to whether 0 consumption values contribute to the characterization of demand continue to date. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
